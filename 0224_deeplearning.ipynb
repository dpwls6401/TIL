{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0224_deeplearning",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "Lg1kXWcYLihY"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 곱셈 계층 역전파(딥러닝 2. 신경망학습 교안 p.39)\n",
        "class MulLayer:\n",
        "  def __init__(self):\n",
        "    self.X = None\n",
        "    self.y = None\n",
        "\n",
        "  # forward\n",
        "  def forward(self,x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    return x * y\n",
        "  \n",
        "  # backward\n",
        "  # dout : 상류에서 흘러오는 값\n",
        "  def backward(self, dout):\n",
        "    dx = dout * self.y\n",
        "    dy = dout * self.x\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "MBXUrrBtLtsE"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward 전방향\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "# backward 역방향\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(f\"price : {price}\")\n",
        "print(f\"dapple : {dapple}\") #미분한 값\n",
        "print(f\"dapple_num : {dapple_num}\")\n",
        "print(f\"dtax : {dtax}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kmetfk4JM2qG",
        "outputId": "73f8d7b9-026b-4dd0-b6ce-94c08b1a3e63"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price : 220.00000000000003\n",
            "dapple : 2.2\n",
            "dapple_num : 110.00000000000001\n",
            "dtax : 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 곱셈&덧셈 계층 역전파(딥러닝 2. 신경망학습 교안 p.40)\n",
        "# 위에서 만든 곱셈 클래스 활용\n",
        "# 덧셈 클래스 생성\n",
        "class AddLayer:\n",
        "  #위 MulLayer에서 초기화했으니 초기화할 필요 없음\n",
        "\n",
        "  # forward\n",
        "  def forward(self, x, y):\n",
        "    return x + y\n",
        "\n",
        "  # backward\n",
        "  # dout : 상류에서 흘러오는 값\n",
        "  def backward(self, dout):\n",
        "    dx = dout\n",
        "    dy = dout\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "nYfwBouZPbO2"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# 계산과정 4번이므로 4개 계층 만들어줌\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "add_ao_layer = AddLayer()\n",
        "\n",
        "# forward (전방향)\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
        "sum_price = add_ao_layer.forward(apple_price, orange_price)\n",
        "price = mul_tax_layer.forward(sum_price, tax)\n",
        "\n",
        "# backward (역방향)\n",
        "dprice = 1\n",
        "dsum_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple_price, dorange_price = add_ao_layer.backward(dsum_price)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
        "\n",
        "print(f\"price : {price}\")\n",
        "print(f\"dapple : {dapple}\")\n",
        "print(f\"dapple_num : {dapple_num}\")\n",
        "print(f\"dorange : {dorange}\")\n",
        "print(f\"dorange_num : {dorange_num}\")\n",
        "print(f\"dtax : {dtax}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV88E3WDVoqy",
        "outputId": "4a692087-8be6-43bf-c5d0-f65ff21a5899"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price : 715.0000000000001\n",
            "dapple : 2.2\n",
            "dapple_num : 110.00000000000001\n",
            "dorange : 3.3000000000000003\n",
            "dorange_num : 165.0\n",
            "dtax : 650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relu 역전파"
      ],
      "metadata": {
        "id": "lNLGTMyTheXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x > 0이면 그대로, x <= 0이면 0으로 만들기\n",
        "class Relu:\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    self.mask = (x <= 0) # 입력이 0이냐 0이 아니냐에 대한 정보\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0 # 0보다 작거나 같은 애들 인덱스로 가져와서 0으로 만들어줌\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0 # y를 가지고 미분한 값이므로 0보다 작은 값 있을 수 있음\n",
        "    dx = dout\n",
        "    return dx"
      ],
      "metadata": {
        "id": "DqG3GUQVbPre"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid 역전파 - 교안 p.45"
      ],
      "metadata": {
        "id": "hO0kNc08kPIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "2XPBWkxWmk7q"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid 노드 정의\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.out = sigmoid(x) #sigmoid 통과한 y\n",
        "    return self.out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * self.out * (1 - self.out)\n",
        "    return dx"
      ],
      "metadata": {
        "id": "FULXDo-fkQuj"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "배치용 Affine 계층 역전파 (교안 p.49) => X * W + B"
      ],
      "metadata": {
        "id": "2xk2y1kG-klq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine:\n",
        "  def __init__(self, W, b): # W는 입력값의 개수와 맞아야 함, b는 출력값의 개수와 맞아야 함\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, x): #W, b는 위에 있으니까 x만!\n",
        "    self.x = x\n",
        "    \n",
        "    out = np.dot(self.x, self.W) + self.b\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout): # dout : 상류에서 흘러내려온 값\n",
        "    dx = np.dot(dout, self.W.T) # 1번과정\n",
        "\n",
        "    self.dW = np.dot(self.x.T, dout) # 2번과정\n",
        "    self.db = np.sum(dout, axis=0) # 3번과정\n",
        "\n",
        "    return dx # 미분한 값을 흘려 보내줘야 다음 값의 input으로 들어갈 수 있음"
      ],
      "metadata": {
        "id": "2uBkdKBBn8ba"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  exp_x = np.exp(x)\n",
        "  sum_exp_x = np.sum(exp_x, axis=-1).reshape(-1,1)\n",
        "  #axis=-1인 이유; 맨 마지막에 있는 차원을 기준으로 구하기 위함??\n",
        "  #np.array([[0, 1, 3], [0, 5, 4]]).shape -> (2,3)인데 여기서 3을 기준으로 계산하기 위함\n",
        "\n",
        "  y = exp_x / sum_exp_x\n",
        "  \n",
        "  return y"
      ],
      "metadata": {
        "id": "RT2z1RrdAHel"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_error(y,t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size) #원래 y.size는 (10,) -> shape 통과시키면 (1,10)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  #batch_size는 sample의 개수만큼 나눠줘야 하는 것\n",
        "  \n",
        "  delta = 1e-7\n",
        "  return -np.sum(t * np.log(y + delta)) / batch_size\n",
        "  #각각에 대한 확률값 구하기 위해 batch_size로 나눠줘야 함"
      ],
      "metadata": {
        "id": "Jzz5xwvBLl01"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxWithLoss():\n",
        "  def __init__(self):\n",
        "    self.loss = None\n",
        "    self.y = None\n",
        "    self.t = None\n",
        "\n",
        "  def forward(self, x, t): #x: 입력값, t: 라벨\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "    return self.loss\n",
        "\n",
        "  def backward(self, dout=1): # 맨마지막 단의 dout은 1이었음\n",
        "    batch_size = self.t.shape[0]\n",
        "\n",
        "    if self.t.size == self.y.size: #y는 원핫 형태; t가 라벨 인코딩되어 있다면 00010 이런식으로 바꿔줘야 함\n",
        "      dx = (self.y - self.t) / batch_size #둘다 원핫일 경우\n",
        "    else: #라벨인코딩 있을 경우\n",
        "      dx = self.y.copy()\n",
        "      dx[np.arange(batch_size), self.t] -= 1\n",
        "      dx = dx / batch_size\n",
        "    return dx"
      ],
      "metadata": {
        "id": "wL-45nUnL9Hb"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _numerical_gradient_no_batch(f,w): #여기서 x는 가중치!\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(w)\n",
        "  \n",
        "  for idx in range(w.size):\n",
        "    tmp_val = w[idx] #n번째 가중치\n",
        "\n",
        "    #f(w+h)\n",
        "    w[idx] = float(tmp_val) + h\n",
        "    fxh1 = f(w)\n",
        "\n",
        "    #f(w-h)\n",
        "    w[idx] = float(tmp_val) - h\n",
        "    fxh2 = f(w)\n",
        "\n",
        "    grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
        "    w[idx] = tmp_val\n",
        "\n",
        "  return grad"
      ],
      "metadata": {
        "id": "nqan8TaUPBpj"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_gradient(f,W): #여기서 X는 가중치(weight)!\n",
        "  #샘플 1개인 경우는 그냥 호출\n",
        "  if W.ndim == 1:\n",
        "    return _numerical_gradient_no_batch(f,W)\n",
        "  \n",
        "  #샘플 여러개 한꺼번에 들어온 경우: batch_size = 2\n",
        "  else:\n",
        "    grad = np.zeros_like(W)\n",
        "    \n",
        "    for idx, W in enumerate(W):\n",
        "      grad[idx] = _numerical_gradient_no_batch(f,W)\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "YQB4QQZhPNL2"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    #파라미터 랜덤 초기화\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    self.layers = OrderedDict() #정렬된 딕셔너리\n",
        "    self.layers[\"Affine1\"] = Affine(self.params['W1'], self.params['b1'])\n",
        "    self.layers[\"Relu1\"] = Relu() #입력값 없음\n",
        "    self.layers[\"Affine2\"] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.lastlayer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "\n",
        "    return x #softmax 들어가기 전의 값(logit)\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.lastlayer.forward(y, t) #softmaxwithloss에 들어가서 교차엔트로피 통과시켜 loss 반환\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    if t.ndim != 1:\n",
        "      t = np.argmax(t, axis=1) #열에 대해 argmax\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0]) #sample 개수만큼 나눠서 평균 구함\n",
        "    return accuracy\n",
        "\n",
        "  #각 파라미터의 기울기 구하기\n",
        "  def numerical_gradient(self, x, t):\n",
        "    #목적 함수 - cross entropy\n",
        "    loss_W = lambda W : self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    #목적함수에 대해 각 파라미터 별로 편미분\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    self.loss(x, t)\n",
        "\n",
        "    dout = 1\n",
        "    dout = self.lastlayer.backward(dout) #마지막단의 역전파 오류값 구해짐\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    grads = {}\n",
        "    #편미분 안하고 backward 하면서 계산한 값 하나씩 가져옴\n",
        "    grads['W1'] = self.layers['Affine1'].dW\n",
        "    grads['b1'] = self.layers['Affine1'].db\n",
        "    grads['W2'] = self.layers['Affine2'].dW\n",
        "    grads['b2'] = self.layers['Affine2'].db\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "QjimLgtvPQbN"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "  def _change_one_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T): #60000개의 row를 하나씩 던져줌\n",
        "      row[X[idx]] = 1 #label에 해당하는 값에 1 - 0000010000 이런식으로 만들어주기\n",
        "\n",
        "    return T\n",
        "\n",
        "  with open('/content/drive/MyDrive/mnist.pkl', 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "  if normalize: #이미지 읽어서 정규화\n",
        "    for key in ('train_img', 'test_img'):\n",
        "      dataset[key] = dataset[key].astype(np.float32)\n",
        "      dataset[key] /= 255.0 #기존 데이터는 0-256까지 정수값 가지고 있음 ->0~1사이 값으로 표현\n",
        "\n",
        "  if one_hot_label: #one-hot encoding\n",
        "    dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "    dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
        "    #60000개 숫자를 10개로 만들어줌\n",
        "  \n",
        "  if not flatten:\n",
        "    for key in ('train_img', 'test_img'):\n",
        "      dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "  return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])"
      ],
      "metadata": {
        "id": "SiQT80TaZyxh"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 로딩 - 원핫 형식으로\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)"
      ],
      "metadata": {
        "id": "O7ykG044ZzQH"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2층 신경망 객체 생성\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
      ],
      "metadata": {
        "id": "-_W663DQZ2eU"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iters_num = 10000 #반복 횟수\n",
        "train_size = x_train.shape[0] #훈련 데이터 크기\n",
        "batch_size = 100 #미니배치 사이즈\n",
        "learning_rate = 0.01 #학습률\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size/batch_size, 1) #1 에폭당 반복 횟수. 최소 한번\n",
        "#한 에폭은 전체 데이터 개수 60000개, 배치는 그걸 자른 것\n",
        "#배치 사이즈 0될 수도 있으니까 1을 줘야 함"
      ],
      "metadata": {
        "id": "L3YaA8FlZ-IE"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss 점점 줄어드는 걸 보기 위함\n",
        "for i in range(iters_num):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  #랜덤하게 0 ~ train_size-1 안에 있는 숫자를 batch_size만큼 가져옴\n",
        "  #index 반환. 중복 가능\n",
        "\n",
        "  x_batch = x_train[batch_mask] #랜덤으로 배치사이즈만큼 훈련 데이터에서 선택\n",
        "  y_batch = y_train[batch_mask] #랜덤으로 배치사이즈만큼 라벨에서 선택\n",
        "\n",
        "  #각 파라미터의 gradient 계산\n",
        "  grad = network.gradient(x_batch, y_batch)\n",
        "\n",
        "  #각 파라미터를 업데이트 (ex. w = w - 학습률 * 기울기)\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    #loss 구하기\n",
        "    loss = network.loss(x_batch, y_batch)\n",
        "    train_loss_list.append(loss) #loss 값을 train_loss_list에 추가\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "    #정확도 검사할 때는 batch가 아니라 전체 데이터 넣어줘야 함\n",
        "      #파라미터 업데이트 후의 훈련 데이터 정확도\n",
        "      train_acc = network.accuracy(x_train, y_train) #전체 데이터 accuracy\n",
        "      #파라미터 업데이트 후의 테스트 데이터 정확도 - 안 본 데이터에 대한 정확도\n",
        "      test_acc = network.accuracy(x_test, y_test)\n",
        "\n",
        "      train_acc_list.append(train_acc) #accuracy 값을 train_acc_list에 추가\n",
        "      test_acc_list.append(test_acc) #accuracy 값을 test_acc_list에 추가\n",
        "\n",
        "      print(f\"loss {loss}, train_accuracy{train_acc}, test_accuracy{test_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1fDLquwacNX",
        "outputId": "4fa29673-3b46-45fe-c358-8290dacbd221"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.25016748846963366, train_accuracy0.9186333333333333, test_accuracy0.9212\n",
            "loss 0.2501510847518174, train_accuracy0.9186333333333333, test_accuracy0.9212\n",
            "loss 0.24950272240527827, train_accuracy0.9186833333333333, test_accuracy0.9211\n",
            "loss 0.24949553859911228, train_accuracy0.9186833333333333, test_accuracy0.9211\n",
            "loss 0.24997857146226415, train_accuracy0.92065, test_accuracy0.9242\n",
            "loss 0.2499629257685171, train_accuracy0.9206333333333333, test_accuracy0.9242\n",
            "loss 0.24896929301546677, train_accuracy0.9205333333333333, test_accuracy0.924\n",
            "loss 0.2489623170919413, train_accuracy0.9205333333333333, test_accuracy0.924\n",
            "loss 0.3764329030766481, train_accuracy0.9224833333333333, test_accuracy0.9243\n",
            "loss 0.3763841429659092, train_accuracy0.9225, test_accuracy0.9243\n",
            "loss 0.3734117513505518, train_accuracy0.9224, test_accuracy0.9239\n",
            "loss 0.3733875124672399, train_accuracy0.9224, test_accuracy0.9238\n",
            "loss 0.15827354306904323, train_accuracy0.9229333333333334, test_accuracy0.9247\n",
            "loss 0.15824925914052154, train_accuracy0.9229333333333334, test_accuracy0.9247\n",
            "loss 0.1574801981312792, train_accuracy0.9229333333333334, test_accuracy0.9246\n",
            "loss 0.15746928487562978, train_accuracy0.9229333333333334, test_accuracy0.9246\n",
            "loss 0.30712340710586866, train_accuracy0.9242333333333334, test_accuracy0.9261\n",
            "loss 0.3071123516533024, train_accuracy0.9242333333333334, test_accuracy0.9261\n",
            "loss 0.3063096859919067, train_accuracy0.9241166666666667, test_accuracy0.9261\n",
            "loss 0.30630783561033864, train_accuracy0.9241166666666667, test_accuracy0.9261\n",
            "loss 0.26271979524002664, train_accuracy0.92555, test_accuracy0.9286\n",
            "loss 0.2626829901228236, train_accuracy0.92555, test_accuracy0.9286\n",
            "loss 0.2614603655369244, train_accuracy0.9254333333333333, test_accuracy0.9287\n",
            "loss 0.2614485450521874, train_accuracy0.9254166666666667, test_accuracy0.9287\n",
            "loss 0.259343603925973, train_accuracy0.9269166666666667, test_accuracy0.9293\n",
            "loss 0.2593237739573107, train_accuracy0.9269166666666667, test_accuracy0.9293\n",
            "loss 0.2583004649402237, train_accuracy0.9269666666666667, test_accuracy0.929\n",
            "loss 0.2582934028890848, train_accuracy0.9269666666666667, test_accuracy0.929\n",
            "loss 0.2267268232313785, train_accuracy0.9280166666666667, test_accuracy0.9311\n",
            "loss 0.2266935657352404, train_accuracy0.9280166666666667, test_accuracy0.9311\n",
            "loss 0.22606695231710947, train_accuracy0.9279833333333334, test_accuracy0.931\n",
            "loss 0.22605980392184158, train_accuracy0.928, test_accuracy0.931\n",
            "loss 0.2824909127040435, train_accuracy0.9292833333333334, test_accuracy0.9306\n",
            "loss 0.282446679390333, train_accuracy0.9293, test_accuracy0.9306\n",
            "loss 0.2809696421521403, train_accuracy0.9293666666666667, test_accuracy0.9309\n",
            "loss 0.28094816733162686, train_accuracy0.9293666666666667, test_accuracy0.9309\n",
            "loss 0.17002941363388174, train_accuracy0.9305833333333333, test_accuracy0.9319\n",
            "loss 0.17001687576626423, train_accuracy0.9305833333333333, test_accuracy0.9318\n",
            "loss 0.16942742274925876, train_accuracy0.9305666666666667, test_accuracy0.9318\n",
            "loss 0.1694227188001757, train_accuracy0.9305666666666667, test_accuracy0.9318\n",
            "loss 0.20335322641295384, train_accuracy0.9313666666666667, test_accuracy0.9334\n",
            "loss 0.20332862566161297, train_accuracy0.9313666666666667, test_accuracy0.9334\n",
            "loss 0.20237671939311, train_accuracy0.93135, test_accuracy0.9335\n",
            "loss 0.2023681411627468, train_accuracy0.93135, test_accuracy0.9335\n",
            "loss 0.2526806388776208, train_accuracy0.9325166666666667, test_accuracy0.9324\n",
            "loss 0.25266908922365994, train_accuracy0.9325166666666667, test_accuracy0.9324\n",
            "loss 0.2522527146537842, train_accuracy0.93255, test_accuracy0.9323\n",
            "loss 0.2522491127637972, train_accuracy0.93255, test_accuracy0.9323\n",
            "loss 0.2226987656506229, train_accuracy0.93395, test_accuracy0.9343\n",
            "loss 0.22265678825310584, train_accuracy0.93395, test_accuracy0.9343\n",
            "loss 0.22154237777865682, train_accuracy0.9339666666666666, test_accuracy0.9346\n",
            "loss 0.22152830346511584, train_accuracy0.9339666666666666, test_accuracy0.9346\n",
            "loss 0.3424320207838493, train_accuracy0.9344166666666667, test_accuracy0.935\n",
            "loss 0.3423720704008767, train_accuracy0.9344666666666667, test_accuracy0.9351\n",
            "loss 0.3405316666554306, train_accuracy0.9344666666666667, test_accuracy0.935\n",
            "loss 0.3405159774089212, train_accuracy0.9344666666666667, test_accuracy0.935\n",
            "loss 0.1261052404263799, train_accuracy0.9356333333333333, test_accuracy0.9344\n",
            "loss 0.1260813916804102, train_accuracy0.9356333333333333, test_accuracy0.9344\n",
            "loss 0.12549177812891116, train_accuracy0.9356166666666667, test_accuracy0.9344\n",
            "loss 0.12548479270509152, train_accuracy0.9356333333333333, test_accuracy0.9344\n",
            "loss 0.1700770159952144, train_accuracy0.9364166666666667, test_accuracy0.9357\n",
            "loss 0.17005316322911723, train_accuracy0.9364166666666667, test_accuracy0.9356\n",
            "loss 0.16928933510086627, train_accuracy0.93645, test_accuracy0.9353\n",
            "loss 0.16928229547728413, train_accuracy0.93645, test_accuracy0.9353\n",
            "loss 0.387411578099444, train_accuracy0.9376166666666667, test_accuracy0.9358\n",
            "loss 0.3873810368377044, train_accuracy0.9376166666666667, test_accuracy0.9358\n",
            "loss 0.38639732904031293, train_accuracy0.9375833333333333, test_accuracy0.9359\n",
            "loss 0.3863927487796326, train_accuracy0.9375833333333333, test_accuracy0.9359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label=\"train acc\")\n",
        "plt.plot(x, test_acc_list, label=\"test acc\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc=\"lower right\") #범례 위치 지정\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "mqK0zcpKacxb",
        "outputId": "d3af8980-f737-4a54-b8e4-46f1de9220d0"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pp4PV2EnnmP9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}