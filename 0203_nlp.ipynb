{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7fda063",
   "metadata": {},
   "source": [
    "문장요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8c35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "def get_news(url):\n",
    "    headers = {'user-agent':'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article=html.select_one(\"#articleBodyContents\").text.strip()\n",
    "    \n",
    "    return article\n",
    "\n",
    "url='https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=018&aid=0004430108'\n",
    "\n",
    "news_article = get_news(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17621aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize #이걸로 문장 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80369274",
   "metadata": {},
   "source": [
    "Luhn Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54fcf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kss 활용\n",
    "from kss import split_sentences\n",
    "\n",
    "def get_sentences(text):\n",
    "    #return sent_tokenize(text)\n",
    "    return split_sentences(text) #text에 있는 문장 토큰화\n",
    "\n",
    "def get_words(text,isNoun=False):\n",
    "    if isNoun: #명사만 가져오기\n",
    "        return [token[0] for token in mecab.pos(text) if token[1][0]=='N' and len(token[0]) > 0]\n",
    "    else: #아니면 형태소로 나누기만 하기\n",
    "        return [token[0] for token in mecab.pos(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c5db7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['과기정통부, 22일 유영민 장관 등 참석해 기념행사2021년까지 1516억원 투입, 5100여종 데이터 구축민간 클라우드 통한 외부연계체계도..\"개방성 강화\"[이데일리 이재운 기자] 국가 차원의 빅데이터 활용 시대가 열린다.',\n",
       " '새로운 산업 창출과 기존 산업의 변화에 이르는 ‘혁신성장’을 위한 센터가 문을 연다. 10개 분야에 걸쳐 ‘데이터 경제’의 발전을 위한 정부의 청사진을 현실로 구현하는데 앞장선다는 계획이다.',\n",
       " '22일 과학기술정보통신부는 서울 중구 대한상공회의소에서 데이터 생태계 조성과 혁신 성장의 기반 마련을 위한 ‘빅데이터 플랫폼 및 센터’ 출범식 행사를 개최했다.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#함수로 분리\n",
    "sentence_list = get_sentences(news_article)\n",
    "print(len(sentence_list))\n",
    "sentence_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d55beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "['과기정통부, 22일 유영민 장관 등 참석해 기념행사2021년까지 1516억원 투입, 5100여종 데이터 구축민간 클라우드 통한 외부연계체계도..\"개방성 강화\"[이데일리 이재운 기자] 국가 차원의 빅데이터 활용 시대가 열린다.', '새로운 산업 창출과 기존 산업의 변화에 이르는 ‘혁신성장’을 위한 센터가 문을 연다. 10개 분야에 걸쳐 ‘데이터 경제’의 발전을 위한 정부의 청사진을 현실로 구현하는데 앞장선다는 계획이다.', '22일 과학기술정보통신부는 서울 중구 대한상공회의소에서 데이터 생태계 조성과 혁신 성장의 기반 마련을 위한 ‘빅데이터 플랫폼 및 센터’ 출범식 행사를 개최했다.', '유영민 과기정통부 장관을 비롯해 노웅래 국회 과학기술정보방송통신위원회 위원장 등 300여명이 참가했다.', '◇10개 분야 100개 센터..3년간 1516억원 투입이미지: 픽사베이빅데이터는 데이터 활용을 통해 혁신성장을 이루자는 문재인 정부의 경제 성장 핵심 요소중 하나다.', '문재인 대통령이 직접 올 들어 데이터 활용과 이에 따른 정보보호(보안)에 대한 중요성을 강조하기도 했다.', '이런 맥락 속에서 빅데이터센터는 공공과 민간이 협업해 활용도 높은 양질의 데이터를 생산·구축하고, 플랫폼은 이를 수집·분석·유통하는 역할을 담당한다.', '과기정통부는 분야별 플랫폼 10개소와 이와 연계된 기관별 센터 100개소를 구축하는데 3년간 총 1516억원을 투입할 계획이며, 올해 우선 640억원 규모의 사업을 추진하고 있다.', '대상 분야는 △금융(BC카드) △환경(한국수자원공사) △문화(한국문화정보원) △교통(한국교통연구원) △헬스케어(국립암센터) △유통·소비(매일방송) △통신(KT) △중소기업(더존비즈온) △지역경제(경기도청) △산림(한국임업진흥원) 등으로 현재 1차 공모를 통해 72개 빅데이터 센터를 선정했고, 다음달 8일까지 2차 공모를 통해 28개를 추가 선정해 총 100개를 지원, 운영할 계획이다.', '이를 통해 데이터 생태계를 혁신하고 기업의 경쟁력을 제고하는 역할을 수행한다.', '주요 활용 전략·사례를 보면 빅데이터 활용을 통해 ‘신(新) 시장’을 창출하는 방안을 담고 있다.', '금융 플랫폼의 경우 소상공인 신용평가 고도화 등을 통해 금융 취약 계층 대상 중금리 대출이자를 2%p 절감해 연간 1조원의 신규대출을 창출할 전망이다.', '유통·소비와 중소기업 플랫폼은 소상공인이나 중소기업의 폐업률 감소를, 문화 플랫폼은 문화·예술 관람률과 생활체육 참여율을 높이는 방안을 모색한다.', '의료비 절감(헬스케어)과 기업의 매출 향상을 통한 산업 육성(통신·산림) 등도 눈길을 끈다.', '과기정통부 제공◇2021년까지 5100여종 데이터 구축..AI 알고리즘 제공도센터는 우선 분야별 데이터 부족 문제를 해소하기 위해 올해 말까지 시장 수요가 높은 1400여종 신규 데이터를 생산ㆍ구축하고, 사업이 완료되는 2021년까지 총 5100여종 양질의 풍부한 데이터를 생산·구축해 시장에 공급할 계획이다.', '특히 공공과 민간 사이 데이터 파일형식 등이 달라 호환이 제대로 이뤄지지 못한 문제를 해소하기 위해 개방형 표준을 적용하고, 품질관리기준도 마련해 운영한다.', '기업들이 실제 활용 가능한 최신 데이터를 확보하는데도 수개월이 소요된다는 문제점을 개선하기 위한 방안도 추진한다.', '센터와 플랫폼 간 연계체계에는 민간 클라우드를 기반으로 활용하고, 센터에 축적된 데이터도 계속 외부와 개방·공유하며 최신·연속성을 확보한다는 계획이다.', '100개 센터에서 수집된 데이터를 융합·분석한 뒤 맞춤형 데이터 제작 등 양질의 데이터로 재생산하고, 기업들이 필요로 하는 데이터를 원하는 형태로 즉시 활용할 수 있도록 제공할 계획이다.', '다양한 분석 도구는 물론 인공지능(AI) 학습 알고리즘도 제공해 이용자가 보다 사용하기 편리한 환경을 제공한다.', '이밖에 필요한 데이터를 쉽게 등록하고 검색할 수 있도록 기준을 마련하고, 데이터 보유와 관리에 대한 체계(거버넌스)를 논의하는 ‘데이터 얼라이언스’를 구성해 보다 안전하게 이용하는 방안도 마련했다.', '유영민 과기정통부 장관은 “오늘 출범식은 대한민국이 데이터 강국으로 가기 위한 초석을 놓은 자리”라며 “세계 주요국들보다 데이터 경제로 나아가는 발걸음이 다소 늦었지만, 빅데이터 플랫폼과 센터를 지렛대로 우리나라의 낙후된 데이터 생태계를 혁신하고 기업의 경쟁력을 한 단계 제고할 수 있도록 정책적 역량을 집중하겠다”고 밝혔다.', '이재운 (jwlee@edaily.co.kr)네이버 홈에서 ‘이데일리’ 뉴스 [구독하기▶]꿀잼가득 [영상보기▶] , 청춘뉘우스~ [스냅타임▶]＜ⓒ종합 경제정보 미디어 이데일리 - 무단전재 & 재배포 금지＞']\n"
     ]
    }
   ],
   "source": [
    "#kss로 문장 분리하기; kss는 한국어 문장 분리기 중 성능 좋지만 시간 오래 걸림\n",
    "from kss import split_sentences\n",
    "sentence_list = split_sentences(news_article)\n",
    "print(len(sentence_list))\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e29996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#중요단어 계산: 중복허용 안되므로 set ==>빈도수 계산\n",
    "def get_keywords(word_list,min_ratio=0.001,max_ratio=0.5):\n",
    "    keywords = set() #중요단어를 set에 넣기\n",
    "    \n",
    "    count_dict={}\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word in count_dict.keys():\n",
    "            count_dict[word] = count_dict[word]+1 #이미 단어 있으면 빈도수 +1\n",
    "        else:\n",
    "            count_dict[word] = 1 #아직 단어 없으면 빈도 1로 변경\n",
    "\n",
    "    for word,cnt in count_dict.items():\n",
    "        #모든 word count에 있는 애들 다 돌기 위함. count_dict는 딕셔너리이므로 key와 value 반환\n",
    "        word_percentage = cnt / len(word_list)\n",
    "        #전체단어 중 해당 단어가 몇번 등장하는지(0.001부터 0.5까지)\n",
    "        #cnt = count_dict[word] ==>해당 단어의 등장빈도 수\n",
    "        \n",
    "        #min-max 사이에 있으면 중요 키워드로 하겠다\n",
    "        if word_percentage >= min_ratio and word_percentage <= max_ratio:\n",
    "            keywords.add(word) #set이니까 add\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9910a2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'사과', '포도'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keywords(['바나나','사과','바나나','바나나','포도'])\n",
    "#바나나는 0.5이상으로 너무 자주 등장해서 키워드에 포함x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8350a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 중요도 계산; sentence 내에서 키워드 있는지 검사 ex.4^2/7 검사하기\n",
    "def get_sentence_weight(token_list, keywords):\n",
    "    window_start=0; window_end =-1\n",
    "    \n",
    "    for i in range(len(token_list)): #token 개수만큼 돌기\n",
    "        if token_list[i] in keywords:\n",
    "            window_start=i #중요단어 등장하는 시작점 찾기\n",
    "            break\n",
    "            \n",
    "    for i in range(len(token_list)-1,-1,-1): #거꾸로 range,0도 포함하기 위해 두번째에 -1\n",
    "        if token_list[i] in keywords:\n",
    "            window_end=i #중요단어 끝지점 찾기\n",
    "            break\n",
    "            \n",
    "    if window_start > window_end:\n",
    "        return 0 #해당 문장에 대한 weight를 0으로 줌\n",
    "    \n",
    "    window_size = window_end - window_start + 1\n",
    "    \n",
    "    #keyword 개수\n",
    "    keyword_cnt=0\n",
    "    for w in token_list:\n",
    "        if w in keywords:\n",
    "            keyword_cnt +=1\n",
    "    return keyword_cnt * keyword_cnt / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cc39ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_weight(['바나나','사과','바나나','바나나','포도'],{'사과', '포도'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59477704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(context, no_sentences=5):\n",
    "    word_list = get_words(context,isNoun=True) #기사에서 단어 가져오기\n",
    "    keywords = get_keywords(word_list) #단어 목록에서 중요 키워드 뽑기\n",
    "    \n",
    "    sentence_list = get_sentences(context) #문장 토큰화해서 리스트로 가져오기\n",
    "    \n",
    "    sentence_weight =[]\n",
    "    for sentence in sentence_list:\n",
    "        token_list = get_words(sentence, isNoun = False) #안 써도 됨\n",
    "        sentence_weight.append((get_sentence_weight(token_list,keywords), sentence)) #어떤 문장인지 보기위해 tuple로 넘겨줌\n",
    "    \n",
    "    sentence_weight.sort(reverse=True)\n",
    "    \n",
    "    return sentence_weight[:no_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "691eb518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26.07758620689655, '대상 분야는 △금융(BC카드) △환경(한국수자원공사) △문화(한국문화정보원) △교통(한국교통연구원) △헬스케어(국립암센터) △유통·소비(매일방송) △통신(KT) △중소기업(더존비즈온) △지역경제(경기도청) △산림(한국임업진흥원) 등으로 현재 1차 공모를 통해 72개 빅데이터 센터를 선정했고, 다음달 8일까지 2차 공모를 통해 28개를 추가 선정해 총 100개를 지원, 운영할 계획이다.')\n",
      "(21.333333333333332, '과기정통부, 22일 유영민 장관 등 참석해 기념행사2021년까지 1516억원 투입, 5100여종 데이터 구축민간 클라우드 통한 외부연계체계도..\"개방성 강화\"[이데일리 이재운 기자] 국가 차원의 빅데이터 활용 시대가 열린다.')\n",
      "(18.753246753246753, '유영민 과기정통부 장관은 “오늘 출범식은 대한민국이 데이터 강국으로 가기 위한 초석을 놓은 자리”라며 “세계 주요국들보다 데이터 경제로 나아가는 발걸음이 다소 늦었지만, 빅데이터 플랫폼과 센터를 지렛대로 우리나라의 낙후된 데이터 생태계를 혁신하고 기업의 경쟁력을 한 단계 제고할 수 있도록 정책적 역량을 집중하겠다”고 밝혔다.')\n"
     ]
    }
   ],
   "source": [
    "sum_sents = summarize(news_article,3)\n",
    "for s in sum_sents:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf421fb",
   "metadata": {},
   "source": [
    "textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ce0c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"딸기 바나나 사과 파인애플 수박. 바나나 사과 딸기 포도. 복숭아 수박. 파인애플 사과 딸기 바나나\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83dfb04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#유사도 함수_자카드 유사도=교집합/합집합\n",
    "def sentence_similarity(sent1,sent2):\n",
    "    \n",
    "    sent1_list = [token[0] for token in mecab.pos(sent1) if token[1][0] in ['N','V']]\n",
    "    sent2_list = [token[0] for token in mecab.pos(sent2) if token[1][0] in ['N','V']]\n",
    "    \n",
    "    union = set(sent1_list).union(set(sent2_list))\n",
    "    intersection = set(sent1_list).intersection(set(sent2_list))\n",
    "    \n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "sentence_similarity(\"나는 치킨을 좋아해\",\"나는 치킨을 싫어해\")\n",
    "#자카드유사도=2/(3+3-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b51cd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.3409091 , 0.11363637, 0.54545456],\n",
       "       [0.45454544, 0.        , 0.        , 0.54545456],\n",
       "       [1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.57142854, 0.4285714 , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def buildMatrix(sentences):\n",
    "    sentences_len = len(sentences)\n",
    "    score = np.ones(sentences_len, dtype=np.float32)\n",
    "    \n",
    "    weighted_edge = np.zeros((sentences_len,sentences_len),dtype=np.float32)\n",
    "    \n",
    "    for i in range(sentences_len):\n",
    "        for j in range(sentences_len):\n",
    "            if i == j: #나와 나 사이의 유사도는 0으로 만들어줌\n",
    "                continue\n",
    "            \n",
    "            weighted_edge[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
    "    \n",
    "    for i in range(sentences_len):\n",
    "        score[i] = weighted_edge[i].sum() #해당 문장에 링크 걸려 있는 것들\n",
    "        weighted_edge[i] /= score[i] #각각의 element를 sum한 걸로 나눠주는 것\n",
    "        #0.5 / (0.5+0.167+0.8) ~~이런 식으로 엣지 가중치 업데이트\n",
    "    \n",
    "    return weighted_edge\n",
    "\n",
    "buildMatrix(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51ea2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score와 엣지 가중치 열끼리 곱한다음 더해서 d 적용해서 score 업데이트\n",
    "#전에는 scoring할 때: summation 활용한 두가지 방법 // dot연산(행렬 곱)\n",
    "def scoring(weighted_edge, score, eps=0.0001, d=0.85, max_iter=50): #eps=trheshold\n",
    "    for it in range(max_iter):\n",
    "        new_score = (1-d) + d * weighted_edge.T.dot(score) #score가 new score가 됨\n",
    "        \n",
    "        for diff in abs(new_score-score):\n",
    "            if diff <= eps: #diff는 score 수만큼, eps=threshold이므로 최소 이 이상 돌리겠다\n",
    "                return new_score\n",
    "            \n",
    "        score = new_score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f03cec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 요약 절차\n",
    "def summize(text, no_sentences):\n",
    "    sentence_list = sent_tokenize(text) #문장 토큰화\n",
    "    weighted_edge = buildMatrix(sentence_list) #엣지 가중치 만들기\n",
    "    score_init = np.ones(weighted_edge.shape[0], dtype=np.float32) #점수 초기값 #정방행렬이므로\n",
    "    score = scoring(weighted_edge, score_init) #점수 업데이트\n",
    "    sorted_score = sorted(enumerate(score), key=lambda x:x[1], reverse=True)[:no_sentences] #그냥 sorting하면 idx로 하게됨,so점수로 sorting하기 위해 x[1]\n",
    "    return [(s[1],sentence_list[s[0]]) for s in sorted_score]\n",
    "\n",
    "#my_점수와 문장 변환 코드\n",
    "#     for i in range(len(sorted_score)):\n",
    "#         print(\"점수: {}, 문장: {}\".format(sorted_score[i][1],sentence_list[sorted_score[i][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40b75a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.4098104, '딸기 바나나 사과 파인애플 수박.'),\n",
       " (1.2793452, '파인애플 사과 딸기 바나나'),\n",
       " (1.0245311, '바나나 사과 딸기 포도.')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summize(text,3) #상위3개 각 문장 점수와 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac9efd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.3107076,\n",
       "  '과기정통부는 분야별 플랫폼 10개소와 이와 연계된 기관별 센터 100개소를 구축하는데 3년간 총 1516억원을 투입할 계획이며, 올해 우선 640억원 규모의 사업을 추진하고 있다.대상 분야는 △금융(BC카드) △환경(한국수자원공사) △문화(한국문화정보원) △교통(한국교통연구원) △헬스케어(국립암센터) △유통·소비(매일방송) △통신(KT) △중소기업(더존비즈온) △지역경제(경기도청) △산림(한국임업진흥원) 등으로 현재 1차 공모를 통해 72개 빅데이터 센터를 선정했고, 다음달 8일까지 2차 공모를 통해 28개를 추가 선정해 총 100개를 지원, 운영할 계획이다.'),\n",
       " (1.2891543,\n",
       "  '유영민 과기정통부 장관을 비롯해 노웅래 국회 과학기술정보방송통신위원회 위원장 등 300여명이 참가했다.◇10개 분야 100개 센터..3년간 1516억원 투입이미지: 픽사베이빅데이터는 데이터 활용을 통해 혁신성장을 이루자는 문재인 정부의 경제 성장 핵심 요소중 하나다.'),\n",
       " (1.2184815,\n",
       "  '10개 분야에 걸쳐 ‘데이터 경제’의 발전을 위한 정부의 청사진을 현실로 구현하는데 앞장선다는 계획이다.22일 과학기술정보통신부는 서울 중구 대한상공회의소에서 데이터 생태계 조성과 혁신 성장의 기반 마련을 위한 ‘빅데이터 플랫폼 및 센터’ 출범식 행사를 개최했다.')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extractive summarization 추출적 요약; 기존 문장에서 뽑는 것 ~~ 통계기반\n",
    "#<=> abstractive summarization 추상적 요약; generate생성모델 ~~ 딥러닝\n",
    "summize(news_article,3) #기사 3문장 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaef9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
