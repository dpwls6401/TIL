{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705582fe",
   "metadata": {},
   "source": [
    "TextRank구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af082eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'바나나': 0, '사과': 1, '파인애플': 2, '딸기': 3}\n",
      "{0: '바나나', 1: '사과', 2: '파인애플', 3: '딸기'}\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"딸기\",\"바나나\",\"사과\",\"딸기\",\"파인애플\"]\n",
    "#unique한 값 뽑기 위해 set으로 중복 제거; unique한거 가져올때 set 많이 씀\n",
    "# nodes = list(set(tokens))\n",
    "\n",
    "vocab=['바나나','사과','파인애플','딸기'] #교재 순서와 맞추기 위해\n",
    "\n",
    "#index가져와서 사전 만들기; vocab[i]:i ~~ 단어:인덱스\n",
    "vocab2id = {vocab[i] : i for i in range(len(vocab))}\n",
    "id2vocab = {i : vocab[i] for i in range(len(vocab))} #반대로도 하나 만들어줌\n",
    "\n",
    "print(vocab2id)\n",
    "print(id2vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d45b55d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 1. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.5       , 0.        , 0.5       ],\n",
       "       [0.5       , 0.        , 0.        , 0.5       ],\n",
       "       [0.        , 0.        , 0.        , 1.        ],\n",
       "       [0.33333334, 0.33333334, 0.33333334, 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#그래프 만들기: score만들기->빈 4*4 matrix 0으로 채움->동시등장하면 1씩 표시\n",
    "import numpy as np\n",
    "\n",
    "#weight matrix 만들기\n",
    "vocab_len = len(vocab) #vocab_len에 맞는 행렬만들기 위함\n",
    "weight_edge = np.zeros((vocab_len,vocab_len), dtype=np.float32)\n",
    "\n",
    "#초기 score; 노드의 스코어 만들기\n",
    "scores = np.ones(vocab_len, dtype=np.float32)\n",
    "\n",
    "#vocab 순서대로 window 지나도록\n",
    "window_size=2\n",
    "\n",
    "covered_coocurrences = []\n",
    "\n",
    "for window_start in range(len(tokens)-window_size+1): #range 특성 때문에 +1; window 개수 정의\n",
    "    window = tokens[window_start : window_start + window_size] #start가 이동할 때마다 window 바꿔서 가져오기 위함\n",
    "    for i in range(window_size -1):\n",
    "        #startwindow부터 window 차례대로 돌기 위함; window_size만큼 반복\n",
    "        #window_size가 2인 경우, range(0,1)~0번째만 돌아도 됨! ex. 딸기-바나나\n",
    "        for j in range(i+1, window_size): #window에서 단어끼리 동시등장 비교하기 위한 반복문; 딸기-바나나, 딸기-사과\n",
    "            \n",
    "            if window[i] in vocab and window[j] in vocab: #window의 두 단어가 vocab에 있는 경우만 edge로 연결\n",
    "                #이미 계산해서 edge 계산한 것들; 계산 반복 피하기 위함\n",
    "                #start window0일 때 바나나:사과 = [1:2] ~> start window 1일 때 [0:1] ~so +1\n",
    "                #그래서 원래 index값 찾기 위해 +1\n",
    "                #window0 [2:3], window1[1,2], window2[0,1] ~~ window2의 경우, window_start가 0이므로 i=2를 더해줘서\n",
    "                #--->원래의 인덱스 값 추출하도록!\n",
    "                index_of_i = window_start + i\n",
    "                index_of_j = window_start + j\n",
    "                \n",
    "                #weight_edge: 동시등장하는 단어관계에 +1 해주기\n",
    "                if (index_of_i, index_of_j) not in covered_coocurrences:\n",
    "                    weight_edge[vocab2id[window[i]]][vocab2id[window[j]]] = 1\n",
    "                    weight_edge[vocab2id[window[j]]][vocab2id[window[i]]] = 1\n",
    "                    #-->행렬 대칭이니까 여기도 +1\n",
    "                    #-->겹치는 지점에 있는 애들은 이미 검사했으니까 그냥 넘어갈 수 있음\n",
    "                    covered_coocurrences.append((index_of_i,index_of_j))\n",
    "\n",
    "print(weight_edge)\n",
    "\n",
    "#weight_matrix 만들기: 각 row의 sum구해서 그걸 edge 숫자에서 나눠줘야 함\n",
    "for i in range(vocab_len):\n",
    "    row_sum = weight_edge[i].sum()\n",
    "    weight_edge[i] = weight_edge[i] / row_sum if row_sum > 0 else 0 #0으로 나누면 error나므로 이를 방지하기 위해\n",
    "weight_edge #이게 교안p.36에 있는 weight_matrix -->이제 스코어에 element끼리 곱해서 더해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8424575b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9837155 , 0.9837155 , 0.56564105, 1.4669281 ], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score값 업데이트\n",
    "#위에서 구한 weight_edge와 scores를 element끼리 곱해서 더하기\n",
    "MAX_ITERATIONS = 50\n",
    "d = 0.85\n",
    "threshold = 0.0001 #prev와 현재 차이가 이거보다 작으면 멈춤\n",
    "\n",
    "for it in range(MAX_ITERATIONS): #사용하는 건x, 이 개수만큼 돌려주는 것\n",
    "    prev_score = np.copy(scores) #prev_score 객체 만들어주기\n",
    "    \n",
    "    #numpy element-wise니까 단어 개수만큼 돌면 됨\n",
    "    #weight_edge 열:i, 각 열과 score:j\n",
    "    for i in range(vocab_len):\n",
    "        \n",
    "        summation = 0 #summation: 스코어랑 weight matrix 곱한 것들의 sum\n",
    "        \n",
    "        for j in range(vocab_len):\n",
    "            summation += weight_edge[j][i]*prev_score[j]\n",
    "            \n",
    "        scores[i] = (1-d) + d * summation\n",
    "    \n",
    "    if np.sum(np.fabs(prev_score - scores)) <= threshold: #fabs:절대값\n",
    "        break\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30d25203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 3], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting\n",
    "np.argsort(scores) #오름차순 sorting하고 해당 자리의 index 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60a10440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#argsort의 결과(오름차순) 거꾸로(열 기준)\n",
    "sorted_index = np.flip(np.argsort(scores),0)\n",
    "sorted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a45d889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딸기 : 1.4669281244277954\n",
      "사과 : 0.9837154746055603\n",
      "바나나 : 0.9837154746055603\n"
     ]
    }
   ],
   "source": [
    "#키워드 score 보기\n",
    "n = 3\n",
    "for i in range(n):\n",
    "    print(\"{} : {}\".format(id2vocab[sorted_index[i]], scores[sorted_index[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b685737",
   "metadata": {},
   "source": [
    "TextRank로 뉴스 키워드 5개 끌어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "19a3d384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1\n",
      "데이터 10.217854499816895\n",
      "센터 4.63233757019043\n",
      "활용 3.913996934890747\n",
      "계획 3.0218217372894287\n",
      "경제 2.6802375316619873\n",
      "doc2\n",
      "데이터 3.142937421798706\n",
      "서비스 2.401495933532715\n",
      "한전 2.391519784927368\n",
      "전력 2.309370279312134\n",
      "제공 2.0044283866882324\n",
      "doc3\n",
      "경기도 3.9427449703216553\n",
      "자율 3.9273698329925537\n",
      "데이터 3.909980535507202\n",
      "주행 3.5127453804016113\n",
      "센터 2.4420278072357178\n",
      "doc4\n",
      "지역 4.432885646820068\n",
      "인구 4.349888801574707\n",
      "이동 3.6016430854797363\n",
      "발생 2.8768599033355713\n",
      "폭 2.7860758304595947\n",
      "doc5\n",
      "데이터 8.730308532714844\n",
      "중계 6.474488735198975\n",
      "기관 5.002007484436035\n",
      "금융 4.417078495025635\n",
      "인프라 3.601242780685425\n"
     ]
    }
   ],
   "source": [
    "def text_rank (tokens,vocab, window_size, num):\n",
    "\n",
    "    vocab2id = {vocab[i] : i for i in range(len(vocab))}\n",
    "    id2vocab = {i : vocab[i] for i in range(len(vocab))}\n",
    "\n",
    "\n",
    "    vocab_len = len(vocab)\n",
    "\n",
    "    weight_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "    score = np.ones(vocab_len,dtype=np.float32)\n",
    "\n",
    "    coverd_coocurences = []\n",
    "    \n",
    "    for window_start in range(0,len(tokens)-window_size+1):\n",
    "        window = tokens[window_start : window_start+window_size]\n",
    "        \n",
    "        for i in range(window_size-1):\n",
    "            for j in range(i+1, window_size):            \n",
    "                if window[i] in vocab and window[j] in vocab:\n",
    "                    index_of_i = window_start + i \n",
    "                    index_of_j = window_start + j\n",
    "                    \n",
    "                    if (index_of_i,index_of_j) not in coverd_coocurences:\n",
    "                        weight_edge[vocab2id[window[i]]][vocab2id[window[j]]] = 1\n",
    "                        weight_edge[vocab2id[window[j]]][vocab2id[window[i]]] = 1\n",
    "                        coverd_coocurences.append((index_of_i,index_of_j))\n",
    "\n",
    "    for i in range(vocab_len):\n",
    "        row_sum = weight_edge[i].sum() \n",
    "        weight_edge[i] = weight_edge[i]/row_sum if row_sum > 0 else 0\n",
    "        \n",
    "    MAX_ITERATIONS = 50\n",
    "    d = 0.85\n",
    "    threshold = 0.0001\n",
    "\n",
    "    for it in range(MAX_ITERATIONS):\n",
    "        prev_score = np.copy(score)\n",
    "\n",
    "        for i in range(vocab_len):\n",
    "            summation = 0\n",
    "            for j in range(vocab_len):\n",
    "                summation += weight_edge[j][i] * prev_score[j]\n",
    "\n",
    "            score[i] = (1 - d) +d*summation\n",
    "\n",
    "        if np.sum(np.fabs(prev_score - score)) <= threshold:\n",
    "            break        \n",
    "\n",
    "    sorted_index = np.flip(np.argsort(score),0)\n",
    "    sorted_index            \n",
    "    \n",
    "    for i in range(num):\n",
    "        print('{} {}'.format(id2vocab[sorted_index[i]], score[sorted_index[i]]))\n",
    "        \n",
    "        \n",
    "for i,url in enumerate(urls):\n",
    "    print(f'doc{i+1}')\n",
    "    tokens = get_news(url).split(' ')\n",
    "    vocab = list(set(get_news(url).split(' ')))\n",
    "    \n",
    "    text_rank(tokens,vocab,window_size=2,num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8f289",
   "metadata": {},
   "source": [
    "강사님 풀이(함수 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a01e1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d = 0.85\n",
    "threshold = 0.0001\n",
    "\n",
    "def get_scores(weight_edge, vocab_len):\n",
    "    scores = np.ones(vocab_len, dtype=np.float32)\n",
    "    \n",
    "    for it in range(MAX_ITERATIONS):\n",
    "        prev_score = np.copy(scores)\n",
    "\n",
    "        for i in range(vocab_len):\n",
    "            summation = (weight_edge[:, i] * prev_score).sum()\n",
    "            scores[i] = (1 - d) + d * summation\n",
    "\n",
    "        if np.sum(np.fabs(prev_score - scores)) <= threshold:\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "10829bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_edge(vocab, tokens, window_size = 2):\n",
    "    vocab_len = len(vocab)\n",
    "    weight_edge = np.zeros((vocab_len, vocab_len), dtype=np.float32)\n",
    "    covered_coocurrences = []\n",
    "\n",
    "    for window_start in range(0, len(tokens)-window_size + 1):\n",
    "        window = tokens[window_start : window_start + window_size]\n",
    "        for i in range(window_size - 1):\n",
    "            for j in range(i+1, window_size):\n",
    "\n",
    "                if window[i] in vocab and window[j] in vocab:\n",
    "                    index_of_i = window_start + i\n",
    "                    index_of_j = window_start + j\n",
    "\n",
    "                    if (index_of_i, index_of_j) not in covered_coocurrences:\n",
    "                        weight_edge[vocab2id[window[i]]][vocab2id[window[j]]] = 1\n",
    "                        weight_edge[vocab2id[window[j]]][vocab2id[window[i]]] = 1\n",
    "                        covered_coocurrences.append((index_of_i, index_of_j))\n",
    "\n",
    "    for i in range(vocab_len):\n",
    "        row_sum =  weight_edge[i].sum()\n",
    "        weight_edge[i] = weight_edge[i] / row_sum if row_sum > 0 else 0\n",
    "\n",
    "    return weight_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "febcdc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(news_article, node_pos= [\"NNG\", \"NNP\"]):\n",
    "    \n",
    "    vocab = [token[0] for token in mecab.pos(news_article) if token[1] in node_pos]\n",
    "    tokens = [token[0] for token in mecab.pos(news_article)]\n",
    "    \n",
    "    vocab = list(set(vocab))\n",
    "             \n",
    "    vocab2id = { vocab[i] : i for i in range(len(vocab))}\n",
    "    id2vocab = { i : vocab[i] for i in range(len(vocab))}\n",
    "    \n",
    "    vocab_len = len(vocab)\n",
    "    \n",
    "    return vocab, tokens, vocab2id, id2vocab, vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "80bee765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_result(scores, id2vocab):\n",
    "    sorted_index = np.flip(np.argsort(scores), 0)\n",
    "    \n",
    "    print(\"핵심키워드\")\n",
    "    for i in range(top_n):        \n",
    "        print(\"{} : {} \".format(id2vocab[sorted_index[i]], scores[sorted_index[i]]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c706d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "핵심키워드\n",
      "데이터 : 7.4343390464782715 \n",
      "활용 : 2.795384407043457 \n",
      "정보 : 2.712031126022339 \n",
      "플랫 : 2.188546657562256 \n",
      "경제 : 2.1860430240631104 \n",
      "\n",
      "핵심키워드\n",
      "대회 : 1.6575965881347656 \n",
      "한전 : 1.6287670135498047 \n",
      "서비스 : 1.5968517065048218 \n",
      "선정 : 1.4593236446380615 \n",
      "전문 : 1.4593236446380615 \n",
      "\n",
      "핵심키워드\n",
      "주행 : 4.062406063079834 \n",
      "데이터 : 3.5409369468688965 \n",
      "자율 : 2.3876304626464844 \n",
      "산업 : 1.9965639114379883 \n",
      "뉴스 : 1.9186471700668335 \n",
      "\n",
      "핵심키워드\n",
      "인구 : 3.6806445121765137 \n",
      "지역 : 2.842866897583008 \n",
      "간 : 2.663395404815674 \n",
      "이동 : 2.6485180854797363 \n",
      "데이터 : 2.024946451187134 \n",
      "\n",
      "핵심키워드\n",
      "데이터 : 4.3792619705200195 \n",
      "금융 : 4.025979995727539 \n",
      "중계 : 3.579340934753418 \n",
      "인프라 : 2.860461950302124 \n",
      "개발 : 2.756382703781128 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "\n",
    "def get_news(url):\n",
    "    headers = {\"user-agent\":\"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article = html.select_one(\"#articleBodyContents\").text\n",
    "    return article\n",
    "\n",
    "urls = [\"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=018&aid=0004430108\",\n",
    "\"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=001&aid=0011614790\",\n",
    "\"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=102&oid=014&aid=0004424362\",\n",
    "\"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=119&aid=0002402191\",\n",
    "\"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=030&aid=0002882728\"]\n",
    "\n",
    "docs = []\n",
    "top_n = 5\n",
    "\n",
    "for url in urls:\n",
    "    news_article = get_news(url)\n",
    "    \n",
    "    vocab, tokens, vocab2id, id2vocab, vocab_len = preprocess(news_article)\n",
    "    \n",
    "    weight_edge = get_weight_edge(vocab, tokens, window_size=2)\n",
    "    scores = get_scores(weight_edge, vocab_len)\n",
    "    \n",
    "    report_result(scores, id2vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a2d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
