{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c577cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a799dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yejin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7430c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e16c692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text,'html.parser')\n",
    "eng_news = soup.select('.article-body p')\n",
    "len(eng_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c99654e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is the present-day darling of the tech world. The current renaissance of Artificial Intelligence (AI) with its sister discipline Machine Learning (ML) has led every IT firm worth its salt to engineer some form of AI onto its platform, into its toolsets and throughout its software applications.',\n",
       " 'IBM CEO Ginni Rometty has already proclaimed that AI will change 100 percent of jobs over the next decade.',\n",
       " \"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\",\n",
       " 'When did it all go so right?',\n",
       " 'But AI used to be a fanciful notion mostly confined science fiction, so when did it all go right?',\n",
       " 'In recent years we’ve had some big changes in technology. Aside from the proliferation of mobile devices that has impacted us all, memory has become a lot cheaper, data storage has become a lot easier (in cloud, and elsewhere) and computer processing speeds have continued to outstrip previous records. With the power of quantum computing around the corner, is the AI renaissance simply a result of the coming together of these ‘tech ingredient’ forces?',\n",
       " \"“It isn't just massive compute power. There are important algorithmic changes that have been developed. Plus, it is much easier to gain access to more data in an Internet-connected world,” said Ted Dunning, CTO at data platform, AI and analytics company MapR. “All three aspects (compute, algorithms, data) combine to make todays machine learning possible. Also and quite frankly, a lot of applications only need data availability... we could have implemented them 25 years ago pretty easily if the data had been available and the output of the model could have been integrated back into the business flow.”\",\n",
       " 'So, in many ways, Dunning really heralds the modern era of the web as the key facilitator for the new age of AI. Information has become not just ubiquitous; it has also become easier to access and more accurately classified into structured, semi-structured and unstructured data in its rawest form.',\n",
       " 'Tuning AI towards life']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = [p.text for p in eng_news[1:10]]\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95287001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It is the present-day darling of the tech world. The current renaissance of Artificial Intelligence (AI) with its sister discipline Machine Learning (ML) has led every IT firm worth its salt to engineer some form of AI onto its platform, into its toolsets and throughout its software applications.\\nIBM CEO Ginni Rometty has already proclaimed that AI will change 100 percent of jobs over the next decade.\\nAnd yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\\nWhen did it all go so right?\\nBut AI used to be a fanciful notion mostly confined science fiction, so when did it all go right?\\nIn recent years we’ve had some big changes in technology. Aside from the proliferation of mobile devices that has impacted us all, memory has become a lot cheaper, data storage has become a lot easier (in cloud, and elsewhere) and computer processing speeds have continued to outstrip previous records. With the power of quantum computing around the corner, is the AI renaissance simply a result of the coming together of these ‘tech ingredient’ forces?\\n“It isn't just massive compute power. There are important algorithmic changes that have been developed. Plus, it is much easier to gain access to more data in an Internet-connected world,” said Ted Dunning, CTO at data platform, AI and analytics company MapR. “All three aspects (compute, algorithms, data) combine to make todays machine learning possible. Also and quite frankly, a lot of applications only need data availability... we could have implemented them 25 years ago pretty easily if the data had been available and the output of the model could have been integrated back into the business flow.”\\nSo, in many ways, Dunning really heralds the modern era of the web as the key facilitator for the new age of AI. Information has become not just ubiquitous; it has also become easier to access and more accurately classified into structured, semi-structured and unstructured data in its rawest form.\\nTuning AI towards life\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '\\n'.join(article)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f58f50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'the', 'present-day', 'darling', 'of', 'the', 'tech', 'world', '.', 'The', 'current', 'renaissance', 'of', 'Artificial', 'Intelligence', '(', 'AI', ')', 'with', 'its', 'sister', 'discipline', 'Machine', 'Learning', '(', 'ML', ')', 'has', 'led']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e578d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Good muffins cost $3.88\\nin New York. Please buy me\\ntwo of them.\\n\\nThanks.'\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8820303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "# 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "word_tokens = TreebankWordTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f625ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Yejin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99122439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tagged = pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0803efa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['muffins', 'New', 'York.', 'Please', 'Thanks']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = []\n",
    "for word,pos in pos_tagged:\n",
    "    if pos[0] == 'N':\n",
    "        nouns.append(word)\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0319f281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Good', 'JJ'),\n",
       " ('muffins', 'NNS'),\n",
       " ('cost', 'VBP'),\n",
       " ('$', '$'),\n",
       " ('3.88', 'CD'),\n",
       " ('in', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York.', 'NNP'),\n",
       " ('Please', 'NNP'),\n",
       " ('buy', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('two', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('them.', 'JJ'),\n",
       " ('Thanks', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3df29c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "397d4d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Yejin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Yejin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa870b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good muffins cost $3.88\\nin New York. Please buy me\\ntwo of them.\\n\\nThanks.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afeabcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a72dbfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Good', 'JJ'),\n",
       " ('muffins', 'NNS'),\n",
       " ('cost', 'VBP'),\n",
       " ('$', '$'),\n",
       " ('3.88', 'CD'),\n",
       " ('in', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Please', 'NNP'),\n",
       " ('buy', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('two', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('them', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Thanks', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tagged = pos_tag(word_tokens)\n",
    "pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f202eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,760.0,168.0\" width=\"760px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"6.31579%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Good</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.15789%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.47368%\" x=\"6.31579%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">muffins</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.0526%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.31579%\" x=\"15.7895%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cost</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.9474%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.15789%\" x=\"22.1053%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">$</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.6842%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.31579%\" x=\"25.2632%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">3.88</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.4211%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.21053%\" x=\"31.5789%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.6842%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"11.5789%\" x=\"35.7895%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">GPE</text></svg><svg width=\"45.4545%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">New</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.7273%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"54.5455%\" x=\"45.4545%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">York</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7273%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.5789%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.15789%\" x=\"47.3684%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"48.9474%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.42105%\" x=\"50.5263%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Please</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.7368%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.26316%\" x=\"58.9474%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">buy</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61.5789%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.26316%\" x=\"64.2105%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">me</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.8421%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.26316%\" x=\"69.4737%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">two</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.1053%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.21053%\" x=\"74.7368%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.8421%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.31579%\" x=\"78.9474%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">them</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.1053%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.15789%\" x=\"85.2632%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"86.8421%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.42105%\" x=\"88.4211%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Thanks</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"92.6316%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.15789%\" x=\"96.8421%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.4211%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Good', 'JJ')]), ('muffins', 'NNS'), ('cost', 'VBP'), ('$', '$'), ('3.88', 'CD'), ('in', 'IN'), Tree('GPE', [('New', 'NNP'), ('York', 'NNP')]), ('.', '.'), ('Please', 'NNP'), ('buy', 'VB'), ('me', 'PRP'), ('two', 'CD'), ('of', 'IN'), ('them', 'PRP'), ('.', '.'), ('Thanks', 'NNS'), ('.', '.')])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_tokens = ne_chunk(pos_tagged)\n",
    "ne_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "976d7a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Good/JJ)\n",
      "  muffins/NNS\n",
      "  cost/VBP\n",
      "  $/$\n",
      "  3.88/CD\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ./.\n",
      "  Please/NNP\n",
      "  buy/VB\n",
      "  me/PRP\n",
      "  two/CD\n",
      "  of/IN\n",
      "  them/PRP\n",
      "  ./.\n",
      "  Thanks/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(ne_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16af8549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20850f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.88, New York, two)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56791176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good ADJ\n",
      "muffins NOUN\n",
      "cost VERB\n",
      "$ SYM\n",
      "3.88 NUM\n",
      "\n",
      " SPACE\n",
      "in ADP\n",
      "New PROPN\n",
      "York PROPN\n",
      ". PUNCT\n",
      "Please INTJ\n",
      "buy VERB\n",
      "me PRON\n",
      "\n",
      " SPACE\n",
      "two NUM\n",
      "of ADP\n",
      "them PRON\n",
      ". PUNCT\n",
      "\n",
      "\n",
      " SPACE\n",
      "Thanks X\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "218dc2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(\"running ->\",ps.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07db0fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yejin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Yejin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52ebb383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believes -> belief\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "print(\"believes ->\",wn.lemmatize(\"believes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47850ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9025eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pos = ['IN','CC','UH','TO','MD','DT','VBZ','VBP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22303fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/'\n",
    "resp = requests.get(url)\n",
    "html = BeautifulSoup(resp.text,'html.parser')\n",
    "eng_news=html.select('.article-body p')\n",
    "article = [p.text for p in eng_news[1:10]]\n",
    "text = '\\n'.join(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f3b0d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('present-day', 'JJ'),\n",
       " ('darling', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('tech', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('current', 'JJ'),\n",
       " ('renaissance', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Artificial', 'JJ'),\n",
       " ('Intelligence', 'NNP'),\n",
       " ('(', '('),\n",
       " ('AI', 'NNP'),\n",
       " (')', ')'),\n",
       " ('with', 'IN'),\n",
       " ('its', 'PRP$'),\n",
       " ('sister', 'NN'),\n",
       " ('discipline', 'NN'),\n",
       " ('Machine', 'NNP'),\n",
       " ('Learning', 'NNP'),\n",
       " ('(', '('),\n",
       " ('ML', 'NNP'),\n",
       " (')', ')'),\n",
       " ('has', 'VBZ'),\n",
       " ('led', 'VBN'),\n",
       " ('every', 'DT'),\n",
       " ('IT', 'NNP'),\n",
       " ('firm', 'NN'),\n",
       " ('worth', 'VBD'),\n",
       " ('its', 'PRP$'),\n",
       " ('salt', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('engineer', 'VB'),\n",
       " ('some', 'DT'),\n",
       " ('form', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('AI', 'NNP'),\n",
       " ('onto', 'IN'),\n",
       " ('its', 'PRP$'),\n",
       " ('platform', 'NN'),\n",
       " (',', ','),\n",
       " ('into', 'IN'),\n",
       " ('its', 'PRP$'),\n",
       " ('toolsets', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('throughout', 'IN'),\n",
       " ('its', 'PRP$'),\n",
       " ('software', 'NN'),\n",
       " ('applications', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('IBM', 'NNP'),\n",
       " ('CEO', 'NNP'),\n",
       " ('Ginni', 'NNP'),\n",
       " ('Rometty', 'NNP'),\n",
       " ('has', 'VBZ'),\n",
       " ('already', 'RB'),\n",
       " ('proclaimed', 'VBN'),\n",
       " ('that', 'IN'),\n",
       " ('AI', 'NNP'),\n",
       " ('will', 'MD'),\n",
       " ('change', 'VB'),\n",
       " ('100', 'CD'),\n",
       " ('percent', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('jobs', 'NNS'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('next', 'JJ'),\n",
       " ('decade', 'NN'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('yes', 'UH'),\n",
       " (',', ','),\n",
       " ('she', 'PRP'),\n",
       " ('does', 'VBZ'),\n",
       " ('mean', 'VB'),\n",
       " ('everybody', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('job', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('yours', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('mine', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('onward', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('role', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('grain', 'NN'),\n",
       " ('farmers', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('Egypt', 'NNP'),\n",
       " (',', ','),\n",
       " ('pastry', 'NN'),\n",
       " ('chefs', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('Paris', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('dog', 'NN'),\n",
       " ('walkers', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('Oregon', 'NNP'),\n",
       " ('i.e', 'NN'),\n",
       " ('.', '.'),\n",
       " ('every', 'DT'),\n",
       " ('job', 'NN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('now', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('help', 'VB'),\n",
       " ('direct', 'VB'),\n",
       " ('all', 'DT'),\n",
       " ('workers', 'NNS'),\n",
       " ('’', 'VBP'),\n",
       " ('actions', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('behavior', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('degree', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('intelligence', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('comes', 'VBZ'),\n",
       " ('from', 'IN'),\n",
       " ('predictive', 'JJ'),\n",
       " ('analytics', 'NNS'),\n",
       " (',', ','),\n",
       " ('all', 'DT'),\n",
       " ('stemming', 'VBG'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('AI', 'NNP'),\n",
       " ('engines', 'VBZ'),\n",
       " ('we', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('now', 'RB'),\n",
       " ('increasingly', 'RB'),\n",
       " ('depend', 'VBP'),\n",
       " ('upon', 'NN'),\n",
       " ('.', '.'),\n",
       " ('When', 'WRB'),\n",
       " ('did', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('go', 'VBP'),\n",
       " ('so', 'RB'),\n",
       " ('right', 'RB'),\n",
       " ('?', '.'),\n",
       " ('But', 'CC'),\n",
       " ('AI', 'NNP'),\n",
       " ('used', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('fanciful', 'JJ'),\n",
       " ('notion', 'NN'),\n",
       " ('mostly', 'RB'),\n",
       " ('confined', 'VBD'),\n",
       " ('science', 'NN'),\n",
       " ('fiction', 'NN'),\n",
       " (',', ','),\n",
       " ('so', 'RB'),\n",
       " ('when', 'WRB'),\n",
       " ('did', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('go', 'VB'),\n",
       " ('right', 'RB'),\n",
       " ('?', '.'),\n",
       " ('In', 'IN'),\n",
       " ('recent', 'JJ'),\n",
       " ('years', 'NNS'),\n",
       " ('we', 'PRP'),\n",
       " ('’', 'VBP'),\n",
       " ('ve', 'NNS'),\n",
       " ('had', 'VBD'),\n",
       " ('some', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('changes', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('technology', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Aside', 'RB'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('proliferation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('mobile', 'JJ'),\n",
       " ('devices', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('has', 'VBZ'),\n",
       " ('impacted', 'VBN'),\n",
       " ('us', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " (',', ','),\n",
       " ('memory', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('become', 'VBN'),\n",
       " ('a', 'DT'),\n",
       " ('lot', 'NN'),\n",
       " ('cheaper', 'JJR'),\n",
       " (',', ','),\n",
       " ('data', 'NNS'),\n",
       " ('storage', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('become', 'VBN'),\n",
       " ('a', 'DT'),\n",
       " ('lot', 'NN'),\n",
       " ('easier', 'JJR'),\n",
       " ('(', '('),\n",
       " ('in', 'IN'),\n",
       " ('cloud', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('elsewhere', 'RB'),\n",
       " (')', ')'),\n",
       " ('and', 'CC'),\n",
       " ('computer', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('speeds', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('continued', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('outstrip', 'VB'),\n",
       " ('previous', 'JJ'),\n",
       " ('records', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('With', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('power', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('quantum', 'NN'),\n",
       " ('computing', 'VBG'),\n",
       " ('around', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('corner', 'NN'),\n",
       " (',', ','),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('AI', 'NNP'),\n",
       " ('renaissance', 'NN'),\n",
       " ('simply', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('result', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('coming', 'VBG'),\n",
       " ('together', 'RB'),\n",
       " ('of', 'IN'),\n",
       " ('these', 'DT'),\n",
       " ('‘', 'NNP'),\n",
       " ('tech', 'NN'),\n",
       " ('ingredient', 'NN'),\n",
       " ('’', 'NN'),\n",
       " ('forces', 'NNS'),\n",
       " ('?', '.'),\n",
       " ('“', 'VB'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " (\"n't\", 'RB'),\n",
       " ('just', 'RB'),\n",
       " ('massive', 'VB'),\n",
       " ('compute', 'NN'),\n",
       " ('power', 'NN'),\n",
       " ('.', '.'),\n",
       " ('There', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('important', 'JJ'),\n",
       " ('algorithmic', 'JJ'),\n",
       " ('changes', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('have', 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('developed', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('Plus', 'CC'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('much', 'RB'),\n",
       " ('easier', 'JJR'),\n",
       " ('to', 'TO'),\n",
       " ('gain', 'VB'),\n",
       " ('access', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('more', 'JJR'),\n",
       " ('data', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('Internet-connected', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " (',', ','),\n",
       " ('”', 'NNP'),\n",
       " ('said', 'VBD'),\n",
       " ('Ted', 'NNP'),\n",
       " ('Dunning', 'NNP'),\n",
       " (',', ','),\n",
       " ('CTO', 'NNP'),\n",
       " ('at', 'IN'),\n",
       " ('data', 'NNS'),\n",
       " ('platform', 'NN'),\n",
       " (',', ','),\n",
       " ('AI', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('analytics', 'NNS'),\n",
       " ('company', 'NN'),\n",
       " ('MapR', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('“', 'VB'),\n",
       " ('All', 'DT'),\n",
       " ('three', 'CD'),\n",
       " ('aspects', 'NNS'),\n",
       " ('(', '('),\n",
       " ('compute', 'NN'),\n",
       " (',', ','),\n",
       " ('algorithms', 'NN'),\n",
       " (',', ','),\n",
       " ('data', 'NNS'),\n",
       " (')', ')'),\n",
       " ('combine', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('make', 'VB'),\n",
       " ('todays', 'NNS'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('possible', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Also', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('quite', 'RB'),\n",
       " ('frankly', 'RB'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('lot', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('applications', 'NNS'),\n",
       " ('only', 'RB'),\n",
       " ('need', 'VBP'),\n",
       " ('data', 'NNS'),\n",
       " ('availability', 'NN'),\n",
       " ('...', ':'),\n",
       " ('we', 'PRP'),\n",
       " ('could', 'MD'),\n",
       " ('have', 'VB'),\n",
       " ('implemented', 'VBN'),\n",
       " ('them', 'PRP'),\n",
       " ('25', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " ('pretty', 'RB'),\n",
       " ('easily', 'RB'),\n",
       " ('if', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('data', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('been', 'VBN'),\n",
       " ('available', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('output', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('model', 'NN'),\n",
       " ('could', 'MD'),\n",
       " ('have', 'VB'),\n",
       " ('been', 'VBN'),\n",
       " ('integrated', 'VBN'),\n",
       " ('back', 'RB'),\n",
       " ('into', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('business', 'NN'),\n",
       " ('flow.', 'NN'),\n",
       " ('”', 'NNP'),\n",
       " ('So', 'NNP'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('many', 'JJ'),\n",
       " ('ways', 'NNS'),\n",
       " (',', ','),\n",
       " ('Dunning', 'VBG'),\n",
       " ('really', 'RB'),\n",
       " ('heralds', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('modern', 'JJ'),\n",
       " ('era', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('web', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('key', 'JJ'),\n",
       " ('facilitator', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('age', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('AI', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Information', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('become', 'VBN'),\n",
       " ('not', 'RB'),\n",
       " ('just', 'RB'),\n",
       " ('ubiquitous', 'JJ'),\n",
       " (';', ':'),\n",
       " ('it', 'PRP'),\n",
       " ('has', 'VBZ'),\n",
       " ('also', 'RB'),\n",
       " ('become', 'VBN'),\n",
       " ('easier', 'JJR'),\n",
       " ('to', 'TO'),\n",
       " ('access', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('more', 'RBR'),\n",
       " ('accurately', 'RB'),\n",
       " ('classified', 'VBN'),\n",
       " ('into', 'IN'),\n",
       " ('structured', 'VBN'),\n",
       " (',', ','),\n",
       " ('semi-structured', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('unstructured', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('its', 'PRP$'),\n",
       " ('rawest', 'JJS'),\n",
       " ('form', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Tuning', 'VBG'),\n",
       " ('AI', 'NNP'),\n",
       " ('towards', 'NNS'),\n",
       " ('life', 'NN')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "pos_tagged = pos_tag(word_tokens)\n",
    "pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f43ab15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for word,pos in pos_tagged:\n",
    "    if word.lower() not in stopwords.words('english') and pos not in stop_pos:\n",
    "        words.append(word)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfec9902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['present-day',\n",
       " 'darling',\n",
       " 'tech',\n",
       " 'world',\n",
       " '.',\n",
       " 'current',\n",
       " 'renaissance',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'sister',\n",
       " 'discipline',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " '(',\n",
       " 'ML',\n",
       " ')',\n",
       " 'led',\n",
       " 'firm',\n",
       " 'worth',\n",
       " 'salt',\n",
       " 'engineer',\n",
       " 'form',\n",
       " 'AI',\n",
       " 'platform',\n",
       " ',',\n",
       " 'toolsets',\n",
       " 'software',\n",
       " 'applications',\n",
       " '.',\n",
       " 'IBM',\n",
       " 'CEO',\n",
       " 'Ginni',\n",
       " 'Rometty',\n",
       " 'already',\n",
       " 'proclaimed',\n",
       " 'AI',\n",
       " 'change',\n",
       " '100',\n",
       " 'percent',\n",
       " 'jobs',\n",
       " 'next',\n",
       " 'decade',\n",
       " '.',\n",
       " ',',\n",
       " 'mean',\n",
       " 'everybody',\n",
       " \"'s\",\n",
       " 'job',\n",
       " 'mine',\n",
       " 'onward',\n",
       " 'role',\n",
       " 'grain',\n",
       " 'farmers',\n",
       " 'Egypt',\n",
       " ',',\n",
       " 'pastry',\n",
       " 'chefs',\n",
       " 'Paris',\n",
       " 'dog',\n",
       " 'walkers',\n",
       " 'Oregon',\n",
       " 'i.e',\n",
       " '.',\n",
       " 'job',\n",
       " '.',\n",
       " 'able',\n",
       " 'help',\n",
       " 'direct',\n",
       " 'workers',\n",
       " 'actions',\n",
       " 'behavior',\n",
       " 'new',\n",
       " 'degree',\n",
       " 'intelligence',\n",
       " 'predictive',\n",
       " 'analytics',\n",
       " ',',\n",
       " 'stemming',\n",
       " 'AI',\n",
       " 'increasingly',\n",
       " 'upon',\n",
       " '.',\n",
       " 'right',\n",
       " '?',\n",
       " 'AI',\n",
       " 'used',\n",
       " 'fanciful',\n",
       " 'notion',\n",
       " 'mostly',\n",
       " 'confined',\n",
       " 'science',\n",
       " 'fiction',\n",
       " ',',\n",
       " 'go',\n",
       " 'right',\n",
       " '?',\n",
       " 'recent',\n",
       " 'years',\n",
       " 'big',\n",
       " 'changes',\n",
       " 'technology',\n",
       " '.',\n",
       " 'Aside',\n",
       " 'proliferation',\n",
       " 'mobile',\n",
       " 'devices',\n",
       " 'impacted',\n",
       " 'us',\n",
       " ',',\n",
       " 'memory',\n",
       " 'become',\n",
       " 'lot',\n",
       " 'cheaper',\n",
       " ',',\n",
       " 'data',\n",
       " 'storage',\n",
       " 'become',\n",
       " 'lot',\n",
       " 'easier',\n",
       " '(',\n",
       " 'cloud',\n",
       " ',',\n",
       " 'elsewhere',\n",
       " ')',\n",
       " 'computer',\n",
       " 'processing',\n",
       " 'speeds',\n",
       " 'continued',\n",
       " 'outstrip',\n",
       " 'previous',\n",
       " 'records',\n",
       " '.',\n",
       " 'power',\n",
       " 'quantum',\n",
       " 'computing',\n",
       " 'corner',\n",
       " ',',\n",
       " 'AI',\n",
       " 'renaissance',\n",
       " 'simply',\n",
       " 'result',\n",
       " 'coming',\n",
       " 'together',\n",
       " '‘',\n",
       " 'tech',\n",
       " 'ingredient',\n",
       " '’',\n",
       " 'forces',\n",
       " '?',\n",
       " '“',\n",
       " \"n't\",\n",
       " 'massive',\n",
       " 'compute',\n",
       " 'power',\n",
       " '.',\n",
       " 'important',\n",
       " 'algorithmic',\n",
       " 'changes',\n",
       " 'developed',\n",
       " '.',\n",
       " ',',\n",
       " 'much',\n",
       " 'easier',\n",
       " 'gain',\n",
       " 'access',\n",
       " 'data',\n",
       " 'Internet-connected',\n",
       " 'world',\n",
       " ',',\n",
       " '”',\n",
       " 'said',\n",
       " 'Ted',\n",
       " 'Dunning',\n",
       " ',',\n",
       " 'CTO',\n",
       " 'data',\n",
       " 'platform',\n",
       " ',',\n",
       " 'AI',\n",
       " 'analytics',\n",
       " 'company',\n",
       " 'MapR',\n",
       " '.',\n",
       " '“',\n",
       " 'three',\n",
       " 'aspects',\n",
       " '(',\n",
       " 'compute',\n",
       " ',',\n",
       " 'algorithms',\n",
       " ',',\n",
       " 'data',\n",
       " ')',\n",
       " 'make',\n",
       " 'todays',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'possible',\n",
       " '.',\n",
       " 'Also',\n",
       " 'quite',\n",
       " 'frankly',\n",
       " ',',\n",
       " 'lot',\n",
       " 'applications',\n",
       " 'data',\n",
       " 'availability',\n",
       " '...',\n",
       " 'implemented',\n",
       " '25',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'pretty',\n",
       " 'easily',\n",
       " 'data',\n",
       " 'available',\n",
       " 'output',\n",
       " 'model',\n",
       " 'integrated',\n",
       " 'back',\n",
       " 'business',\n",
       " 'flow.',\n",
       " '”',\n",
       " ',',\n",
       " 'many',\n",
       " 'ways',\n",
       " ',',\n",
       " 'Dunning',\n",
       " 'really',\n",
       " 'modern',\n",
       " 'era',\n",
       " 'web',\n",
       " 'key',\n",
       " 'facilitator',\n",
       " 'new',\n",
       " 'age',\n",
       " 'AI',\n",
       " '.',\n",
       " 'Information',\n",
       " 'become',\n",
       " 'ubiquitous',\n",
       " ';',\n",
       " 'also',\n",
       " 'become',\n",
       " 'easier',\n",
       " 'access',\n",
       " 'accurately',\n",
       " 'classified',\n",
       " 'structured',\n",
       " ',',\n",
       " 'semi-structured',\n",
       " 'unstructured',\n",
       " 'data',\n",
       " 'rawest',\n",
       " 'form',\n",
       " '.',\n",
       " 'Tuning',\n",
       " 'AI',\n",
       " 'towards',\n",
       " 'life']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9688cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be6747ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19),\n",
       " ('.', 14),\n",
       " ('AI', 9),\n",
       " ('data', 7),\n",
       " ('(', 4),\n",
       " (')', 4),\n",
       " ('become', 4),\n",
       " ('?', 3),\n",
       " ('lot', 3),\n",
       " ('easier', 3),\n",
       " ('tech', 2),\n",
       " ('world', 2),\n",
       " ('renaissance', 2),\n",
       " ('form', 2),\n",
       " ('platform', 2),\n",
       " ('applications', 2),\n",
       " ('job', 2),\n",
       " ('new', 2),\n",
       " ('analytics', 2),\n",
       " ('right', 2),\n",
       " ('years', 2),\n",
       " ('changes', 2),\n",
       " ('power', 2),\n",
       " ('“', 2),\n",
       " ('compute', 2),\n",
       " ('access', 2),\n",
       " ('”', 2),\n",
       " ('Dunning', 2),\n",
       " ('present-day', 1),\n",
       " ('darling', 1),\n",
       " ('current', 1),\n",
       " ('Artificial', 1),\n",
       " ('Intelligence', 1),\n",
       " ('sister', 1),\n",
       " ('discipline', 1),\n",
       " ('Machine', 1),\n",
       " ('Learning', 1),\n",
       " ('ML', 1),\n",
       " ('led', 1),\n",
       " ('firm', 1),\n",
       " ('worth', 1),\n",
       " ('salt', 1),\n",
       " ('engineer', 1),\n",
       " ('toolsets', 1),\n",
       " ('software', 1),\n",
       " ('IBM', 1),\n",
       " ('CEO', 1),\n",
       " ('Ginni', 1),\n",
       " ('Rometty', 1),\n",
       " ('already', 1),\n",
       " ('proclaimed', 1),\n",
       " ('change', 1),\n",
       " ('100', 1),\n",
       " ('percent', 1),\n",
       " ('jobs', 1),\n",
       " ('next', 1),\n",
       " ('decade', 1),\n",
       " ('mean', 1),\n",
       " ('everybody', 1),\n",
       " (\"'s\", 1),\n",
       " ('mine', 1),\n",
       " ('onward', 1),\n",
       " ('role', 1),\n",
       " ('grain', 1),\n",
       " ('farmers', 1),\n",
       " ('Egypt', 1),\n",
       " ('pastry', 1),\n",
       " ('chefs', 1),\n",
       " ('Paris', 1),\n",
       " ('dog', 1),\n",
       " ('walkers', 1),\n",
       " ('Oregon', 1),\n",
       " ('i.e', 1),\n",
       " ('able', 1),\n",
       " ('help', 1),\n",
       " ('direct', 1),\n",
       " ('workers', 1),\n",
       " ('actions', 1),\n",
       " ('behavior', 1),\n",
       " ('degree', 1),\n",
       " ('intelligence', 1),\n",
       " ('predictive', 1),\n",
       " ('stemming', 1),\n",
       " ('increasingly', 1),\n",
       " ('upon', 1),\n",
       " ('used', 1),\n",
       " ('fanciful', 1),\n",
       " ('notion', 1),\n",
       " ('mostly', 1),\n",
       " ('confined', 1),\n",
       " ('science', 1),\n",
       " ('fiction', 1),\n",
       " ('go', 1),\n",
       " ('recent', 1),\n",
       " ('big', 1),\n",
       " ('technology', 1),\n",
       " ('Aside', 1),\n",
       " ('proliferation', 1),\n",
       " ('mobile', 1),\n",
       " ('devices', 1),\n",
       " ('impacted', 1),\n",
       " ('us', 1),\n",
       " ('memory', 1),\n",
       " ('cheaper', 1),\n",
       " ('storage', 1),\n",
       " ('cloud', 1),\n",
       " ('elsewhere', 1),\n",
       " ('computer', 1),\n",
       " ('processing', 1),\n",
       " ('speeds', 1),\n",
       " ('continued', 1),\n",
       " ('outstrip', 1),\n",
       " ('previous', 1),\n",
       " ('records', 1),\n",
       " ('quantum', 1),\n",
       " ('computing', 1),\n",
       " ('corner', 1),\n",
       " ('simply', 1),\n",
       " ('result', 1),\n",
       " ('coming', 1),\n",
       " ('together', 1),\n",
       " ('‘', 1),\n",
       " ('ingredient', 1),\n",
       " ('’', 1),\n",
       " ('forces', 1),\n",
       " (\"n't\", 1),\n",
       " ('massive', 1),\n",
       " ('important', 1),\n",
       " ('algorithmic', 1),\n",
       " ('developed', 1),\n",
       " ('much', 1),\n",
       " ('gain', 1),\n",
       " ('Internet-connected', 1),\n",
       " ('said', 1),\n",
       " ('Ted', 1),\n",
       " ('CTO', 1),\n",
       " ('company', 1),\n",
       " ('MapR', 1),\n",
       " ('three', 1),\n",
       " ('aspects', 1),\n",
       " ('algorithms', 1),\n",
       " ('make', 1),\n",
       " ('todays', 1),\n",
       " ('machine', 1),\n",
       " ('learning', 1),\n",
       " ('possible', 1),\n",
       " ('Also', 1),\n",
       " ('quite', 1),\n",
       " ('frankly', 1),\n",
       " ('availability', 1),\n",
       " ('...', 1),\n",
       " ('implemented', 1),\n",
       " ('25', 1),\n",
       " ('ago', 1),\n",
       " ('pretty', 1),\n",
       " ('easily', 1),\n",
       " ('available', 1),\n",
       " ('output', 1),\n",
       " ('model', 1),\n",
       " ('integrated', 1),\n",
       " ('back', 1),\n",
       " ('business', 1),\n",
       " ('flow.', 1),\n",
       " ('many', 1),\n",
       " ('ways', 1),\n",
       " ('really', 1),\n",
       " ('modern', 1),\n",
       " ('era', 1),\n",
       " ('web', 1),\n",
       " ('key', 1),\n",
       " ('facilitator', 1),\n",
       " ('age', 1),\n",
       " ('Information', 1),\n",
       " ('ubiquitous', 1),\n",
       " (';', 1),\n",
       " ('also', 1),\n",
       " ('accurately', 1),\n",
       " ('classified', 1),\n",
       " ('structured', 1),\n",
       " ('semi-structured', 1),\n",
       " ('unstructured', 1),\n",
       " ('rawest', 1),\n",
       " ('Tuning', 1),\n",
       " ('towards', 1),\n",
       " ('life', 1)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(words).most_common()\n",
    "#데이터 개수 세기\n",
    "#.most_common(): 데이터 개수 많은 순으로 배열 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7e309e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = []\n",
    "for word in words:\n",
    "    stemmed_words.append(ps.stem(word))\n",
    "    \n",
    "lemmed_words = []\n",
    "for word in words:\n",
    "    lemmed_words.append(wn.lemmatize(word))\n",
    "#일반적으로 stem보다 lem이 더 정확하게 어근 단어를 찾아줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e70a7848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19),\n",
       " ('.', 14),\n",
       " ('AI', 9),\n",
       " ('data', 7),\n",
       " ('(', 4),\n",
       " (')', 4),\n",
       " ('become', 4),\n",
       " ('change', 3),\n",
       " ('job', 3),\n",
       " ('?', 3),\n",
       " ('lot', 3),\n",
       " ('easier', 3),\n",
       " ('tech', 2),\n",
       " ('world', 2),\n",
       " ('renaissance', 2),\n",
       " ('form', 2),\n",
       " ('platform', 2),\n",
       " ('application', 2),\n",
       " ('new', 2),\n",
       " ('analytics', 2),\n",
       " ('right', 2),\n",
       " ('year', 2),\n",
       " ('power', 2),\n",
       " ('“', 2),\n",
       " ('compute', 2),\n",
       " ('access', 2),\n",
       " ('”', 2),\n",
       " ('Dunning', 2),\n",
       " ('present-day', 1),\n",
       " ('darling', 1),\n",
       " ('current', 1),\n",
       " ('Artificial', 1),\n",
       " ('Intelligence', 1),\n",
       " ('sister', 1),\n",
       " ('discipline', 1),\n",
       " ('Machine', 1),\n",
       " ('Learning', 1),\n",
       " ('ML', 1),\n",
       " ('led', 1),\n",
       " ('firm', 1),\n",
       " ('worth', 1),\n",
       " ('salt', 1),\n",
       " ('engineer', 1),\n",
       " ('toolsets', 1),\n",
       " ('software', 1),\n",
       " ('IBM', 1),\n",
       " ('CEO', 1),\n",
       " ('Ginni', 1),\n",
       " ('Rometty', 1),\n",
       " ('already', 1),\n",
       " ('proclaimed', 1),\n",
       " ('100', 1),\n",
       " ('percent', 1),\n",
       " ('next', 1),\n",
       " ('decade', 1),\n",
       " ('mean', 1),\n",
       " ('everybody', 1),\n",
       " (\"'s\", 1),\n",
       " ('mine', 1),\n",
       " ('onward', 1),\n",
       " ('role', 1),\n",
       " ('grain', 1),\n",
       " ('farmer', 1),\n",
       " ('Egypt', 1),\n",
       " ('pastry', 1),\n",
       " ('chef', 1),\n",
       " ('Paris', 1),\n",
       " ('dog', 1),\n",
       " ('walker', 1),\n",
       " ('Oregon', 1),\n",
       " ('i.e', 1),\n",
       " ('able', 1),\n",
       " ('help', 1),\n",
       " ('direct', 1),\n",
       " ('worker', 1),\n",
       " ('action', 1),\n",
       " ('behavior', 1),\n",
       " ('degree', 1),\n",
       " ('intelligence', 1),\n",
       " ('predictive', 1),\n",
       " ('stemming', 1),\n",
       " ('increasingly', 1),\n",
       " ('upon', 1),\n",
       " ('used', 1),\n",
       " ('fanciful', 1),\n",
       " ('notion', 1),\n",
       " ('mostly', 1),\n",
       " ('confined', 1),\n",
       " ('science', 1),\n",
       " ('fiction', 1),\n",
       " ('go', 1),\n",
       " ('recent', 1),\n",
       " ('big', 1),\n",
       " ('technology', 1),\n",
       " ('Aside', 1),\n",
       " ('proliferation', 1),\n",
       " ('mobile', 1),\n",
       " ('device', 1),\n",
       " ('impacted', 1),\n",
       " ('u', 1),\n",
       " ('memory', 1),\n",
       " ('cheaper', 1),\n",
       " ('storage', 1),\n",
       " ('cloud', 1),\n",
       " ('elsewhere', 1),\n",
       " ('computer', 1),\n",
       " ('processing', 1),\n",
       " ('speed', 1),\n",
       " ('continued', 1),\n",
       " ('outstrip', 1),\n",
       " ('previous', 1),\n",
       " ('record', 1),\n",
       " ('quantum', 1),\n",
       " ('computing', 1),\n",
       " ('corner', 1),\n",
       " ('simply', 1),\n",
       " ('result', 1),\n",
       " ('coming', 1),\n",
       " ('together', 1),\n",
       " ('‘', 1),\n",
       " ('ingredient', 1),\n",
       " ('’', 1),\n",
       " ('force', 1),\n",
       " (\"n't\", 1),\n",
       " ('massive', 1),\n",
       " ('important', 1),\n",
       " ('algorithmic', 1),\n",
       " ('developed', 1),\n",
       " ('much', 1),\n",
       " ('gain', 1),\n",
       " ('Internet-connected', 1),\n",
       " ('said', 1),\n",
       " ('Ted', 1),\n",
       " ('CTO', 1),\n",
       " ('company', 1),\n",
       " ('MapR', 1),\n",
       " ('three', 1),\n",
       " ('aspect', 1),\n",
       " ('algorithm', 1),\n",
       " ('make', 1),\n",
       " ('today', 1),\n",
       " ('machine', 1),\n",
       " ('learning', 1),\n",
       " ('possible', 1),\n",
       " ('Also', 1),\n",
       " ('quite', 1),\n",
       " ('frankly', 1),\n",
       " ('availability', 1),\n",
       " ('...', 1),\n",
       " ('implemented', 1),\n",
       " ('25', 1),\n",
       " ('ago', 1),\n",
       " ('pretty', 1),\n",
       " ('easily', 1),\n",
       " ('available', 1),\n",
       " ('output', 1),\n",
       " ('model', 1),\n",
       " ('integrated', 1),\n",
       " ('back', 1),\n",
       " ('business', 1),\n",
       " ('flow.', 1),\n",
       " ('many', 1),\n",
       " ('way', 1),\n",
       " ('really', 1),\n",
       " ('modern', 1),\n",
       " ('era', 1),\n",
       " ('web', 1),\n",
       " ('key', 1),\n",
       " ('facilitator', 1),\n",
       " ('age', 1),\n",
       " ('Information', 1),\n",
       " ('ubiquitous', 1),\n",
       " (';', 1),\n",
       " ('also', 1),\n",
       " ('accurately', 1),\n",
       " ('classified', 1),\n",
       " ('structured', 1),\n",
       " ('semi-structured', 1),\n",
       " ('unstructured', 1),\n",
       " ('rawest', 1),\n",
       " ('Tuning', 1),\n",
       " ('towards', 1),\n",
       " ('life', 1)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(lemmed_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cdffed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt\n",
    "hannanum=Hannanum()\n",
    "kkma=Kkma()\n",
    "komoran=Komoran()\n",
    "mecab=Mecab()\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e8bd221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f27d846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://news.naver.com/main/read.naver?mode=LS2D&mid=shm&sid1=101&sid2=258&oid=015&aid=0004655938\"\n",
    "headers = {'user-agent':\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\"}\n",
    "response=requests.get(url,headers=headers)\n",
    "html = BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "651825d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n오늘은 중국주식에 투자하는 분이라면 한 번 쯤은 들어보셨을 법한, 하지만 너무 복잡하고 낯설어서 잘 이해가 가지 않는 얘기를 좀 해보려고 합니다. 중국 기업이 해외에 상장할 때 쓰는 독특한 구조인 가변이익실체 얘깁니다. 영어로는 variable interest entity, 줄여서 VIE라고 하는데요. 그동안 베이징나우나 기사를 통해서 몇 번 소개해 드리긴 했는데, 최근 중국이 새로 규정도 만들고 해서 이번 기회에 종합 정리를 좀 해볼까 합니다.사진=REUTERS 지분 관계 없이 계약으로만 기업을 지배지난해 많은 투자자들을 울린 디디추싱을 예로 들어서 설명을 좀 해보겠습니다. 가변이익실체에는 보통 4개의 회사가 등장합니다. 벌써 복잡해지기 시작하죠?1번, 편의상 A라고 하겠습니다. A는 중국에서 사업하는 실체 회사입니다. 디디추싱에 대입해 보면, 밑에 중국 전역에서 디디추싱이라는 이름으로 승차호출 서비스를 하고 있는 여러 자회사들을 거느리고 있는 지주회사인 베이징샤오쥐라고 하겠습니다.2번은 A를 지배하는 페이퍼컴퍼니인 B입니다. B는 A의 주식을 한 주도 갖지 않습니다. 그냥 B가 A를 지배한다는 계약만 맺습니다. 이 계약이 가변이익실체 구조의 핵심입니다. 물론 A와 B의 실제 주인은 똑같습니다. 그러니 A가 계약을 파기하거나 할 우려는 매우 적습니다. 디디추싱에 대입하면 베이징디디라는 페이퍼컴퍼니입니다.여기까지는 중국 본토 내 기업들이고요, 다음은 본토 밖 기업들입니다. 3번은 B를 100% 보유하는 홍콩 페이퍼컴퍼니입니다. C라고 하겠습니다. 디디추싱에선 홍콩샤오쥐입니다. 아까 중국 본토 지주회사가 베이징샤오쥐라고 했는데요, 홍콩 페이퍼컴퍼니는 홍콩샤오쥐입니다. 샤오쥐는 귤인데요, 디디추싱은 자전거 공유 브랜드에는 칭쥐, 청귤이라는 이름을 붙였습니다. 귤을 여기저기에 갖다 붙이고 있습니다.4번이자 D는 홍콩 페이퍼컴퍼니를 100% 보유하는 해외 페이퍼컴퍼니입니다. 보통 조세회피처인 영국령 케이만이나 버진아일랜드에 세웁니다. 디디추싱은 이걸 디디글로벌이라는 이름을 붙였습니다. 그리고 미국 뉴욕에 상장한 게 이 디디글로벌입니다. 미국 홍콩엔 페이퍼컴퍼니가 상장거꾸로 정리해 보면 D는 케이만군도에 설립된 페이퍼컴퍼니이고, 이게 뉴욕에 상장을 했습니다. 뉴욕증시에서 디디추싱 주식을 산 사람은 중국에서 실제로 사업을 하는 디디추싱 내지는 지주회사인 베이징샤오쥐의 주식을 산 게 아니라 페이퍼컴퍼니 주식을 산 겁니다. 하지만 그 실체는 중국 디디추싱하고 다를 바가 없긴 합니다. 디디추싱이 올리는 실적을 페이퍼컴퍼니인 디디글로벌이 미국 증권감독위원회에 자기 실적이라고 보고하고요, 미국도 이런 방식을 인정합니다.미국에 상장한 페이퍼컴퍼니가 홍콩 페이퍼컴퍼니인 C를 100% 보유하고요, 이 C가 중국 본토 페이퍼컴퍼니인 B를 100% 보유합니다. 그리고 B는 실제로 사업을 하는 실체인 A와 지배 계약을 맺고 있습니다. D가 미국에서 IPO를 하면서 조달한 자금은 A에게 대출을 해줍니다. A는 그런 식으로 자금을 확보하는 거고요.사족을 하나 달자면 우리가 말하는 가변이익실체, VIE는 중국에서 사업하는 실체인 A를 말합니다. 중요한 건 아닌데요, 많은 분들이 VIE를 미국에 상장하는 페이퍼컴퍼니로 생각하셔서 참고로 말씀드립니다. 이게 중요한 건 아니고요, 중요한 건 VIE를 활용하는 상장 구조를 이해하는 거라고 하겠습니다. 외국인의 자국 기업 소유 제한하는 중국이런 복잡한 구조가 왜 나왔을까요. 이유는 간단합니다. 특정 산업군에서 외국인이 중국 기업 지분을 보유하는 것을 원칙적으로 막아놨기 때문입니다. 스포츠 문화 교육 과학기술연구 이런 부문이고요, 또 대표적으로 인터넷 산업이 있습니다. 또 본토증시는 상장 요건이 매우 까다롭습니다. 그래서 페이퍼컴퍼니를 만들어서 뉴욕이나 홍콩에 상장을 하는 거죠.물론 당국의 허가를 받으면 외국인도 해당 산업군의 기업 지분을 가질 수 있습니다. 외국 기업과 합작사도 많고요. 하지만 주식시장에선 수시로 소유권이 바뀌죠. 그때마다 허가를 받을 수도 없고요. 그러니까 페이퍼컴퍼니를 외국에 상장시켜서 외국인이 그 주식을 사도록 하면, 외국인이 중국 본토에 있는 가변이익실체의 주식을 갖는 건 아니니까 중국 법 위반도 아니게 되는 겁니다.중국 내에서 증권 계좌를 만든 외국인은 중국 본토 주식을 살 수 있습니다. 외국인도 중국에서 세금을 내고 하는 몇 가지 요건을 갖추면 계좌를 만들 수 있고요. 또 홍콩거래소 교차매매시스템인 선강퉁과 후강퉁을 통해서 중국 본토주식을 살 수도 있습니다. 그런 면에서 외국인이 중국 기업을 소유하지 못하게 하는 규제는 투자 측면에서는 의미가 크게 줄어들기도 했습니다. 빅테크 압박 수단으로 VIE 규제?중국 당국은 이제까지 이런 가변이익실체 구조를 묵인해 왔습니다. 관련 규정도 없었죠. 기업들이 해외에서 달러를 조달해서 중국으로 가져온다는 면에서 굳이 막을 필요성도 못 느꼈던 것 같습니다. 하지만 2020년 하반기부터 빅테크 압박을 본격화하면서 가변이익실체도 규제할 것이란 분석이 제기됐습니다.알리바바나 디디추싱 같은 기업들이 중국에서 사업하면서 돈을 버는데, 상장은 해외에서 해서 주가가 오르면 중국인들에게는 주가 상승 혜택이 돌아가지 않는 것 아니냐, 이런 지적이 나온 겁니다. 한국에서도 쿠팡이 뉴욕에 상장할 때 일부 정치인들이 국부 유출 논란을 제기했는데, 중국에서도 비슷한 논란이 일어난 거죠. 정치인 수준이 중국이나 한국이나 비슷하다고 하겠습니다.주로 미국 외신들은 중국이 가변이익실체 해외 상장을 금지할 거다, 그렇게 되면 미국에 상장한 중국 기업들이 다 상장폐지가 될 거다, 투자자들은 주의해야 한다 이런 식의 기사를 계속 써왔습니다.중국 증권감독위원회가 작년 말에 공식 입장을 내놨습니다. 요약하면 \"국내 법을 준수하는 기업은 VIE 구조를 활용해서 해외에 상장할 수 있다\"입니다.증감위가 제시한 조건들은 다음과 같습니다.국내 법을 준수할 것, 국가 안보를 위협하지 않을 것, 주요 주주나 경영진이 최근 3년 간 법적 조사를 받거나 기소된 적이 없을 것, 그리고 핵심 자산과 기술의 해외 유출 가능성이 없을 것 이렇게입니다.증감위는 또 기존에 VIE 방식으로 상장한 기업들에는 이 규정을 적용하지 않겠다고도 했습니다.  해외 상장 사실상 금지그런데 중국 당국이 내놓은 조건들을 보면 이게 앞으로 VIE 구조를 통한 해외 상장을 허가해 주겠다는 건지 아닌 건지 좀 헷갈리긴 합니다. 저는 중국이 제도와 규정은 만들어 놨지만 실제로 해외 상장을 허가해 주지는 않는 상태를 당분간 유지할 것으로 보고 있습니다. 요즘 같은 분위기에서 굳이 해외에 상장하려는 기업도 별로 없을 것이고요.또 해외 상장 승인 권한을 주무부처가 갖도록 했는데, 주무부처 장관이 책임지고 승인을 해주는 경우도 거의 없을 것으로 보입니다.중국은 또 기업이 해외 상장을 승인받는다고 해도 외국자본은 대상 기업의 경영에 직접 참여할 수 없도록 했고요, 단일 외국자본은 대상 기업 지분을 10% 이내에서만 보유할 수 있고 전체 외국자본 지분도 30%를 넘어서는 안 된다는 규정도 추가했습니다. 홍콩만 열어줘최근에 나온 중국 기업의 해외 상장 관련 규정에서 주목할 부분은 홍콩입니다. 중국은 작년에 디디추싱이 당국이 말리는데도 뉴욕 상장을 강행하자 국가안보 카드를 들고 나왔죠. 사용자 100만명 이상인 인터넷 기업이 해외에 상장하려면 국가보안 심사를 받도록 했습니다.중국 당국은 그동안 이 규정이 홍콩 상장에도 적용된다고 해왔습니다. 그런데 지난 4일 내놓은 새 규정을 보면 작지만 큰 변경이 있었습니다. 그 이전에는 경외 상장이라는 표현을 했는데 이번에는 해외 상장이라는 말로 바꾼 겁니다. 경외는 국경 밖이라는 의미인데, 중국에서는 중국 본토와 홍콩마카오를 구분할 때 경외라는 말을 씁니다. 그리고 해외는 중국본토와 홍콩마카오를 포함한 큰 의미의 중국과 나머지 외국을 구분할 때 씁니다. 결국 홍콩 상장에는 국가보안 심사를 면제해 주겠다는 얘깁니다.이건 중국이 홍콩을 아시아의 자본시장 허브로 계속 키우겠다는 방침을 보여주기도 하고요, 또 한편으로는 중국 공산당과 정부가 홍콩 자본시장을 컨트롤할 수 있다는 자신감을 보여주는 장면이라고도 하겠습니다.새 규정은 뉴욕에서 홍콩으로 이전을 추진하고 있는 디디추싱에게는 희소식이라고 하겠습니다. 국가안보 심사라는 높은 허들을 넘지 않아도 되기 때문이죠. 또 미국이 중국 기업에 대한 회계 감사를 강화할 방침이어서 뉴욕에 상장된 중국 기업들 다수가 홍콩 2차 상장을 추진하고 있는데, 이 과정도 좀 속도가 날 것으로 보입니다. 올해 50개 넘는 기업이 홍콩에 2차 상장을 할 것이란 얘기도 있는데요, 홍콩 증시도 이걸 계기로 활기를 좀 찾을 것으로 기대됩니다. 투자 대상 선별 필요가변이익실체 관련 규정도 만들고, 해외 상장 절차도 정비하고, 홍콩은 예외로 해주고 한 이런 일련의 정책들을 보면 결국 중국이 미국과의 갈등과 경쟁에서 비롯되는 리스크를 최소화하겠다는 방침이 밑바닥에 깔려 있다고 하겠습니다.내수 장사로만 먹고사는 인터넷 기업들은 앞으로 계속 찬밥 대접을 받을 것이고요, 반도체나 인공지능, 신재생에너지 같은 핵심 과학기술과 기후변화 대응이라는 키워드를 갖고 있는 기업들은 전략적으로 지원을 이어갈 겁니다. 중국 주식 투자도 이런 기업들로 포커스를 맞추는 게 바람직해 보입니다.베이징=강현우 특파원 hkang@hankyung.com\\n\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = html.select_one(\"#articleBodyContents\").text\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d6ddf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(article)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb0a8710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오늘은 중국주식에 투자하는 분이라면 한 번 쯤은 들어보셨을 법한, 하지만 너무 복잡하고 낯설어서 잘 이해가 가지 않는 얘기를 좀 해보려고 합니다.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sentences[0].strip()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5a9f452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘',\n",
       " '은',\n",
       " '중국',\n",
       " '주식',\n",
       " '에',\n",
       " '투자',\n",
       " '하',\n",
       " '는',\n",
       " '분',\n",
       " '이',\n",
       " '라면',\n",
       " '한',\n",
       " '번',\n",
       " '쯤',\n",
       " '은',\n",
       " '들',\n",
       " '어',\n",
       " '보',\n",
       " '시',\n",
       " '었',\n",
       " '을',\n",
       " '법',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " ',',\n",
       " '하지만',\n",
       " '너무',\n",
       " '복잡',\n",
       " '하',\n",
       " '고',\n",
       " '낯설',\n",
       " '어서',\n",
       " '잘',\n",
       " '이해',\n",
       " '가',\n",
       " '가지',\n",
       " '않',\n",
       " '는',\n",
       " '얘기',\n",
       " '를',\n",
       " '좀',\n",
       " '해보',\n",
       " '려고',\n",
       " '하',\n",
       " 'ㅂ니다',\n",
       " '.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "komoran.morphs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bda20276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘',\n",
       " '은',\n",
       " '중국주식',\n",
       " '에',\n",
       " '투자하',\n",
       " '는',\n",
       " '분',\n",
       " '이',\n",
       " '라면',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '번',\n",
       " '쯤',\n",
       " '은',\n",
       " '들',\n",
       " '어',\n",
       " '보',\n",
       " '셨을',\n",
       " '법',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " ',',\n",
       " '하',\n",
       " '지만',\n",
       " '너무',\n",
       " '복잡',\n",
       " '하고',\n",
       " '낯설',\n",
       " '어서',\n",
       " '잘',\n",
       " '이해',\n",
       " '가',\n",
       " '가지',\n",
       " '않',\n",
       " '는',\n",
       " '얘',\n",
       " '이',\n",
       " '기',\n",
       " '를',\n",
       " '좀',\n",
       " '하',\n",
       " '어',\n",
       " '보',\n",
       " '려고',\n",
       " '하',\n",
       " 'ㅂ니다',\n",
       " '.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannanum.morphs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc2edce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘',\n",
       " '은',\n",
       " '중국',\n",
       " '주식',\n",
       " '에',\n",
       " '투자',\n",
       " '하는',\n",
       " '분이라면',\n",
       " '한',\n",
       " '번',\n",
       " '쯤',\n",
       " '은',\n",
       " '들어',\n",
       " '보셨을',\n",
       " '법',\n",
       " '한',\n",
       " ',',\n",
       " '하지만',\n",
       " '너무',\n",
       " '복잡하고',\n",
       " '낯설어서',\n",
       " '잘',\n",
       " '이해',\n",
       " '가',\n",
       " '가지',\n",
       " '않는',\n",
       " '얘기',\n",
       " '를',\n",
       " '좀',\n",
       " '해보려고',\n",
       " '합니다',\n",
       " '.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32186aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘',\n",
       " '은',\n",
       " '중국',\n",
       " '주식',\n",
       " '에',\n",
       " '투자',\n",
       " '하',\n",
       " '는',\n",
       " '분',\n",
       " '이',\n",
       " '라면',\n",
       " '한',\n",
       " '번',\n",
       " '쯤',\n",
       " '은',\n",
       " '들어보',\n",
       " '시',\n",
       " '었',\n",
       " '을',\n",
       " '법',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " ',',\n",
       " '하지만',\n",
       " '너무',\n",
       " '복잡',\n",
       " '하',\n",
       " '고',\n",
       " '낯설',\n",
       " '어서',\n",
       " '잘',\n",
       " '이해',\n",
       " '가',\n",
       " '가',\n",
       " '지',\n",
       " '않',\n",
       " '는',\n",
       " '얘기',\n",
       " '를',\n",
       " '좀',\n",
       " '해보',\n",
       " '려고',\n",
       " '하',\n",
       " 'ㅂ니다',\n",
       " '.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.morphs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72ee7ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘',\n",
       " '은',\n",
       " '중국',\n",
       " '주식',\n",
       " '에',\n",
       " '투자',\n",
       " '하',\n",
       " '는',\n",
       " '분',\n",
       " '이',\n",
       " '라면',\n",
       " '한',\n",
       " '번',\n",
       " '쯤',\n",
       " '은',\n",
       " '들',\n",
       " '어',\n",
       " '보',\n",
       " '셨',\n",
       " '을',\n",
       " '법',\n",
       " '한',\n",
       " ',',\n",
       " '하지만',\n",
       " '너무',\n",
       " '복잡',\n",
       " '하',\n",
       " '고',\n",
       " '낯설',\n",
       " '어서',\n",
       " '잘',\n",
       " '이해',\n",
       " '가',\n",
       " '가',\n",
       " '지',\n",
       " '않',\n",
       " '는',\n",
       " '얘기',\n",
       " '를',\n",
       " '좀',\n",
       " '해',\n",
       " '보',\n",
       " '려고',\n",
       " '합니다',\n",
       " '.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab.morphs(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9f84165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'NNG'),\n",
       " ('은', 'JX'),\n",
       " ('중국', 'NNP'),\n",
       " ('주식', 'NNP'),\n",
       " ('에', 'JKB'),\n",
       " ('투자', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('는', 'ETM'),\n",
       " ('분', 'NNB'),\n",
       " ('이', 'VCP'),\n",
       " ('라면', 'EC'),\n",
       " ('한', 'MM'),\n",
       " ('번', 'NNB'),\n",
       " ('쯤', 'NNB'),\n",
       " ('은', 'JX'),\n",
       " ('들', 'VV'),\n",
       " ('어', 'EC'),\n",
       " ('보', 'VX'),\n",
       " ('시', 'EP'),\n",
       " ('었', 'EP'),\n",
       " ('을', 'ETM'),\n",
       " ('법', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('ㄴ', 'ETM'),\n",
       " (',', 'SP'),\n",
       " ('하지만', 'MAJ'),\n",
       " ('너무', 'MAG'),\n",
       " ('복잡', 'XR'),\n",
       " ('하', 'XSA'),\n",
       " ('고', 'EC'),\n",
       " ('낯설', 'VA'),\n",
       " ('어서', 'EC'),\n",
       " ('잘', 'MAG'),\n",
       " ('이해', 'NNG'),\n",
       " ('가', 'JKS'),\n",
       " ('가지', 'NNB'),\n",
       " ('않', 'VX'),\n",
       " ('는', 'ETM'),\n",
       " ('얘기', 'NNG'),\n",
       " ('를', 'JKO'),\n",
       " ('좀', 'MAG'),\n",
       " ('해보', 'VV'),\n",
       " ('려고', 'EC'),\n",
       " ('하', 'VX'),\n",
       " ('ㅂ니다', 'EF'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "komoran.pos(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71985409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'NNG'),\n",
       " ('은', 'JX'),\n",
       " ('중국', 'NNP'),\n",
       " ('주식', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('투자', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('는', 'ETM'),\n",
       " ('분', 'NNB'),\n",
       " ('이', 'VCP'),\n",
       " ('라면', 'EC'),\n",
       " ('한', 'MM'),\n",
       " ('번', 'NNBC'),\n",
       " ('쯤', 'NNG'),\n",
       " ('은', 'JX'),\n",
       " ('들', 'VV'),\n",
       " ('어', 'EC'),\n",
       " ('보', 'VX'),\n",
       " ('셨', 'EP+EP'),\n",
       " ('을', 'ETM'),\n",
       " ('법', 'NNG'),\n",
       " ('한', 'XSA+ETM'),\n",
       " (',', 'SC'),\n",
       " ('하지만', 'MAJ'),\n",
       " ('너무', 'MAG'),\n",
       " ('복잡', 'XR'),\n",
       " ('하', 'XSA'),\n",
       " ('고', 'EC'),\n",
       " ('낯설', 'VA'),\n",
       " ('어서', 'EC'),\n",
       " ('잘', 'MAG'),\n",
       " ('이해', 'NNG'),\n",
       " ('가', 'JKS'),\n",
       " ('가', 'VV'),\n",
       " ('지', 'EC'),\n",
       " ('않', 'VX'),\n",
       " ('는', 'ETM'),\n",
       " ('얘기', 'NNG'),\n",
       " ('를', 'JKO'),\n",
       " ('좀', 'MAG'),\n",
       " ('해', 'VV+EC'),\n",
       " ('보', 'VX'),\n",
       " ('려고', 'EC'),\n",
       " ('합니다', 'VX+EF'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab.pos(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2deb243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams,word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1b1faff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object bigrams at 0x00000203FA6F3F48>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_list = bigrams(mecab.morphs(sentence))\n",
    "bigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08cde4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('오늘', '은')\n",
      "('은', '중국')\n",
      "('중국', '주식')\n",
      "('주식', '에')\n",
      "('에', '투자')\n",
      "('투자', '하')\n",
      "('하', '는')\n",
      "('는', '분')\n",
      "('분', '이')\n",
      "('이', '라면')\n",
      "('라면', '한')\n",
      "('한', '번')\n",
      "('번', '쯤')\n",
      "('쯤', '은')\n",
      "('은', '들')\n",
      "('들', '어')\n",
      "('어', '보')\n",
      "('보', '셨')\n",
      "('셨', '을')\n",
      "('을', '법')\n",
      "('법', '한')\n",
      "('한', ',')\n",
      "(',', '하지만')\n",
      "('하지만', '너무')\n",
      "('너무', '복잡')\n",
      "('복잡', '하')\n",
      "('하', '고')\n",
      "('고', '낯설')\n",
      "('낯설', '어서')\n",
      "('어서', '잘')\n",
      "('잘', '이해')\n",
      "('이해', '가')\n",
      "('가', '가')\n",
      "('가', '지')\n",
      "('지', '않')\n",
      "('않', '는')\n",
      "('는', '얘기')\n",
      "('얘기', '를')\n",
      "('를', '좀')\n",
      "('좀', '해')\n",
      "('해', '보')\n",
      "('보', '려고')\n",
      "('려고', '합니다')\n",
      "('합니다', '.')\n"
     ]
    }
   ],
   "source": [
    "for token in bigram_list:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b3c60d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x203fa7419c8>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_list = ngrams(mecab.morphs(sentence), 3)\n",
    "trigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bcd6fa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('오늘', '은', '중국')\n",
      "('은', '중국', '주식')\n",
      "('중국', '주식', '에')\n",
      "('주식', '에', '투자')\n",
      "('에', '투자', '하')\n",
      "('투자', '하', '는')\n",
      "('하', '는', '분')\n",
      "('는', '분', '이')\n",
      "('분', '이', '라면')\n",
      "('이', '라면', '한')\n",
      "('라면', '한', '번')\n",
      "('한', '번', '쯤')\n",
      "('번', '쯤', '은')\n",
      "('쯤', '은', '들')\n",
      "('은', '들', '어')\n",
      "('들', '어', '보')\n",
      "('어', '보', '셨')\n",
      "('보', '셨', '을')\n",
      "('셨', '을', '법')\n",
      "('을', '법', '한')\n",
      "('법', '한', ',')\n",
      "('한', ',', '하지만')\n",
      "(',', '하지만', '너무')\n",
      "('하지만', '너무', '복잡')\n",
      "('너무', '복잡', '하')\n",
      "('복잡', '하', '고')\n",
      "('하', '고', '낯설')\n",
      "('고', '낯설', '어서')\n",
      "('낯설', '어서', '잘')\n",
      "('어서', '잘', '이해')\n",
      "('잘', '이해', '가')\n",
      "('이해', '가', '가')\n",
      "('가', '가', '지')\n",
      "('가', '지', '않')\n",
      "('지', '않', '는')\n",
      "('않', '는', '얘기')\n",
      "('는', '얘기', '를')\n",
      "('얘기', '를', '좀')\n",
      "('를', '좀', '해')\n",
      "('좀', '해', '보')\n",
      "('해', '보', '려고')\n",
      "('보', '려고', '합니다')\n",
      "('려고', '합니다', '.')\n"
     ]
    }
   ],
   "source": [
    "for token in trigram_list:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e0ef4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "docs = [\"오늘 동물원에서 원숭이를 봤어\",\n",
    "     \"오늘 동물원에서 코끼리를 봤어 봤어\",\n",
    "     \"동물원에서 원숭이에게 바나나를 줬어 바나나를\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90dea550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['오늘', '동물원에서', '원숭이를', '봤어'],\n",
       " ['오늘', '동물원에서', '코끼리를', '봤어', '봤어'],\n",
       " ['동물원에서', '원숭이에게', '바나나를', '줬어', '바나나를']]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ls = [doc.split() for doc in docs]\n",
    "doc_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1adbb02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'오늘': 0,\n",
       " '동물원에서': 1,\n",
       " '원숭이를': 2,\n",
       " '봤어': 3,\n",
       " '코끼리를': 4,\n",
       " '원숭이에게': 5,\n",
       " '바나나를': 6,\n",
       " '줬어': 7}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id={}\n",
    "\n",
    "index=0\n",
    "for doc in doc_ls:\n",
    "    for token in doc:\n",
    "        if token not in word2id:\n",
    "            word2id[token] = index\n",
    "            index+=1\n",
    "word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3f0ff942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'오늘': 0,\n",
       " '동물원에서': 1,\n",
       " '원숭이를': 2,\n",
       " '봤어': 3,\n",
       " '코끼리를': 4,\n",
       " '원숭이에게': 5,\n",
       " '바나나를': 6,\n",
       " '줬어': 7}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id = {}\n",
    "\n",
    "for doc in doc_ls:\n",
    "    for token in doc:\n",
    "        if token not in word2id:\n",
    "            word2id[token] = len(word2id)\n",
    "word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f69a22f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x00000203CBE871F8>, {})\n",
      "defaultdict(<function <lambda> at 0x00000203CBE871F8>, {'오늘': 0, '동물원에서': 1, '원숭이를': 2, '봤어': 3, '코끼리를': 4, '원숭이에게': 5, '바나나를': 6, '줬어': 7})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word2id = defaultdict(lambda : len(word2id))\n",
    "print(word2id)\n",
    "[word2id[token] for doc in doc_ls for token in doc]\n",
    "print(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32cde299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 0, 2, 1, 0, 0, 0], [0, 1, 0, 0, 0, 1, 2, 1]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "BoW_ls=[]\n",
    "\n",
    "for doc in doc_ls:\n",
    "    bow = np.zeros(len(word2id), dtype=int)\n",
    "    for token in doc:\n",
    "        bow[word2id[token]] += 1\n",
    "    BoW_ls.append(bow.tolist())\n",
    "BoW_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8db02801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 0 : 오늘 동물원에서 원숭이를 봤어\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>오늘</th>\n",
       "      <th>동물원에서</th>\n",
       "      <th>원숭이를</th>\n",
       "      <th>봤어</th>\n",
       "      <th>코끼리를</th>\n",
       "      <th>원숭이에게</th>\n",
       "      <th>바나나를</th>\n",
       "      <th>줬어</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   오늘  동물원에서  원숭이를  봤어  코끼리를  원숭이에게  바나나를  줬어\n",
       "0   1      1     1   1     0      0     0   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "문서 1 : 오늘 동물원에서 코끼리를 봤어 봤어\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>오늘</th>\n",
       "      <th>동물원에서</th>\n",
       "      <th>원숭이를</th>\n",
       "      <th>봤어</th>\n",
       "      <th>코끼리를</th>\n",
       "      <th>원숭이에게</th>\n",
       "      <th>바나나를</th>\n",
       "      <th>줬어</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   오늘  동물원에서  원숭이를  봤어  코끼리를  원숭이에게  바나나를  줬어\n",
       "0   1      1     0   2     1      0     0   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "문서 2 : 동물원에서 원숭이에게 바나나를 줬어 바나나를\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>오늘</th>\n",
       "      <th>동물원에서</th>\n",
       "      <th>원숭이를</th>\n",
       "      <th>봤어</th>\n",
       "      <th>코끼리를</th>\n",
       "      <th>원숭이에게</th>\n",
       "      <th>바나나를</th>\n",
       "      <th>줬어</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   오늘  동물원에서  원숭이를  봤어  코끼리를  원숭이에게  바나나를  줬어\n",
       "0   0      1     0   0     0      1     2   1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.core import display as ICD\n",
    "\n",
    "sorted_vocab = sorted((value,key) for key,value in word2id.items())\n",
    "vocab = [v[1] for v in sorted_vocab]\n",
    "for i in range(len(docs)):\n",
    "    print(\"문서 {} : {}\".format(i,docs[i]))\n",
    "    ICD.display(pd.DataFrame([BoW_ls[i]], columns=vocab))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c08388f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 0, 2, 1, 0, 0, 0, 1],\n",
       "       [1, 2, 0, 0, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "BoW = count_vect.fit_transform(docs)\n",
    "BoW.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9993ae89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['동물원에서', '바나나를', '봤어', '오늘', '원숭이를', '원숭이에게', '줬어', '코끼리를'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core import display as ICD\n",
    "\n",
    "vocab = count_vect.get_feature_names_out()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f8309b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 0 : 오늘 동물원에서 원숭이를 봤어\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>동물원에서</th>\n",
       "      <th>바나나를</th>\n",
       "      <th>봤어</th>\n",
       "      <th>오늘</th>\n",
       "      <th>원숭이를</th>\n",
       "      <th>원숭이에게</th>\n",
       "      <th>줬어</th>\n",
       "      <th>코끼리를</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   동물원에서  바나나를  봤어  오늘  원숭이를  원숭이에게  줬어  코끼리를\n",
       "0      1     0   1   1     1      0   0     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "문서 1 : 오늘 동물원에서 코끼리를 봤어 봤어\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>동물원에서</th>\n",
       "      <th>바나나를</th>\n",
       "      <th>봤어</th>\n",
       "      <th>오늘</th>\n",
       "      <th>원숭이를</th>\n",
       "      <th>원숭이에게</th>\n",
       "      <th>줬어</th>\n",
       "      <th>코끼리를</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   동물원에서  바나나를  봤어  오늘  원숭이를  원숭이에게  줬어  코끼리를\n",
       "0      1     0   2   1     0      0   0     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "문서 2 : 동물원에서 원숭이에게 바나나를 줬어 바나나를\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>동물원에서</th>\n",
       "      <th>바나나를</th>\n",
       "      <th>봤어</th>\n",
       "      <th>오늘</th>\n",
       "      <th>원숭이를</th>\n",
       "      <th>원숭이에게</th>\n",
       "      <th>줬어</th>\n",
       "      <th>코끼리를</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   동물원에서  바나나를  봤어  오늘  원숭이를  원숭이에게  줬어  코끼리를\n",
       "0      1     2   0   0     0      1   1     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(\"문서 {} : {}\".format(i,docs[i]))\n",
    "    ICD.display(pd.DataFrame([BoW.toarray()[i]], columns=vocab))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1fe8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
