{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0225_deeplearning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow 설치"
      ],
      "metadata": {
        "id": "0x5hPXmMV7-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "mUjtfFCyV7sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3D9CR1WgVwFs",
        "outputId": "e51d2418-0ed3-4c07-9c91-278524b32645"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cVqZsTnDWGru",
        "outputId": "36fe1827-0d86-495e-da3f-ebb9d72722b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.3.0\n",
            "  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 54 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.3.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 58.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 75.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.37.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.43.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.15.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.17.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.6.3)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.8.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.2.0)\n",
            "Installing collected packages: numpy, tensorflow-estimator, h5py, gast, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 numpy-1.18.5 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "h5py",
                  "numpy",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2차 방정식의 계수 추정"
      ],
      "metadata": {
        "id": "fuvaYHjLXHnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant(2.0) #constant : 상수(변하지 않음)\n",
        "y = tf.constant(8.0)\n",
        "x = tf.Variable(10.0) #variable도 하나의 tensor\n",
        "\n",
        "print(a) #tensor : 흘러가는 데이터\n",
        "print(x) #variable 객체 : 데이터를 다룰 때 numpy array 형태로 다룸"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOo59aUnWpRC",
        "outputId": "c092f32c-0b71-4880-af6c-3028bc6419a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(2.0, shape=(), dtype=float32)\n",
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=10.0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loss함수 정의\n",
        "loss = tf.math.abs(a * x - y) #x가 weight, a는 상수\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BezOt8pDXeCh",
        "outputId": "6743c652-19e4-4a22-c4d2-5452dee74411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=12.0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_func():\n",
        "  #미분 사용하기\n",
        "  with tf.GradientTape() as tape:\n",
        "    #loss 함수에서 loss 구하기\n",
        "    loss = tf.math.abs(a * x - y)\n",
        "    #loss 찍어보면 0; but 그안에 식에 대한 정보 가지고 있음(그래프로 연결되어 있으므로)\n",
        "    #type(loss) => EagerTensor\n",
        "    print(\"loss : {}, type : {}\".format(loss, type(loss)))\n",
        "  \n",
        "  #loss함수에 대해 x로 미분하기\n",
        "  dx = tape.gradient(loss, x)\n",
        "  #x와 기울기 출력\n",
        "  print(\"x = {}, dx = {}\".format(x.numpy(), dx))\n",
        "\n",
        "  #가중치 업데이트 - assign : x를 (x-dx)로 갱신해라\n",
        "  x.assign(x - dx)\n",
        "\n",
        "for i in range(4):\n",
        "  train_func()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IkN6q7udIH_",
        "outputId": "a2d262e0-223a-4fcc-83dd-6520359c0af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = 10.0, dx = 2.0\n",
            "x = 8.0, dx = 2.0\n",
            "x = 6.0, dx = 2.0\n",
            "x = 4.0, dx = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2차 방정식의 계수 추정 - 2차함수 연속값이므로 cost함수로 MSE 활용\n",
        "- 딥러닝 2-1. 신경망 학습 p.12 참고\n",
        "- t : 정답, y : 예측한 값\n",
        "- tensorflow : numpy와 작동방식 비슷, element-wised & broadcasting 지원"
      ],
      "metadata": {
        "id": "tFacxRKVfdYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer - Gradient Descent"
      ],
      "metadata": {
        "id": "q4JXwGv_nCSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 임의로 x 범위 설정(-5부터 5까지 0.1간격으로 데이터 만들기)\n",
        "x = np.array(np.arange(-5, 5, 0.1))\n",
        "y = 2 * x * x + 3 * x + 5\n",
        "lr = 0.001\n",
        "\n",
        "# update해야 할 값들\n",
        "w1 = tf.Variable(1.0) #변해야 하는 값이니까 variable\n",
        "w2 = tf.Variable(1.0)\n",
        "b = tf.Variable(1.0)\n",
        "\n",
        "histloss = []\n",
        "for epoch in range(10000):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # RMSE - Root MSE => loss 함수\n",
        "    # loss는 수식이고 이걸 그래프로 만들려면 gradienttape안에 넣어줘야 함\n",
        "    loss = tf.sqrt(tf.reduce_mean(tf.square(w1 * x * x + w2 * x + b - y))) #예측값; tf.square = 제곱\n",
        "\n",
        "  # 변화시킬 값들의 미분값 구하기\n",
        "  dw1, dw2, db = tape.gradient(loss, [w1, w2, b])\n",
        "\n",
        "  # 변화시킬 값들 update : 예측값 - 학습률 * 미분값\n",
        "  w1.assign_sub(lr * dw1) #w1.assign(w1 - lr * dw1) 이 식과 동일\n",
        "  w2.assign_sub(lr * dw2)\n",
        "  b.assign_sub(lr * db)\n",
        "\n",
        "  #loss 줄어드는 것 보기위해 histloss에 append\n",
        "  histloss.append(loss)\n",
        "  if epoch % 50 == 0: #50번마다 찍겠다\n",
        "    print(f\"epoch = {epoch}, loss = {loss}\")\n",
        "    # loss가 15에서 0.06까지 감소"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qPQhVd4eUg5",
        "outputId": "9af14928-51df-4844-cbaa-9a3494c71f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0, loss = 15.339600563049316\n",
            "epoch = 50, loss = 10.53668212890625\n",
            "epoch = 100, loss = 7.254340648651123\n",
            "epoch = 150, loss = 5.8860344886779785\n",
            "epoch = 200, loss = 5.382297992706299\n",
            "epoch = 250, loss = 5.042600631713867\n",
            "epoch = 300, loss = 4.732669353485107\n",
            "epoch = 350, loss = 4.437992095947266\n",
            "epoch = 400, loss = 4.158863067626953\n",
            "epoch = 450, loss = 3.8971211910247803\n",
            "epoch = 500, loss = 3.654714584350586\n",
            "epoch = 550, loss = 3.433422565460205\n",
            "epoch = 600, loss = 3.2346432209014893\n",
            "epoch = 650, loss = 3.059171676635742\n",
            "epoch = 700, loss = 2.9070029258728027\n",
            "epoch = 750, loss = 2.7772514820098877\n",
            "epoch = 800, loss = 2.668198585510254\n",
            "epoch = 850, loss = 2.577488660812378\n",
            "epoch = 900, loss = 2.502406597137451\n",
            "epoch = 950, loss = 2.440169334411621\n",
            "epoch = 1000, loss = 2.3881609439849854\n",
            "epoch = 1050, loss = 2.344085454940796\n",
            "epoch = 1100, loss = 2.306025266647339\n",
            "epoch = 1150, loss = 2.272446870803833\n",
            "epoch = 1200, loss = 2.242157459259033\n",
            "epoch = 1250, loss = 2.214254856109619\n",
            "epoch = 1300, loss = 2.188066005706787\n",
            "epoch = 1350, loss = 2.1630992889404297\n",
            "epoch = 1400, loss = 2.1389970779418945\n",
            "epoch = 1450, loss = 2.1155030727386475\n",
            "epoch = 1500, loss = 2.092434883117676\n",
            "epoch = 1550, loss = 2.06966233253479\n",
            "epoch = 1600, loss = 2.04709529876709\n",
            "epoch = 1650, loss = 2.0246691703796387\n",
            "epoch = 1700, loss = 2.002340316772461\n",
            "epoch = 1750, loss = 1.98008131980896\n",
            "epoch = 1800, loss = 1.957867980003357\n",
            "epoch = 1850, loss = 1.9356844425201416\n",
            "epoch = 1900, loss = 1.913521409034729\n",
            "epoch = 1950, loss = 1.8913716077804565\n",
            "epoch = 2000, loss = 1.8692306280136108\n",
            "epoch = 2050, loss = 1.8470953702926636\n",
            "epoch = 2100, loss = 1.8249636888504028\n",
            "epoch = 2150, loss = 1.8028345108032227\n",
            "epoch = 2200, loss = 1.7807068824768066\n",
            "epoch = 2250, loss = 1.7585798501968384\n",
            "epoch = 2300, loss = 1.736453652381897\n",
            "epoch = 2350, loss = 1.7143278121948242\n",
            "epoch = 2400, loss = 1.692202091217041\n",
            "epoch = 2450, loss = 1.6700763702392578\n",
            "epoch = 2500, loss = 1.6479506492614746\n",
            "epoch = 2550, loss = 1.6258248090744019\n",
            "epoch = 2600, loss = 1.6036992073059082\n",
            "epoch = 2650, loss = 1.581573486328125\n",
            "epoch = 2700, loss = 1.5594478845596313\n",
            "epoch = 2750, loss = 1.5373221635818481\n",
            "epoch = 2800, loss = 1.5151963233947754\n",
            "epoch = 2850, loss = 1.4930706024169922\n",
            "epoch = 2900, loss = 1.470944881439209\n",
            "epoch = 2950, loss = 1.4488190412521362\n",
            "epoch = 3000, loss = 1.426693320274353\n",
            "epoch = 3050, loss = 1.4045674800872803\n",
            "epoch = 3100, loss = 1.382441759109497\n",
            "epoch = 3150, loss = 1.3603160381317139\n",
            "epoch = 3200, loss = 1.3381901979446411\n",
            "epoch = 3250, loss = 1.3160643577575684\n",
            "epoch = 3300, loss = 1.2939386367797852\n",
            "epoch = 3350, loss = 1.2718126773834229\n",
            "epoch = 3400, loss = 1.2496869564056396\n",
            "epoch = 3450, loss = 1.227561116218567\n",
            "epoch = 3500, loss = 1.2054353952407837\n",
            "epoch = 3550, loss = 1.1833094358444214\n",
            "epoch = 3600, loss = 1.1611838340759277\n",
            "epoch = 3650, loss = 1.139057993888855\n",
            "epoch = 3700, loss = 1.1169321537017822\n",
            "epoch = 3750, loss = 1.094806432723999\n",
            "epoch = 3800, loss = 1.0726807117462158\n",
            "epoch = 3850, loss = 1.050554871559143\n",
            "epoch = 3900, loss = 1.0284289121627808\n",
            "epoch = 3950, loss = 1.006303071975708\n",
            "epoch = 4000, loss = 0.9841771721839905\n",
            "epoch = 4050, loss = 0.9620516300201416\n",
            "epoch = 4100, loss = 0.9399256706237793\n",
            "epoch = 4150, loss = 0.9177998304367065\n",
            "epoch = 4200, loss = 0.8956741094589233\n",
            "epoch = 4250, loss = 0.8735482692718506\n",
            "epoch = 4300, loss = 0.8514225482940674\n",
            "epoch = 4350, loss = 0.8292967081069946\n",
            "epoch = 4400, loss = 0.8071708083152771\n",
            "epoch = 4450, loss = 0.7850450873374939\n",
            "epoch = 4500, loss = 0.7629193067550659\n",
            "epoch = 4550, loss = 0.7407935857772827\n",
            "epoch = 4600, loss = 0.7186676859855652\n",
            "epoch = 4650, loss = 0.6965417861938477\n",
            "epoch = 4700, loss = 0.6744162440299988\n",
            "epoch = 4750, loss = 0.6522905230522156\n",
            "epoch = 4800, loss = 0.6301647424697876\n",
            "epoch = 4850, loss = 0.6080388426780701\n",
            "epoch = 4900, loss = 0.5859130620956421\n",
            "epoch = 4950, loss = 0.5637873411178589\n",
            "epoch = 5000, loss = 0.5416615009307861\n",
            "epoch = 5050, loss = 0.5195357203483582\n",
            "epoch = 5100, loss = 0.49740979075431824\n",
            "epoch = 5150, loss = 0.4752840995788574\n",
            "epoch = 5200, loss = 0.45315828919410706\n",
            "epoch = 5250, loss = 0.4310324490070343\n",
            "epoch = 5300, loss = 0.40890660881996155\n",
            "epoch = 5350, loss = 0.38678088784217834\n",
            "epoch = 5400, loss = 0.36465513706207275\n",
            "epoch = 5450, loss = 0.3425292670726776\n",
            "epoch = 5500, loss = 0.3204036056995392\n",
            "epoch = 5550, loss = 0.29827749729156494\n",
            "epoch = 5600, loss = 0.2761518061161041\n",
            "epoch = 5650, loss = 0.2540260851383209\n",
            "epoch = 5700, loss = 0.23190033435821533\n",
            "epoch = 5750, loss = 0.20977434515953064\n",
            "epoch = 5800, loss = 0.18764857947826385\n",
            "epoch = 5850, loss = 0.1655227094888687\n",
            "epoch = 5900, loss = 0.14339697360992432\n",
            "epoch = 5950, loss = 0.12127117067575455\n",
            "epoch = 6000, loss = 0.09914541989564896\n",
            "epoch = 6050, loss = 0.0770195946097374\n",
            "epoch = 6100, loss = 0.0548938550055027\n",
            "epoch = 6150, loss = 0.06270178407430649\n",
            "epoch = 6200, loss = 0.06277883052825928\n",
            "epoch = 6250, loss = 0.06280405819416046\n",
            "epoch = 6300, loss = 0.06281872093677521\n",
            "epoch = 6350, loss = 0.06282556056976318\n",
            "epoch = 6400, loss = 0.06282777339220047\n",
            "epoch = 6450, loss = 0.06282980740070343\n",
            "epoch = 6500, loss = 0.06283137202262878\n",
            "epoch = 6550, loss = 0.06282679736614227\n",
            "epoch = 6600, loss = 0.06283307075500488\n",
            "epoch = 6650, loss = 0.06282666325569153\n",
            "epoch = 6700, loss = 0.0628267303109169\n",
            "epoch = 6750, loss = 0.062828429043293\n",
            "epoch = 6800, loss = 0.06282889097929001\n",
            "epoch = 6850, loss = 0.06283562630414963\n",
            "epoch = 6900, loss = 0.06285765022039413\n",
            "epoch = 6950, loss = 0.06285368651151657\n",
            "epoch = 7000, loss = 0.06284035742282867\n",
            "epoch = 7050, loss = 0.06286904960870743\n",
            "epoch = 7100, loss = 0.06289292871952057\n",
            "epoch = 7150, loss = 0.06290508806705475\n",
            "epoch = 7200, loss = 0.06291399151086807\n",
            "epoch = 7250, loss = 0.06292282044887543\n",
            "epoch = 7300, loss = 0.0629316046833992\n",
            "epoch = 7350, loss = 0.06293746829032898\n",
            "epoch = 7400, loss = 0.06293746829032898\n",
            "epoch = 7450, loss = 0.06293746829032898\n",
            "epoch = 7500, loss = 0.06293746829032898\n",
            "epoch = 7550, loss = 0.06293746829032898\n",
            "epoch = 7600, loss = 0.06293746829032898\n",
            "epoch = 7650, loss = 0.06293746829032898\n",
            "epoch = 7700, loss = 0.06293746829032898\n",
            "epoch = 7750, loss = 0.06293746829032898\n",
            "epoch = 7800, loss = 0.06293746829032898\n",
            "epoch = 7850, loss = 0.06293746829032898\n",
            "epoch = 7900, loss = 0.06293746829032898\n",
            "epoch = 7950, loss = 0.06293746829032898\n",
            "epoch = 8000, loss = 0.06293746829032898\n",
            "epoch = 8050, loss = 0.06293746829032898\n",
            "epoch = 8100, loss = 0.06293746829032898\n",
            "epoch = 8150, loss = 0.06293746829032898\n",
            "epoch = 8200, loss = 0.06293746829032898\n",
            "epoch = 8250, loss = 0.06293746829032898\n",
            "epoch = 8300, loss = 0.06293746829032898\n",
            "epoch = 8350, loss = 0.06293746829032898\n",
            "epoch = 8400, loss = 0.06293746829032898\n",
            "epoch = 8450, loss = 0.06293746829032898\n",
            "epoch = 8500, loss = 0.06293746829032898\n",
            "epoch = 8550, loss = 0.06293746829032898\n",
            "epoch = 8600, loss = 0.06293746829032898\n",
            "epoch = 8650, loss = 0.06293746829032898\n",
            "epoch = 8700, loss = 0.06293746829032898\n",
            "epoch = 8750, loss = 0.06293746829032898\n",
            "epoch = 8800, loss = 0.06293746829032898\n",
            "epoch = 8850, loss = 0.06293746829032898\n",
            "epoch = 8900, loss = 0.06293746829032898\n",
            "epoch = 8950, loss = 0.06293746829032898\n",
            "epoch = 9000, loss = 0.06293746829032898\n",
            "epoch = 9050, loss = 0.06293746829032898\n",
            "epoch = 9100, loss = 0.06293746829032898\n",
            "epoch = 9150, loss = 0.06293746829032898\n",
            "epoch = 9200, loss = 0.06293746829032898\n",
            "epoch = 9250, loss = 0.06293746829032898\n",
            "epoch = 9300, loss = 0.06293746829032898\n",
            "epoch = 9350, loss = 0.06293746829032898\n",
            "epoch = 9400, loss = 0.06293746829032898\n",
            "epoch = 9450, loss = 0.06293746829032898\n",
            "epoch = 9500, loss = 0.06293746829032898\n",
            "epoch = 9550, loss = 0.06293746829032898\n",
            "epoch = 9600, loss = 0.06293746829032898\n",
            "epoch = 9650, loss = 0.06293746829032898\n",
            "epoch = 9700, loss = 0.06293746829032898\n",
            "epoch = 9750, loss = 0.06293746829032898\n",
            "epoch = 9800, loss = 0.06293746829032898\n",
            "epoch = 9850, loss = 0.06293746829032898\n",
            "epoch = 9900, loss = 0.06293746829032898\n",
            "epoch = 9950, loss = 0.06293746829032898\n",
            "CPU times: user 14.1 s, sys: 97 ms, total: 14.2 s\n",
            "Wall time: 14.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss 함수 시각화\n",
        "plt.plot(histloss, color='red', linewidth=1)\n",
        "plt.title(\"loss function\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "# loss 떨어지는걸 보니 학습 잘 되는구나!\n",
        "# epoch 6000쯤부터 변화x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "rb740FmtjIhG",
        "outputId": "c6c000e1-045c-4279-d725-db53d705919b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer - Momentum"
      ],
      "metadata": {
        "id": "BoB-ZKNhkvD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers"
      ],
      "metadata": {
        "id": "sHz-1GR3kr8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "x = np.array(np.arange(-5, 5, 0.1))\n",
        "y = 2 * x * x + 3 * x + 5\n",
        "lr = 0.001\n",
        "\n",
        "# update해야 할 값들\n",
        "w1 = tf.Variable(1.0) #변해야 하는 값이니까 variable\n",
        "w2 = tf.Variable(1.0)\n",
        "b = tf.Variable(1.0)\n",
        "var_list = [w1, w2, b]\n",
        "\n",
        "# Momentum 적용 - SGD를 보완한 것\n",
        "opt = optimizers.SGD(learning_rate=lr, momentum=0.7)\n",
        "\n",
        "histloss = []\n",
        "for epoch in range(10000):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # RMSE - Root MSE => loss 함수\n",
        "    # loss는 수식이고 이걸 그래프로 만들려면 gradienttape안에 넣어줘야 함\n",
        "    loss = tf.sqrt(tf.reduce_mean(tf.square(w1 * x * x + w2 * x + b - y))) #예측값; tf.square = 제곱\n",
        "\n",
        "  # 변화시킬 값들의 미분값 구하기\n",
        "  grads = tape.gradient(loss, var_list)\n",
        "  # 가중치 업데이트 - 구한 grads를 어떻게 업데이트할지 결정하는게 optimizer\n",
        "  opt.apply_gradients(zip(grads, var_list))\n",
        "\n",
        "  #loss 줄어드는 것 보기위해 histloss에 append\n",
        "  histloss.append(loss)\n",
        "  if epoch % 500 == 0: #50번마다 찍겠다\n",
        "    print(f\"epoch = {epoch}, loss = {loss}\")\n",
        "    print(f\"w1 = {w1.numpy()}, w2 = {w2.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ1a_1rcjjjc",
        "outputId": "2f036844-a80d-4f67-a04c-146143f9160e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0, loss = 15.339600563049316\n",
            "w1 = 1.010164737701416, w2 = 1.0009921789169312\n",
            "epoch = 500, loss = 2.023297071456909\n",
            "w1 = 2.202728509902954, w2 = 2.9980216026306152\n",
            "epoch = 1000, loss = 1.2853883504867554\n",
            "w1 = 2.128865957260132, w2 = 3.008208751678467\n",
            "epoch = 1500, loss = 0.5479283928871155\n",
            "w1 = 2.054847478866577, w2 = 3.0034937858581543\n",
            "epoch = 2000, loss = 0.0375760979950428\n",
            "w1 = 1.996767520904541, w2 = 3.0000345706939697\n",
            "epoch = 2500, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 3000, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 3500, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 4000, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 4500, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 5000, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 5500, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 6000, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 6500, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 7000, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 7500, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 8000, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 8500, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 9000, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "epoch = 9500, loss = 0.03759327903389931\n",
            "w1 = 1.9967671632766724, w2 = 3.0000345706939697\n",
            "CPU times: user 19.4 s, sys: 77.2 ms, total: 19.4 s\n",
            "Wall time: 19.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(histloss, color='red', linewidth=1)\n",
        "plt.title(\"loss function\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "#위와 달리 epoch 2000번 안쪽에서 수렴 - 수렴속도 더 빠름(2000이후 학습 필요 x)\n",
        "#계산량은 더 많지만 튀는 정도 덜하기 때문에 빠르게 수렴"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "06Gx9cpcmPc_",
        "outputId": "99f2890a-1024-4ca5-8397-9e6dc28c6d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer - Adam"
      ],
      "metadata": {
        "id": "n7E6n2ZOqOke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "x = np.array(np.arange(-5, 5, 0.1))\n",
        "y = 2 * x * x + 3 * x + 5\n",
        "lr = 0.001\n",
        "\n",
        "# update해야 할 값들\n",
        "w1 = tf.Variable(1.0) #변해야 하는 값이니까 variable\n",
        "w2 = tf.Variable(1.0)\n",
        "b = tf.Variable(1.0)\n",
        "var_list = [w1, w2, b]\n",
        "\n",
        "# Adam\n",
        "opt = optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "def loss():\n",
        "  return tf.sqrt(tf.reduce_mean(tf.square(w1 * x * x + w2 * x + b - y)))\n",
        "\n",
        "histLoss = []\n",
        "for epoch in range(10000):\n",
        "  # minimize : gradient 구하고 가중치 업데이트\n",
        "  opt.minimize(loss, var_list = var_list)\n",
        "\n",
        "  #loss 줄어드는 것 보기위해 histloss에 append\n",
        "  histLoss.append(loss())\n",
        "  if epoch % 500 == 0: #500번마다 찍겠다\n",
        "    print(f\"epoch = {epoch}, loss = {histLoss[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XV7xvCDmlhb",
        "outputId": "7da8df5d-be3b-4ea8-9e6c-52bcdb572130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0, loss = 15.327645301818848\n",
            "epoch = 500, loss = 9.454402923583984\n",
            "epoch = 1000, loss = 4.246377468109131\n",
            "epoch = 1500, loss = 1.9247740507125854\n",
            "epoch = 2000, loss = 1.4152452945709229\n",
            "epoch = 2500, loss = 1.0947701930999756\n",
            "epoch = 3000, loss = 0.7711126208305359\n",
            "epoch = 3500, loss = 0.44227075576782227\n",
            "epoch = 4000, loss = 0.11060690134763718\n",
            "epoch = 4500, loss = 0.000905002816580236\n",
            "epoch = 5000, loss = 0.0009234699537046254\n",
            "epoch = 5500, loss = 0.0008568525081500411\n",
            "epoch = 6000, loss = 0.0008003414841368794\n",
            "epoch = 6500, loss = 0.0007409106474369764\n",
            "epoch = 7000, loss = 0.0007376551511697471\n",
            "epoch = 7500, loss = 0.0007144282571971416\n",
            "epoch = 8000, loss = 0.0007153843180276453\n",
            "epoch = 8500, loss = 0.0007511757430620492\n",
            "epoch = 9000, loss = 0.0007269848720170557\n",
            "epoch = 9500, loss = 0.0007421072223223746\n",
            "CPU times: user 28.6 s, sys: 95.6 ms, total: 28.7 s\n",
            "Wall time: 29.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(histLoss, color='red', linewidth=1)\n",
        "plt.title(\"loss function\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "#Adam이 항상 제일 좋은 건 아님..\n",
        "#초기값 어떻게 주는지에 따라 성능 달라짐"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "iDgolBERrEmQ",
        "outputId": "a0e1095a-4f11-4a47-9060-76d3e1c05623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras : Sequential model\n",
        "- 위처럼 직접 계산하지 않도록 도와주는 툴"
      ],
      "metadata": {
        "id": "tf5EqUxSteay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# 입출력 정해지면 w 차원 정해짐. dense layer 개수만큼 w 생김.\n",
        "# bias 자동설정\n",
        "x = np.array(np.arange(-5, 5, 0.1))\n",
        "y = 2 * x * x + 3 * x + 5\n",
        "dataX = np.stack([x * x, x]).T #stack : 쌓는 것"
      ],
      "metadata": {
        "id": "6UuIpktbroOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([x*x, x]).shape\n",
        "#array에서 두개 만들었고, x가 100개이므로 shape = (2,100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7P5qivluSrI",
        "outputId": "83e45200-1fc2-4ec7-9e7b-67bc398b8105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.stack([x*x, x]).T.shape\n",
        "# (100,2) = (샘플의 개수, 입력의 개수)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbCG3-W-uV1S",
        "outputId": "6d778ce6-c620-463e-e412-b5f706404d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model 구성\n",
        "model = Sequential()\n",
        "#layer를 계속 붙이는 방식\n",
        "model.add(Dense(1, input_dim=2)) #Xavier glorot 초기값 사용\n",
        "model.compile(loss='mse', optimizer=optimizers.RMSprop(lr=0.05))\n",
        "#학습 & 학습기록 histloss처럼 기록\n",
        "h = model.fit(dataX, y,batch_size=10, epochs=300) #랜덤하게 10개씩 뽑아서 300번 돌리기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSmy5b-wuYnr",
        "outputId": "9227bb2d-a09c-41d0-e477-e04f83640f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 224.5917\n",
            "Epoch 2/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 95.0915\n",
            "Epoch 3/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 45.3892\n",
            "Epoch 4/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 25.6676\n",
            "Epoch 5/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 15.7695\n",
            "Epoch 6/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 9.2668\n",
            "Epoch 7/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 5.2406\n",
            "Epoch 8/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.1194\n",
            "Epoch 9/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.8393\n",
            "Epoch 10/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.1762\n",
            "Epoch 11/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7452\n",
            "Epoch 12/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4015\n",
            "Epoch 13/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2100\n",
            "Epoch 14/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2017\n",
            "Epoch 15/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0376\n",
            "Epoch 16/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0874\n",
            "Epoch 17/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2213\n",
            "Epoch 18/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.1799e-04\n",
            "Epoch 19/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2065\n",
            "Epoch 20/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0849\n",
            "Epoch 21/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0070\n",
            "Epoch 22/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1105\n",
            "Epoch 23/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1270\n",
            "Epoch 24/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1024\n",
            "Epoch 25/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1010\n",
            "Epoch 26/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0405e-04\n",
            "Epoch 27/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1810\n",
            "Epoch 28/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0274\n",
            "Epoch 29/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2185\n",
            "Epoch 30/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 2.7293e-05\n",
            "Epoch 31/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0159\n",
            "Epoch 32/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1950\n",
            "Epoch 33/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.5286e-05\n",
            "Epoch 34/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1608\n",
            "Epoch 35/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.0327e-04\n",
            "Epoch 36/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0772\n",
            "Epoch 37/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1391\n",
            "Epoch 38/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0183\n",
            "Epoch 39/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2578\n",
            "Epoch 40/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0043\n",
            "Epoch 41/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0442\n",
            "Epoch 42/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1644\n",
            "Epoch 43/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0272\n",
            "Epoch 44/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1825\n",
            "Epoch 45/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.6788e-04\n",
            "Epoch 46/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0991\n",
            "Epoch 47/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2144\n",
            "Epoch 48/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0503\n",
            "Epoch 49/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.3701e-04\n",
            "Epoch 50/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2013\n",
            "Epoch 51/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0138\n",
            "Epoch 52/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2566\n",
            "Epoch 53/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 2.1785e-04\n",
            "Epoch 54/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0646\n",
            "Epoch 55/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1306\n",
            "Epoch 56/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0668\n",
            "Epoch 57/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2007\n",
            "Epoch 58/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.4068e-04\n",
            "Epoch 59/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0084\n",
            "Epoch 60/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2136\n",
            "Epoch 61/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 62/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2476\n",
            "Epoch 63/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.8516e-04\n",
            "Epoch 64/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1210\n",
            "Epoch 65/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0398\n",
            "Epoch 66/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0477\n",
            "Epoch 67/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1511\n",
            "Epoch 68/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0212\n",
            "Epoch 69/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2854\n",
            "Epoch 70/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0013\n",
            "Epoch 71/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0243\n",
            "Epoch 72/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1783\n",
            "Epoch 73/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0055\n",
            "Epoch 74/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1450\n",
            "Epoch 75/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0703\n",
            "Epoch 76/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1810\n",
            "Epoch 77/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 78/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0107\n",
            "Epoch 79/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1920\n",
            "Epoch 80/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.7789e-04\n",
            "Epoch 81/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2411\n",
            "Epoch 82/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 9.8599e-04\n",
            "Epoch 83/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 8.9772e-04\n",
            "Epoch 84/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1789\n",
            "Epoch 85/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0835\n",
            "Epoch 86/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1299\n",
            "Epoch 87/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0189\n",
            "Epoch 88/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0709\n",
            "Epoch 89/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1447\n",
            "Epoch 90/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 91/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1772\n",
            "Epoch 92/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 93/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1274\n",
            "Epoch 94/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0865\n",
            "Epoch 95/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1511\n",
            "Epoch 96/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0092\n",
            "Epoch 97/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2327\n",
            "Epoch 98/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 99/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0127\n",
            "Epoch 100/300\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.1685\n",
            "Epoch 101/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0827\n",
            "Epoch 102/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1140\n",
            "Epoch 103/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1541\n",
            "Epoch 104/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.1737e-04\n",
            "Epoch 105/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1105\n",
            "Epoch 106/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0995\n",
            "Epoch 107/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1256\n",
            "Epoch 108/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0618\n",
            "Epoch 109/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0377\n",
            "Epoch 110/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0931\n",
            "Epoch 111/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0936\n",
            "Epoch 112/300\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1022\n",
            "Epoch 113/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0932\n",
            "Epoch 114/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0321\n",
            "Epoch 115/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1406\n",
            "Epoch 116/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2230\n",
            "Epoch 117/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.7688e-04\n",
            "Epoch 118/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.3073e-04\n",
            "Epoch 119/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2273\n",
            "Epoch 120/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0029\n",
            "Epoch 121/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1064\n",
            "Epoch 122/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0851\n",
            "Epoch 123/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.0690e-05\n",
            "Epoch 124/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2009\n",
            "Epoch 125/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0067\n",
            "Epoch 126/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0100\n",
            "Epoch 127/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2360\n",
            "Epoch 128/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 6.8097e-05\n",
            "Epoch 129/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0739\n",
            "Epoch 130/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1708\n",
            "Epoch 131/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0424\n",
            "Epoch 132/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1066\n",
            "Epoch 133/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1199\n",
            "Epoch 134/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0540\n",
            "Epoch 135/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1650\n",
            "Epoch 136/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0338\n",
            "Epoch 137/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1185\n",
            "Epoch 138/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0815\n",
            "Epoch 139/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0078\n",
            "Epoch 140/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1503\n",
            "Epoch 141/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0787\n",
            "Epoch 142/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1888\n",
            "Epoch 143/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0880\n",
            "Epoch 144/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.8303e-05\n",
            "Epoch 145/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1731\n",
            "Epoch 146/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0099\n",
            "Epoch 147/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1438\n",
            "Epoch 148/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0348\n",
            "Epoch 149/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0429\n",
            "Epoch 150/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1865\n",
            "Epoch 151/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0870\n",
            "Epoch 152/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.5238e-04\n",
            "Epoch 153/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1393\n",
            "Epoch 154/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0585\n",
            "Epoch 155/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1747\n",
            "Epoch 156/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4553e-04\n",
            "Epoch 157/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2235\n",
            "Epoch 158/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0313\n",
            "Epoch 159/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.1408e-04\n",
            "Epoch 160/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2439\n",
            "Epoch 161/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 162/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0699\n",
            "Epoch 163/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0884\n",
            "Epoch 164/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0989\n",
            "Epoch 165/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1456\n",
            "Epoch 166/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 2.0168e-05\n",
            "Epoch 167/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2393\n",
            "Epoch 168/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0359\n",
            "Epoch 169/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.1386e-04\n",
            "Epoch 170/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2555\n",
            "Epoch 171/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.8173e-04\n",
            "Epoch 172/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0029\n",
            "Epoch 173/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2302\n",
            "Epoch 174/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.1500e-06\n",
            "Epoch 175/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1142\n",
            "Epoch 176/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1994\n",
            "Epoch 177/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.4455e-04\n",
            "Epoch 178/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1207\n",
            "Epoch 179/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0190\n",
            "Epoch 180/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1579\n",
            "Epoch 181/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0226\n",
            "Epoch 182/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1880\n",
            "Epoch 183/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0710\n",
            "Epoch 184/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0288\n",
            "Epoch 185/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1793\n",
            "Epoch 186/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0309\n",
            "Epoch 187/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1465\n",
            "Epoch 188/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0059\n",
            "Epoch 189/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2988\n",
            "Epoch 190/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 9.3838e-04\n",
            "Epoch 191/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9293e-04\n",
            "Epoch 192/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1803\n",
            "Epoch 193/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0023\n",
            "Epoch 194/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1770\n",
            "Epoch 195/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0747\n",
            "Epoch 196/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1005\n",
            "Epoch 197/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0946\n",
            "Epoch 198/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0126\n",
            "Epoch 199/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1346\n",
            "Epoch 200/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0903\n",
            "Epoch 201/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1941\n",
            "Epoch 202/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 7.8570e-04\n",
            "Epoch 203/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2196\n",
            "Epoch 204/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 205/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1207\n",
            "Epoch 206/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0318\n",
            "Epoch 207/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1048\n",
            "Epoch 208/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2385\n",
            "Epoch 209/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.2054e-08\n",
            "Epoch 210/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.9678e-07\n",
            "Epoch 211/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1756\n",
            "Epoch 212/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0909\n",
            "Epoch 213/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1050\n",
            "Epoch 214/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0965\n",
            "Epoch 215/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0566\n",
            "Epoch 216/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0394\n",
            "Epoch 217/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1439\n",
            "Epoch 218/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0438\n",
            "Epoch 219/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1128\n",
            "Epoch 220/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1340\n",
            "Epoch 221/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1537\n",
            "Epoch 222/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.9220e-04\n",
            "Epoch 223/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1972\n",
            "Epoch 224/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 9.4506e-04\n",
            "Epoch 225/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0551\n",
            "Epoch 226/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2324\n",
            "Epoch 227/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 228/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0473\n",
            "Epoch 229/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2240\n",
            "Epoch 230/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0993e-04\n",
            "Epoch 231/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1228\n",
            "Epoch 232/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.9718e-04\n",
            "Epoch 233/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1281\n",
            "Epoch 234/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0824\n",
            "Epoch 235/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1304\n",
            "Epoch 236/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0504\n",
            "Epoch 237/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1196\n",
            "Epoch 238/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0815\n",
            "Epoch 239/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2301\n",
            "Epoch 240/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0045\n",
            "Epoch 241/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.8743e-05\n",
            "Epoch 242/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2423\n",
            "Epoch 243/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.5743e-04\n",
            "Epoch 244/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0563\n",
            "Epoch 245/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1599\n",
            "Epoch 246/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 247/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1775\n",
            "Epoch 248/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0045\n",
            "Epoch 249/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1795\n",
            "Epoch 250/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7526e-05\n",
            "Epoch 251/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1674\n",
            "Epoch 252/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0258\n",
            "Epoch 253/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0819\n",
            "Epoch 254/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0785\n",
            "Epoch 255/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1576\n",
            "Epoch 256/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 257/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0856\n",
            "Epoch 258/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0987\n",
            "Epoch 259/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0927\n",
            "Epoch 260/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0534\n",
            "Epoch 261/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1199\n",
            "Epoch 262/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0798\n",
            "Epoch 263/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1640\n",
            "Epoch 264/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0729\n",
            "Epoch 265/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0535\n",
            "Epoch 266/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0784\n",
            "Epoch 267/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1480\n",
            "Epoch 268/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 269/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0517\n",
            "Epoch 270/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1963\n",
            "Epoch 271/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0820e-04\n",
            "Epoch 272/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1489\n",
            "Epoch 273/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0231\n",
            "Epoch 274/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2210\n",
            "Epoch 275/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 9.9314e-04\n",
            "Epoch 276/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1573\n",
            "Epoch 277/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0287\n",
            "Epoch 278/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0332\n",
            "Epoch 279/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2368\n",
            "Epoch 280/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.2632e-04\n",
            "Epoch 281/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0183\n",
            "Epoch 282/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1577\n",
            "Epoch 283/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1189\n",
            "Epoch 284/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0093\n",
            "Epoch 285/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1693\n",
            "Epoch 286/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.5960e-06\n",
            "Epoch 287/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1938\n",
            "Epoch 288/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0441\n",
            "Epoch 289/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0081\n",
            "Epoch 290/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1326\n",
            "Epoch 291/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0016\n",
            "Epoch 292/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.1687\n",
            "Epoch 293/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1202\n",
            "Epoch 294/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0168\n",
            "Epoch 295/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1955\n",
            "Epoch 296/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0286\n",
            "Epoch 297/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0594\n",
            "Epoch 298/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1407\n",
            "Epoch 299/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 300/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(h.history['loss'], color='red', linewidth=1)\n",
        "plt.title(\"Loss function\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "fESc0kNYwJWM",
        "outputId": "ea9c53e0-99ca-445c-e7a1-f8e4f1686770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbDUlEQVR4nO3dfbRddX3n8ff33twk5AESuJcQA0l4SHXQKqUp0tG6GOmosDqD7aKItYgMLbNmcC1dYx2xMorOtMu2y7bLGaulC0esqODTSB1aBcb6MDMowUEetEgIiUlMSEIgCblJyMN3/vjts++5OTeXm8i55172+7XWWWef397n7O9v73PO5+7fPvecyEwkSQLo63UBkqSpw1CQJNUMBUlSzVCQJNUMBUlSzVCQJNUMBekYRMRxEfF3EbEjIr4wyet+OCIumMx1qjlm9LoA6ecREWuB38vMuyZ51ZcCi4CTMvNAt1YSEZ8CNmTm9a22zHxpt9YneaQgHZtlwE+6GQhSLxgKekGKiFkR8ZcR8bPq8pcRMauaNxgRX4uIpyNie0R8JyL6qnnviYiNEbErIh6JiAvHeOwPAu8H3hQRz0TE1RFxQ0R8pm2Z5RGRETGjuv2PEfGfI+J/V4/9jYgYbFv+1RHxf6qa1kfE2yLiGuAtwH+s1vN31bJrI+LXJ9DPCyJiQ0S8KyK2RMSmiLiqW9tcLwyGgl6o3gecD5wDvAI4D2gNwbwL2AAMUYaA/hDIiHgx8HbgVzJzPvB6YO3hD5yZHwD+GLg1M+dl5k0TrOl3gKuAk4GZwB8ARMQy4O+B/1rVdA5wf2beCNwC/Gm1nn91lP0EOAU4AVgCXA18LCIWTrBeNZChoBeqtwAfyswtmbkV+CBwRTVvP7AYWJaZ+zPzO1m+BOwgMAs4OyIGMnNtZj72PNb03zPzJ5m5B7iN8kYOJSzuyszPVfU8mZn3T/Axx+snlL5+qHrcO4BngBc/P93RC5GhoBeqFwHr2m6vq9oA/gxYDXwjItZExHUAmbkaeCdwA7AlIj4fES/i+bO5bXoYmFdNnwYca/iM10+AJw8779G+XqmDoaAXqp9RTga3LK3ayMxdmfmuzDwD+NfAf2idO8jMz2bmq6v7JvAnE1zfbmBO2+1TjqLW9cCZR5j3XF9jfMR+SsfCUNALwUBEzG67zAA+B1wfEUPVCd33A58BiIjfiIizIiKAHZRho0MR8eKIeG11onYvsAc4NMEa7gdeExFLI+IE4L1HUf8twK9HxGURMSMiToqI1tDSE8AZ49z3iP2UjoWhoBeCOyhv4K3LDcB/AVYBDwAPAj+o2gBWAHdRxtf/L/BXmflNyvmEDwPbKEM9JzPBN/fMvBO4tVrffcDXJlp8Zv4UuJhyAnw7JWBeUc2+iXKO4+mI+B9j3H28fkpHLfyRHUlSi0cKkqSaoSBJqhkKkqSaoSBJqk3rb0kdHBzM5cuX97oMSZpW7rvvvm2ZOTTWvGkdCsuXL2fVqlW9LkOSppWIWHekeQ4fSZJqhoIkqWYoSJJqhoIkqWYoSJJqhoIkqWYoSJJqzQyFZ5+F73yn11VI0pTTzFDYsQN+67d6XYUkTTnNDIW+Pjh4sNdVSNKU08xQ6O+HQxP9lUVJao5mhkJfn6EgSWNobig4fCRJHZoZCg4fSdKYmhkKDh9J0pgMBUlSrbmh4DkFSerQ3FDILBdJUq2ZoRBRLoaCJI3SzFAAh5AkaQzNDQU/lipJHZobCn4CSZI6GAqSpFqzQ8FzCpI0SnNDwXMKktShuaHg8JEkdWh2KDh8JEmjdC0UIuK0iPhmRPwoIh6OiHdU7SdGxJ0R8Wh1vbBqj4j4aESsjogHIuLcbtUGOHwkSWPo5pHCAeBdmXk2cD5wbUScDVwH3J2ZK4C7q9sAFwErqss1wMe7WJvDR5I0hq6FQmZuyswfVNO7gB8DS4BLgJurxW4G3lhNXwJ8Oot7gAURsbhb9RkKktRpUs4pRMRy4JeA7wGLMnNTNWszsKiaXgKsb7vbhqrt8Me6JiJWRcSqrVu3HntRnlOQpA5dD4WImAd8CXhnZu5sn5eZCRzVt9Jl5o2ZuTIzVw4NDR17YZ5TkKQOXQ2FiBigBMItmfnlqvmJ1rBQdb2lat8InNZ291Ortu5w+EiSOnTz00cB3AT8ODP/vG3W7cCV1fSVwFfb2t9afQrpfGBH2zDT88/hI0nqMKOLj/0q4ArgwYi4v2r7Q+DDwG0RcTWwDrismncHcDGwGhgGrupibQ4fSdIYuhYKmfldII4w+8Ixlk/g2m7V08HhI0nq0Oz/aDYUJGmUZoeC5xQkaZTmhoLnFCSpQ3NDweEjSerQ7FBw+EiSRmluKDh8JEkdmhsKDh9JUgdDQZJUa3YoeE5BkkZpbih4TkGSOjQ3FBw+kqQOzQ4Fh48kaZTmhoLDR5LUobmh4PCRJHVodig4fCRJozQ7FDxSkKRRmhsKnlOQpA7NDQWPFCSpQ7NDwXMKkjRKc0PB4SNJ6tDcUHD4SJI6NDsUHD6SpFGaGwoOH0lSh+aGgsNHktTBUJAk1ZodCp5TkKRRmhsKnlOQpA7NDQWHjySpQ7NDweEjSRqluaHg8JEkdWhuKDh8JEkdDAVJUq3ZoeA5BUkapbmh4DkFSerQ3FBw+EiSOnQtFCLikxGxJSIeamu7ISI2RsT91eXitnnvjYjVEfFIRLy+W3XVHD6SpA7dPFL4FPCGMdr/IjPPqS53AETE2cDlwEur+/xVRPR3sTaHjyRpDF0Lhcz8NrB9gotfAnw+M/dl5uPAauC8btUGOHwkSWPoxTmFt0fEA9Xw0sKqbQmwvm2ZDVVbh4i4JiJWRcSqrVu3HnsVhoIkdZjsUPg4cCZwDrAJ+MjRPkBm3piZKzNz5dDQ0LFX4jkFSeowqaGQmU9k5sHMPAT8DSNDRBuB09oWPbVq6x7PKUhSh0kNhYhY3HbzN4HWJ5NuBy6PiFkRcTqwAvh+V4tx+EiSOszo1gNHxOeAC4DBiNgAfAC4ICLOARJYC/xbgMx8OCJuA34EHACuzczuju04fCRJHboWCpn55jGabxpn+T8C/qhb9XRw+EiSOvgfzZKkmqEgSao1OxQ8pyBJozQ3FDynIEkdmhsKDh9JUodmh4LDR5I0SnNDweEjSerQ3FBw+EiSOhgKkqRas0PBcwqSNEpzQ8FzCpLUobmh4PCRJHVodig4fCRJozQ3FBw+kqQOzQ0Fh48kqUOzQ8HhI0kapdmh4JGCJI3S3FDwnIIkdWhuKHikIEkdmh0KnlOQpFGaGwoOH0lShwmFQkS8IyKOj+KmiPhBRLyu28V1lcNHktRhokcK/yYzdwKvAxYCVwAf7lpVk8HhI0nqMNFQiOr6YuBvM/PhtrbpySMFSeow0VC4LyK+QQmFr0fEfGB6v6N6TkGSOsyY4HJXA+cAazJzOCJOBK7qXlmTwCMFSeow0SOFXwUeycynI+J3geuBHd0raxJ4TkGSOkw0FD4ODEfEK4B3AY8Bn+5aVZPB4SNJ6jDRUDiQmQlcAvy3zPwYML97ZU0Ch48kqcNEzynsioj3Uj6K+msR0QcMdK+sSeDwkSR1mOiRwpuAfZT/V9gMnAr8WdeqmgwOH0lShwmFQhUEtwAnRMRvAHszc3qfU3D4SJI6TPRrLi4Dvg/8NnAZ8L2IuLSbhXWdoSBJHSZ6TuF9wK9k5haAiBgC7gK+2K3Cus5zCpLUYaLnFPpagVB58ijuOzV5TkGSOkz0jf0fIuLrEfG2iHgb8D+BO8a7Q0R8MiK2RMRDbW0nRsSdEfFodb2wao+I+GhErI6IByLi3GPt0IQ5fCRJHSZ6ovndwI3Ay6vLjZn5nue426eANxzWdh1wd2auAO6ubgNcBKyoLtdQ/lmuuxw+kqQOEz2nQGZ+CfjSUSz/7YhYfljzJcAF1fTNwD8C76naP139g9w9EbEgIhZn5qaJru+oOXwkSR3GDYWI2AXkWLOAzMzjj3J9i9re6DcDi6rpJcD6tuU2VG0doRAR11COJli6dOlRrr6Nw0eS1GHcUMjMrn2VRWZmRIwVOM91vxspQ1msXLnyqO9fMxQkqcNkf4LoiYhYDFBdtz7RtBE4rW25U6u27vGcgiR1mOxQuB24spq+EvhqW/tbq08hnQ/s6Or5BPCcgiSNYcInmo9WRHyOclJ5MCI2AB+g/K7zbRFxNbCO8t/RUD7eejGwGhhmMn7Ap78fDhzo+mokaTrpWihk5puPMOvCMZZN4Npu1TKmmTNh//5JXaUkTXXT+7+Sfx4zZ8Kzz/a6CkmaUpobCv395dqTzZJUa24ogEcLknQYQ8FQkKSaoWAoSFLNUDAUJKlmKBgKklQzFAwFSao1OxQGBgwFSWrT7FDwSEGSRjEUDAVJqhkKhoIk1QwFvxRPkmqGgkcKklQzFAwFSaoZCoaCJNUMBUNBkmqGgqEgSTVDwVCQpJqhYChIUs1QMBQkqWYoGAqSVDMUDAVJqhkKhoIk1QwFQ0GSaoaCoSBJtWaHgr+8JkmjNDsUPFKQpFEMBUNBkmqGgqEgSTVDwV9ek6SaoeCRgiTVDAVDQZJqhoKhIEk1Q8FQkKTajF6sNCLWAruAg8CBzFwZEScCtwLLgbXAZZn5VFcLMRQkaZReHin8i8w8JzNXVrevA+7OzBXA3dXt7jIUJGmUqTR8dAlwczV9M/DGrq/RUJCkUXoVCgl8IyLui4hrqrZFmbmpmt4MLBrrjhFxTUSsiohVW7du/fmqMBQkaZSenFMAXp2ZGyPiZODOiPin9pmZmRGRY90xM28EbgRYuXLlmMtMmKEgSaP05EghMzdW11uArwDnAU9ExGKA6npL1wsxFCRplEkPhYiYGxHzW9PA64CHgNuBK6vFrgS+2vViZs6Effu6vhpJmi56MXy0CPhKRLTW/9nM/IeIuBe4LSKuBtYBl3W9kjlzYM8eyIRSjyQ12qSHQmauAV4xRvuTwIWTWszAQLkMD8PcuZO6akmaiqbSR1J744QTYMeOXlchSVOCoWAoSFLNUDAUJKlmKBgKklQzFAwFSaoZCoaCJNUMBUNBkmqGgqEgSTVDwVCQpJqhYChIUs1QMBQkqWYoGAqSVDMUDAVJqhkKhoIk1QwFQ0GSaobCggWwcyccPNjrSiSp5wyFgQEYHIRNm3pdiST1nKEAsHQp/PSnva5CknrOUABDQZIqhgIYCpJUMRTAUJCkiqEAhoIkVQwFMBQkqWIoQAmFdet6XYUk9ZyhAHDSSeV669be1iFJPWYoAETAy14GDz7Y60okqacMhZaXv9xQkNR4hkLLL/4iPPBAr6uQpJ4yFFo8UpAkQ6H2spfBww/D/v29rkSSesZQaDn+eHjJS+Cee3pdiST1jKHQ7qKL4I47el2FJPWModDu4osNBUmNZii0e+UrYds2uPfeXlciST1hKLTr74cbboB3vxsye12NJE26KRcKEfGGiHgkIlZHxHWTXsBVV8GePXD99QaDpMaZUqEQEf3Ax4CLgLOBN0fE2ZNaxIwZ8LWvlXMLr30t3HorPPPMpJYgSb0yo9cFHOY8YHVmrgGIiM8DlwA/mtQqhobKeYXPfhY+9Sn4/d+HM8+E+fNh8WLYtw+2b4clS8r1wEC5z6FDsGsX9PXBcceVyxNPlPm7d8OcOTA4CJs3l/CZMaMcjcybBzt2lOGrPXvKZfnykenh4fIYg4OwZUu5PTRU2rZtgxNOKI951lkj95kxozzmsmVw8CCsXw+LFpXrZcvKY/T3w8yZsHMnzJ0La9bAGWeUOjdvhoULS939/SUYBwfLchs2lG3x5JPlo7wnn1yWP3iwLHvccWX9O3fC7Nml/dCh0r57d2k77jh4+uly+/TTy3bbtq30qa+vXIaGSl/27SuP199fHufgwXLZvx+efXbkEgErVpR9+Pjj5dtvI8r9Z8+GjRvLNpg5Ex57rOxLgKeeKtuwrw/27i3zW/tr9uzy2LNnlxoPHiyP13qeDA+Xfdjax/PmlccYHi7bK6Jsu74+OHCgPM727aW23bvL8+Oss8q8vXvLJXNkOy9fXm6vWQO/8Atleu3akfs//XR5Hg4Pl3qfempk38ycWfrV2k/79pXHfeyxsh0WLy7bZO7csp0zy345eLBc79tX+r53b7nd2vYLFpT2vr5S/4wZ5fWxc2d5fs6eXdbd11f2Weu6fbqvrzz+pk2ljiefLM/X5cvLtti0CU47rdQ1f37pJ5THnTGjtO/cWfq9fHnZ7o8+Wp6LAwMj+3TXrtK/1vZpPZdmzy5te/aU+fPmldsHDpT1RDz3Zc+esg+Ghkots2aVfrzoRaXek04q227TprJ9BgbKNm5dDh0amYayXVr7b9u20tf588s2XriwbLPdu0udu3fDRz4C55//vL/9RU6hIZKIuBR4Q2b+XnX7CuCVmfn2tmWuAa4BWLp06S+vm4yvvN61Cx55pLy4f/az8oRauLC8wZ50Utm5W7aUJ9u8eWUnt97MBwfLE23OnHJ7+/byxD1woLzAIkr7CSeUtoGBkTewOXNGLnv3lifcokXlBbp1a2kbHCwvpkWLyhvh3Lll/v79pZb168uTbcmS8uQ69dTy2xHz55e6n322TO/cWV5cjz9e2hYtKrXOn1/qnDu3PFH37Ckv4p074cQTy/VTT428GA8cKMscOlTuOzw88ia/d29p27t39Itx/fpyPTQ0sl0OHCjrmzOnvNhaQdD+BjMwUF44M2eOrHv16rLPli0rb6oR5f7Dw+XFumVL2TbLl5cXa39/2Zc7dpT9Nnv2yJvn7t1l2YGBUu+8eWUfz5pV1rF1a1m+r29ke+/eXebPmVPqb22HQ4fKuoaHy3Nmw4ay3CmnlG0+c2a5PWtWqXnnzrKv1q8vdS1bVt7MI8r0+vXljf7448tjtd7UFiwoz9fBwVLT00+P7Kfjjivzli4tz4Unnyz7cvfuMq+/v/S9r6881syZI2/ww8Olz4eH/aJFZX+uW1dqOOWU8vzZt6/0uT3EW9Ot64GBcv/Nm8u+nz+/BN7AQHmc9evLfti5s+yjiNKn/fvL9p0/v9S9Zk3pwxlnjHzT8YIFI2+wzzwz8vprPZeefba0tbbJ7t1lXYe/cY93mT27XFp/mO3dW7Z16w+q7dvLtlu0qNR46NBIoPT1jQ4YKM/f1v4bGir93LGjvLZaoTh3bql77tzyD7cLFhzTW1pE3JeZK8ecN91Cod3KlStz1apVk1miJE1744XClDqnAGwETmu7fWrVJkmaBFMtFO4FVkTE6RExE7gcuL3HNUlSY0ypE82ZeSAi3g58HegHPpmZD/e4LElqjCkVCgCZeQfgd01IUg9MteEjSVIPGQqSpJqhIEmqGQqSpNqU+ue1oxURW4Fj/ZfmQWDb81hOL9mXqcm+TE32BZZl5tBYM6Z1KPw8ImLVkf6jb7qxL1OTfZma7Mv4HD6SJNUMBUlSrcmhcGOvC3ge2Zepyb5MTfZlHI09pyBJ6tTkIwVJ0mEMBUlSrZGhEBFviIhHImJ1RFzX63qOVkSsjYgHI+L+iFhVtZ0YEXdGxKPV9cJe1zmWiPhkRGyJiIfa2sasPYqPVvvpgYg4t3eVdzpCX26IiI3Vvrk/Ii5um/feqi+PRMTre1N1p4g4LSK+GRE/ioiHI+IdVfu02y/j9GU67pfZEfH9iPhh1ZcPVu2nR8T3qppvrX5mgIiYVd1eXc1ffkwrzsxGXShfyf0YcAYwE/ghcHav6zrKPqwFBg9r+1Pgumr6OuBPel3nEWp/DXAu8NBz1Q5cDPw9EMD5wPd6Xf8E+nID8AdjLHt29VybBZxePQf7e92HqrbFwLnV9HzgJ1W9026/jNOX6bhfAphXTQ8A36u2923A5VX7J4B/V03/e+AT1fTlwK3Hst4mHimcB6zOzDWZ+SzweeCSHtf0fLgEuLmavhl4Yw9rOaLM/Daw/bDmI9V+CfDpLO4BFkTE4smp9LkdoS9Hcgnw+czcl5mPA6spz8Wey8xNmfmDanoX8GNgCdNwv4zTlyOZyvslM/OZ6uZAdUngtcAXq/bD90trf30RuDCi9QPQE9fEUFgCrG+7vYHxnzRTUQLfiIj7IuKaqm1RZm6qpjcDi3pT2jE5Uu3TdV+9vRpW+WTbMN606Es15PBLlL9Kp/V+OawvMA33S0T0R8T9wBbgTsqRzNOZeaBapL3eui/V/B3ASUe7ziaGwgvBqzPzXOAi4NqIeE37zCzHj9Pys8bTufbKx4EzgXOATcBHelvOxEXEPOBLwDszc2f7vOm2X8boy7TcL5l5MDPPofxe/XnAS7q9ziaGwkbgtLbbp1Zt00ZmbqyutwBfoTxZnmgdwlfXW3pX4VE7Uu3Tbl9l5hPVC/kQ8DeMDEVM6b5ExADlTfSWzPxy1Twt98tYfZmu+6UlM58Gvgn8KmW4rvWrme311n2p5p8APHm062piKNwLrKjO4M+knJC5vcc1TVhEzI2I+a1p4HXAQ5Q+XFktdiXw1d5UeEyOVPvtwFurT7ucD+xoG86Ykg4bW/9Nyr6B0pfLq0+InA6sAL4/2fWNpRp3vgn4cWb+edusabdfjtSXabpfhiJiQTV9HPAvKedIvglcWi12+H5p7a9Lgf9VHeEdnV6fYe/FhfLpiZ9Qxufe1+t6jrL2Myiflvgh8HCrfsrY4d3Ao8BdwIm9rvUI9X+Ocvi+nzIeevWRaqd8+uJj1X56EFjZ6/on0Je/rWp9oHqRLm5b/n1VXx4BLup1/W11vZoyNPQAcH91uXg67pdx+jId98vLgf9X1fwQ8P6q/QxKcK0GvgDMqtpnV7dXV/PPOJb1+jUXkqRaE4ePJElHYChIkmqGgiSpZihIkmqGgiSpZihIPRIRF0TE13pdh9TOUJAk1QwF6TlExO9W32t/f0T8dfUlZc9ExF9U33N/d0QMVcueExH3VF+89pW23yA4KyLuqr4b/wcRcWb18PMi4osR8U8RccuxfKul9HwyFKRxRMQ/A94EvCrLF5MdBN4CzAVWZeZLgW8BH6ju8mngPZn5csp/0LbabwE+lpmvAP455T+hoXyL5zsp3+t/BvCqrndKGseM515EarQLgV8G7q3+iD+O8sVwh4Bbq2U+A3w5Ik4AFmTmt6r2m4EvVN9VtSQzvwKQmXsBqsf7fmZuqG7fDywHvtv9bkljMxSk8QVwc2a+d1RjxH86bLlj/b6YfW3TB/E1qR5z+Ega393ApRFxMtS/W7yM8tppfVPl7wDfzcwdwFMR8WtV+xXAt7L8AtiGiHhj9RizImLOpPZCmiD/KpHGkZk/iojrKb9010f5RtRrgd3AedW8LZTzDlC+uvgT1Zv+GuCqqv0K4K8j4kPVY/z2JHZDmjC/JVU6BhHxTGbO63Ud0vPN4SNJUs0jBUlSzSMFSVLNUJAk1QwFSVLNUJAk1QwFSVLt/wMasb3vAbCh4gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 구조 파악\n",
        "model.summary()\n",
        "#input 2개 & output 1개 => param 3개!\n",
        "#param 3개 : w0, w1, b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fdXEXncwvqb",
        "outputId": "8757a505-1a61-4c6c-8096-2d1dfeac802f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 3\n",
            "Trainable params: 3\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = model.layers[0].get_weights()\n",
        "print(\"w1 : {:.2f}\".format(params[0][0][0]))\n",
        "print(\"w2 : {:.2f}\".format(params[0][1][0]))\n",
        "print(\"b : {:.2f}\".format(params[1][0]))\n",
        "# 원래 위에서 w1:2, w2:3, b:5 이렇게 썼는데 model 돌린 추정값도 이와 비슷하게 나옴!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0kTLcGsw2G6",
        "outputId": "1c70598f-e01e-4a87-ed85-46dd694f989b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w1 : 1.96\n",
            "w2 : 2.97\n",
            "b : 4.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras - Functional API"
      ],
      "metadata": {
        "id": "phFtYpRNxvm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "x = np.array(np.arange(-5, 5, 0.1))\n",
        "y = 2 * x * x + 3 * x + 5\n",
        "dataX = np.stack([x * x, x]).T\n",
        "\n",
        "xInput = Input(batch_shape=(None, dataX.shape[1]))\n",
        "yOutput = Dense(1)(xInput)\n",
        "model = Model(xInput, yOutput)\n",
        "model.compile(loss=\"mse\", optimizer=optimizers.Adam(learning_rate=0.05))\n",
        "\n",
        "h = model.fit(dataX, y, batch_size=10, epochs=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GXesQbVxKmn",
        "outputId": "0980eaf5-17f5-49b6-ce8b-7c7f5c63841f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 233.5495\n",
            "Epoch 2/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 106.4796\n",
            "Epoch 3/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 35.5300\n",
            "Epoch 4/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 18.7476\n",
            "Epoch 5/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 12.6378\n",
            "Epoch 6/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.7239\n",
            "Epoch 7/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 6.1930\n",
            "Epoch 8/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.5962\n",
            "Epoch 9/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.1000\n",
            "Epoch 10/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.6729\n",
            "Epoch 11/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.3964\n",
            "Epoch 12/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.0637\n",
            "Epoch 13/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.7850\n",
            "Epoch 14/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.5068\n",
            "Epoch 15/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.2607\n",
            "Epoch 16/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.0329\n",
            "Epoch 17/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.8113\n",
            "Epoch 18/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.6144\n",
            "Epoch 19/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.4238\n",
            "Epoch 20/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.2533\n",
            "Epoch 21/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.1114\n",
            "Epoch 22/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.9703\n",
            "Epoch 23/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.8504\n",
            "Epoch 24/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7424\n",
            "Epoch 25/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.6425\n",
            "Epoch 26/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.5528\n",
            "Epoch 27/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.4760\n",
            "Epoch 28/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.4121\n",
            "Epoch 29/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.3485\n",
            "Epoch 30/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.3009\n",
            "Epoch 31/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2537\n",
            "Epoch 32/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.2146\n",
            "Epoch 33/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1806\n",
            "Epoch 34/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1554\n",
            "Epoch 35/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1284\n",
            "Epoch 36/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1073\n",
            "Epoch 37/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0886\n",
            "Epoch 38/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0736\n",
            "Epoch 39/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0608\n",
            "Epoch 40/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0503\n",
            "Epoch 41/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0417\n",
            "Epoch 42/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0345\n",
            "Epoch 43/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0276\n",
            "Epoch 44/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0226\n",
            "Epoch 45/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 46/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0147\n",
            "Epoch 47/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0119\n",
            "Epoch 48/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0095\n",
            "Epoch 49/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0076\n",
            "Epoch 50/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0061\n",
            "Epoch 51/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0048\n",
            "Epoch 52/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Epoch 53/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 54/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0024\n",
            "Epoch 55/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 56/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0015\n",
            "Epoch 57/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 58/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.8019e-04\n",
            "Epoch 59/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.8681e-04\n",
            "Epoch 60/300\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 5.2161e-04\n",
            "Epoch 61/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.0824e-04\n",
            "Epoch 62/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.1171e-04\n",
            "Epoch 63/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 2.3671e-04\n",
            "Epoch 64/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.8469e-04\n",
            "Epoch 65/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.3631e-04\n",
            "Epoch 66/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0329e-04\n",
            "Epoch 67/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.8987e-05\n",
            "Epoch 68/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.7948e-05\n",
            "Epoch 69/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.3468e-05\n",
            "Epoch 70/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.2848e-05\n",
            "Epoch 71/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.4143e-05\n",
            "Epoch 72/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.7821e-05\n",
            "Epoch 73/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.3247e-05\n",
            "Epoch 74/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 9.7647e-06\n",
            "Epoch 75/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.1616e-06\n",
            "Epoch 76/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.0951e-06\n",
            "Epoch 77/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.7467e-06\n",
            "Epoch 78/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.7332e-06\n",
            "Epoch 79/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.9319e-06\n",
            "Epoch 80/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.4035e-06\n",
            "Epoch 81/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0224e-06\n",
            "Epoch 82/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.2682e-07\n",
            "Epoch 83/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 5.0991e-07\n",
            "Epoch 84/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.6346e-07\n",
            "Epoch 85/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 2.5523e-07\n",
            "Epoch 86/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.7947e-07\n",
            "Epoch 87/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.2421e-07\n",
            "Epoch 88/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 8.9195e-08\n",
            "Epoch 89/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.0372e-08\n",
            "Epoch 90/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.1338e-08\n",
            "Epoch 91/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.8482e-08\n",
            "Epoch 92/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9942e-08\n",
            "Epoch 93/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.3569e-08\n",
            "Epoch 94/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 9.2391e-09\n",
            "Epoch 95/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.1846e-09\n",
            "Epoch 96/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2432e-09\n",
            "Epoch 97/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.8383e-09\n",
            "Epoch 98/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9816e-09\n",
            "Epoch 99/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.3087e-09\n",
            "Epoch 100/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 9.2877e-10\n",
            "Epoch 101/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.2787e-10\n",
            "Epoch 102/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.8985e-10\n",
            "Epoch 103/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.7598e-10\n",
            "Epoch 104/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.8722e-10\n",
            "Epoch 105/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.0833e-10\n",
            "Epoch 106/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.1315e-11\n",
            "Epoch 107/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 5.0511e-11\n",
            "Epoch 108/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.9990e-11\n",
            "Epoch 109/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.9990e-11\n",
            "Epoch 110/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.9990e-11\n",
            "Epoch 111/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.9990e-11\n",
            "Epoch 112/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.9990e-11\n",
            "Epoch 113/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.9990e-11\n",
            "Epoch 114/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.9990e-11\n",
            "Epoch 115/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.9990e-11\n",
            "Epoch 116/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.9990e-11\n",
            "Epoch 117/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.9990e-11\n",
            "Epoch 118/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 4.9990e-11\n",
            "Epoch 119/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.9990e-11\n",
            "Epoch 120/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.9990e-11\n",
            "Epoch 121/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.9990e-11\n",
            "Epoch 122/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.9990e-11\n",
            "Epoch 123/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4683e-11\n",
            "Epoch 124/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4067e-11\n",
            "Epoch 125/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4067e-11\n",
            "Epoch 126/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4067e-11\n",
            "Epoch 127/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.4067e-11\n",
            "Epoch 128/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.4067e-11\n",
            "Epoch 129/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.4067e-11\n",
            "Epoch 130/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4067e-11\n",
            "Epoch 131/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4067e-11\n",
            "Epoch 132/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.4067e-11\n",
            "Epoch 133/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4067e-11\n",
            "Epoch 134/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4067e-11\n",
            "Epoch 135/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.4067e-11\n",
            "Epoch 136/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.4067e-11\n",
            "Epoch 137/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.4067e-11\n",
            "Epoch 138/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.4067e-11\n",
            "Epoch 139/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4067e-11\n",
            "Epoch 140/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.4067e-11\n",
            "Epoch 141/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 4.4358e-11\n",
            "Epoch 142/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 143/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 144/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 145/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 146/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 147/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 148/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 149/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 150/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 151/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 152/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 4.2330e-11\n",
            "Epoch 153/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 154/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 155/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 156/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 157/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 158/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 159/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 160/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 161/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 162/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 163/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 4.2330e-11\n",
            "Epoch 164/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.2330e-11\n",
            "Epoch 165/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 4.1205e-11\n",
            "Epoch 166/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 2.0676e-11\n",
            "Epoch 167/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.9136e-11\n",
            "Epoch 168/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.9136e-11\n",
            "Epoch 169/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 170/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.9136e-11\n",
            "Epoch 171/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.9136e-11\n",
            "Epoch 172/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 173/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 174/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 175/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 176/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 177/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 178/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 179/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 180/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 181/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 182/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 183/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 184/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 185/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 186/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 187/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 188/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 189/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 190/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 191/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.9136e-11\n",
            "Epoch 192/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.9136e-11\n",
            "Epoch 193/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1.9136e-11\n",
            "Epoch 194/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.0580e-11\n",
            "Epoch 195/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.4866e-11\n",
            "Epoch 196/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 197/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 198/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 199/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 200/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 201/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 202/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 203/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 204/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 205/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 206/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 207/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 208/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 209/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 210/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 211/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 212/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.2199e-11\n",
            "Epoch 213/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.2199e-11\n",
            "Epoch 214/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.2199e-11\n",
            "Epoch 215/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 216/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 1.2199e-11\n",
            "Epoch 217/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 9.6935e-12\n",
            "Epoch 218/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 219/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 8.9182e-12\n",
            "Epoch 220/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 221/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 222/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 223/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 224/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 8.9182e-12\n",
            "Epoch 225/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 226/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 227/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 228/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 229/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 230/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 231/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 232/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 233/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 8.9182e-12\n",
            "Epoch 234/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 235/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 236/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 237/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 238/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 8.9182e-12\n",
            "Epoch 239/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 240/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 241/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 242/300\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 8.9182e-12\n",
            "Epoch 243/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 244/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 245/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 246/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 247/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 248/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.9182e-12\n",
            "Epoch 249/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 8.9182e-12\n",
            "Epoch 250/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 8.5203e-12\n",
            "Epoch 251/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 7.4925e-12\n",
            "Epoch 252/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 7.4925e-12\n",
            "Epoch 253/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 254/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 255/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 256/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 257/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 258/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 7.4925e-12\n",
            "Epoch 259/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 260/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 261/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 262/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 263/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 264/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.4925e-12\n",
            "Epoch 265/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 7.4925e-12\n",
            "Epoch 266/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 7.4925e-12\n",
            "Epoch 267/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 7.4925e-12\n",
            "Epoch 268/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 7.2379e-12\n",
            "Epoch 269/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 270/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 6.7581e-12\n",
            "Epoch 271/300\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 6.7581e-12\n",
            "Epoch 272/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 273/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 274/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 275/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 6.7581e-12\n",
            "Epoch 276/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 277/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 278/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 279/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 280/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 281/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 6.7581e-12\n",
            "Epoch 282/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 283/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 284/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 285/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 286/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 6.7581e-12\n",
            "Epoch 287/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 6.7581e-12\n",
            "Epoch 288/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 6.7581e-12\n",
            "Epoch 289/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 6.7581e-12\n",
            "Epoch 290/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 291/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.7581e-12\n",
            "Epoch 292/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.3102e-12\n",
            "Epoch 293/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 5.0596e-12\n",
            "Epoch 294/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.0596e-12\n",
            "Epoch 295/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.0596e-12\n",
            "Epoch 296/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.0596e-12\n",
            "Epoch 297/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 5.0596e-12\n",
            "Epoch 298/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 5.0596e-12\n",
            "Epoch 299/300\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.0596e-12\n",
            "Epoch 300/300\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 5.0596e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(h.history['loss'], color='red', linewidth=1)\n",
        "plt.title(\"Loss function\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "OqV6L3rex7fV",
        "outputId": "fefb7f5e-b752-4ef9-d583-9ababd0d7134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "#inputlayer - param이 없음\n",
        "#dense layer - param 3개"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG3ff9voGylW",
        "outputId": "fde2c9e0-4c7d-44e6-8257-a1b9b2464bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 3\n",
            "Trainable params: 3\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = model.layers[1].get_weights()\n",
        "print(\"w1 : {:.2f}\".format(params[0][0][0]))\n",
        "print(\"w2 : {:.2f}\".format(params[0][1][0]))\n",
        "print(\"b : {:.2f}\".format(params[1][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sohSHjtCG1ip",
        "outputId": "e5cc642e-1bcb-4d4d-87e6-a3f5b41c77c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w1 : 2.00\n",
            "w2 : 3.00\n",
            "b : 5.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensorflow v1과 비교"
      ],
      "metadata": {
        "id": "q8s0VbKvHVAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qoYN2pmcHLCs",
        "outputId": "9ee9cc45-d99b-4c3c-9df4-c095f570a642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j38zjVXHahk",
        "outputId": "732a6e0b-800c-4465-e4d6-0b1dcc1c0399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iris tensorflow v1로 해보기"
      ],
      "metadata": {
        "id": "XL1yEdvKIAJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "pn9XcE1eHiSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "iris_X = iris.data\n",
        "# iris target을 one-hot encoding\n",
        "iris_y = pd.get_dummies(iris.target).to_numpy()"
      ],
      "metadata": {
        "id": "L7WhDRdBHrwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_y[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0HWvyZXH86W",
        "outputId": "5a113e62-7ef6-49d6-a16b-9977be5b211c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(iris_X, iris_y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "cJ7dW48lH-Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape\n",
        "#sample의 개수: 105개\n",
        "#feature 개수: 4개"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5esdUF7IdmI",
        "outputId": "09d7b11e-0e93-4e8b-d180-87d797c6714c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<신경망 layer 복습>\n",
        "- 샘플의 개수: 입출력을 105번 돌리는 것\n",
        "- 입력노드의 개수: 4개 - 여기에 bias는 포함x\n",
        "- 출력노드의 개수: 3개 - why? 우리가 맞혀야 하는 건 꽃 종류의 수! so, output 개수는 3개!\n",
        "- w는 1차원: w.shape = (입력의 크기, 출력의 크기) = (4,3)\n",
        "- b도 1차원: b.shape = (1, 출력의 크기) = (1,3) = (3,)"
      ],
      "metadata": {
        "id": "X-D2y1lPKRWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "불편한 예전 버전 V1"
      ],
      "metadata": {
        "id": "fYgyHSGQO-0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#v1에서 사용하는 방법!\n",
        "#x, y의 변수 선언 - 샘플 개수는 아직 모르니까 None\n",
        "x = tf.placeholder(tf.float32, [None, 4]) #변수의 수가 4개; 값은 비어있음\n",
        "y = tf.placeholder(tf.float32, [None, 3]) #클래스가 3개"
      ],
      "metadata": {
        "id": "4x5h3BiXKYSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.zeros([4,3])) #weight shape : (입력의 개수, 출력의 개수)\n",
        "b = tf.Variable(tf.zeros([3])) #bias shapep : (출력의 개수)"
      ],
      "metadata": {
        "id": "BCYZSt-6LPwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#출력\n",
        "h = tf.nn.softmax(tf.matmul(x, W) + b) #matmul로 output 구해서 softmax 함수 취함"
      ],
      "metadata": {
        "id": "vkhrUfMFLmxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#손실함수 정의\n",
        "cross_entropy = -tf.reduce_sum(y * tf.log(h), reduction_indices=[1]) #답지 * 예측값\n",
        "loss = tf.reduce_mean(cross_entropy)"
      ],
      "metadata": {
        "id": "KGPE4fqoL4WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 정의 - loss함수, optimizer\n",
        "#v1에서는 먼저 정의만 하고 연결은 나중에\n",
        "train = tf.train.GradientDescentOptimizer(0.001).minimize(loss)"
      ],
      "metadata": {
        "id": "J4BORtXOMLds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#v1에서는 이렇게 세션 따로 만들어 사용했었음\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "metadata": {
        "id": "CcxUR75OMwuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100000):\n",
        "  #placeholder에 값 넣어주는 것\n",
        "  _, loss_values = sess.run([train, loss], feed_dict={x:train_X, y:train_y})\n",
        "  if i % 10000 == 0:\n",
        "    print(i, loss_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBkZ3ifgM3uD",
        "outputId": "78533812-ef82-4633-9820-557ef4a057e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.28379795\n",
            "10000 0.23611484\n",
            "20000 0.20631167\n",
            "30000 0.18581773\n",
            "40000 0.17079735\n",
            "50000 0.15927438\n",
            "60000 0.15012614\n",
            "70000 0.14267017\n",
            "80000 0.13646227\n",
            "90000 0.13120523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_prediction = tf.equal(tf.argmax(h, 1), tf.argmax(y, 1))"
      ],
      "metadata": {
        "id": "dB8n0BwQNa9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubb70PC4OUm-",
        "outputId": "c3acc844-39a6-485c-eeca-3c5b5fa86df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Mean_1:0' shape=() dtype=float32>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터는 session에 있음 - 값 보려면 이렇게 해줘야 함\n",
        "print(sess.run(accuracy, feed_dict={x:test_X, y:test_y}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85vEfiaPOfwL",
        "outputId": "a21c09b2-d35c-4a23-a3de-26df85b59835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "원래 버전 V2 - Sequential"
      ],
      "metadata": {
        "id": "BTCoMhO9PUtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import optimizers"
      ],
      "metadata": {
        "id": "_Xtz-ZIdPFXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = train_X.shape[1]\n",
        "#3층 layer - 원하는대로 layer 추가해서 만들 수 있음\n",
        "#input_shape : 1차원 -> 입력 개수 나타냄\n",
        "model = Sequential()\n",
        "#10개, 8개는 hidden layer\n",
        "#relu니까 he_normal로 가중치 초기화\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "#위의 10개의 output이 아래의 input => so, 따로 input 넣지 않아도 됨\n",
        "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
        "\n",
        "#맨 마지막 층의 output 개수는 3개로 정해져 있음\n",
        "model.add(Dense(3, activation='softmax')) #마지막 layer"
      ],
      "metadata": {
        "id": "k8ItPx1wOpZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#가중치 학습 방법, loss함수(목적함수), 평가지표 정의\n",
        "#sparse_categorical_crossentropy : label로 받음, 층 여러개 있을 때 교차 엔트로피\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "0KovAD3cP6KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_X, np.argmax(train_y, axis=1), epochs=300, batch_size=32) #verbose=0; 로그 안 찍히도록"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFi6OQc5VBp4",
        "outputId": "cccdc2d1-5785-4182-f457-6ff468a41023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 105 samples\n",
            "Epoch 1/300\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 2.2821 - acc: 0.3524\n",
            "Epoch 2/300\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 2.1139 - acc: 0.3524\n",
            "Epoch 3/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 1.9720 - acc: 0.3619\n",
            "Epoch 4/300\n",
            "105/105 [==============================] - 0s 95us/sample - loss: 1.8618 - acc: 0.3810\n",
            "Epoch 5/300\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 1.7717 - acc: 0.4667\n",
            "Epoch 6/300\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 1.6949 - acc: 0.5810\n",
            "Epoch 7/300\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 1.6367 - acc: 0.6571\n",
            "Epoch 8/300\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 1.5784 - acc: 0.6190\n",
            "Epoch 9/300\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 1.5225 - acc: 0.6000\n",
            "Epoch 10/300\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 1.4649 - acc: 0.5810\n",
            "Epoch 11/300\n",
            "105/105 [==============================] - 0s 57us/sample - loss: 1.4127 - acc: 0.5810\n",
            "Epoch 12/300\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 1.3613 - acc: 0.5810\n",
            "Epoch 13/300\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 1.3170 - acc: 0.6000\n",
            "Epoch 14/300\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 1.2677 - acc: 0.6190\n",
            "Epoch 15/300\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 1.2282 - acc: 0.6286\n",
            "Epoch 16/300\n",
            "105/105 [==============================] - 0s 113us/sample - loss: 1.1887 - acc: 0.6381\n",
            "Epoch 17/300\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 1.1488 - acc: 0.6381\n",
            "Epoch 18/300\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 1.1136 - acc: 0.6667\n",
            "Epoch 19/300\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 1.0745 - acc: 0.6667\n",
            "Epoch 20/300\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 1.0419 - acc: 0.6476\n",
            "Epoch 21/300\n",
            "105/105 [==============================] - 0s 60us/sample - loss: 1.0108 - acc: 0.6381\n",
            "Epoch 22/300\n",
            "105/105 [==============================] - 0s 107us/sample - loss: 0.9789 - acc: 0.6381\n",
            "Epoch 23/300\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.9538 - acc: 0.6286\n",
            "Epoch 24/300\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.9245 - acc: 0.6381\n",
            "Epoch 25/300\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.9005 - acc: 0.6476\n",
            "Epoch 26/300\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.8770 - acc: 0.6381\n",
            "Epoch 27/300\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.8548 - acc: 0.6571\n",
            "Epoch 28/300\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.8344 - acc: 0.6381\n",
            "Epoch 29/300\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.8158 - acc: 0.6190\n",
            "Epoch 30/300\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.8011 - acc: 0.6000\n",
            "Epoch 31/300\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.7851 - acc: 0.5905\n",
            "Epoch 32/300\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.7670 - acc: 0.6000\n",
            "Epoch 33/300\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.7509 - acc: 0.6000\n",
            "Epoch 34/300\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.7374 - acc: 0.6095\n",
            "Epoch 35/300\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.7229 - acc: 0.6095\n",
            "Epoch 36/300\n",
            "105/105 [==============================] - 0s 99us/sample - loss: 0.7097 - acc: 0.5905\n",
            "Epoch 37/300\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.6972 - acc: 0.5714\n",
            "Epoch 38/300\n",
            "105/105 [==============================] - 0s 59us/sample - loss: 0.6853 - acc: 0.6000\n",
            "Epoch 39/300\n",
            "105/105 [==============================] - 0s 49us/sample - loss: 0.6735 - acc: 0.6095\n",
            "Epoch 40/300\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.6632 - acc: 0.5905\n",
            "Epoch 41/300\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.6528 - acc: 0.5905\n",
            "Epoch 42/300\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.6424 - acc: 0.6190\n",
            "Epoch 43/300\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.6324 - acc: 0.6857\n",
            "Epoch 44/300\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.6236 - acc: 0.8476\n",
            "Epoch 45/300\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.6121 - acc: 0.8952\n",
            "Epoch 46/300\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.6034 - acc: 0.9048\n",
            "Epoch 47/300\n",
            "105/105 [==============================] - 0s 59us/sample - loss: 0.5952 - acc: 0.9143\n",
            "Epoch 48/300\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 0.5864 - acc: 0.9238\n",
            "Epoch 49/300\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.5766 - acc: 0.9143\n",
            "Epoch 50/300\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.5681 - acc: 0.9048\n",
            "Epoch 51/300\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.5602 - acc: 0.8952\n",
            "Epoch 52/300\n",
            "105/105 [==============================] - 0s 61us/sample - loss: 0.5526 - acc: 0.8952\n",
            "Epoch 53/300\n",
            "105/105 [==============================] - 0s 71us/sample - loss: 0.5450 - acc: 0.8952\n",
            "Epoch 54/300\n",
            "105/105 [==============================] - 0s 80us/sample - loss: 0.5383 - acc: 0.9143\n",
            "Epoch 55/300\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.5319 - acc: 0.9238\n",
            "Epoch 56/300\n",
            "105/105 [==============================] - 0s 54us/sample - loss: 0.5255 - acc: 0.9238\n",
            "Epoch 57/300\n",
            "105/105 [==============================] - 0s 49us/sample - loss: 0.5193 - acc: 0.9238\n",
            "Epoch 58/300\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.5126 - acc: 0.9238\n",
            "Epoch 59/300\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.5067 - acc: 0.9238\n",
            "Epoch 60/300\n",
            "105/105 [==============================] - 0s 54us/sample - loss: 0.5009 - acc: 0.9143\n",
            "Epoch 61/300\n",
            "105/105 [==============================] - 0s 53us/sample - loss: 0.4949 - acc: 0.9143\n",
            "Epoch 62/300\n",
            "105/105 [==============================] - 0s 60us/sample - loss: 0.4905 - acc: 0.8952\n",
            "Epoch 63/300\n",
            "105/105 [==============================] - 0s 35us/sample - loss: 0.4840 - acc: 0.8952\n",
            "Epoch 64/300\n",
            "105/105 [==============================] - 0s 56us/sample - loss: 0.4787 - acc: 0.9143\n",
            "Epoch 65/300\n",
            "105/105 [==============================] - 0s 56us/sample - loss: 0.4734 - acc: 0.9143\n",
            "Epoch 66/300\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.4684 - acc: 0.9238\n",
            "Epoch 67/300\n",
            "105/105 [==============================] - 0s 108us/sample - loss: 0.4634 - acc: 0.9238\n",
            "Epoch 68/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.4580 - acc: 0.9143\n",
            "Epoch 69/300\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.4540 - acc: 0.9143\n",
            "Epoch 70/300\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.4496 - acc: 0.9048\n",
            "Epoch 71/300\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.4452 - acc: 0.9143\n",
            "Epoch 72/300\n",
            "105/105 [==============================] - 0s 99us/sample - loss: 0.4416 - acc: 0.9143\n",
            "Epoch 73/300\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.4361 - acc: 0.9048\n",
            "Epoch 74/300\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 0.4321 - acc: 0.9238\n",
            "Epoch 75/300\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.4274 - acc: 0.9238\n",
            "Epoch 76/300\n",
            "105/105 [==============================] - 0s 53us/sample - loss: 0.4236 - acc: 0.9238\n",
            "Epoch 77/300\n",
            "105/105 [==============================] - 0s 46us/sample - loss: 0.4192 - acc: 0.9238\n",
            "Epoch 78/300\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.4150 - acc: 0.9238\n",
            "Epoch 79/300\n",
            "105/105 [==============================] - 0s 112us/sample - loss: 0.4108 - acc: 0.9143\n",
            "Epoch 80/300\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.4076 - acc: 0.9143\n",
            "Epoch 81/300\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.4049 - acc: 0.9238\n",
            "Epoch 82/300\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.4013 - acc: 0.9143\n",
            "Epoch 83/300\n",
            "105/105 [==============================] - 0s 111us/sample - loss: 0.3973 - acc: 0.9143\n",
            "Epoch 84/300\n",
            "105/105 [==============================] - 0s 66us/sample - loss: 0.3933 - acc: 0.9238\n",
            "Epoch 85/300\n",
            "105/105 [==============================] - 0s 60us/sample - loss: 0.3897 - acc: 0.9238\n",
            "Epoch 86/300\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.3885 - acc: 0.9333\n",
            "Epoch 87/300\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.3843 - acc: 0.9333\n",
            "Epoch 88/300\n",
            "105/105 [==============================] - 0s 94us/sample - loss: 0.3815 - acc: 0.9143\n",
            "Epoch 89/300\n",
            "105/105 [==============================] - 0s 51us/sample - loss: 0.3754 - acc: 0.9143\n",
            "Epoch 90/300\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3731 - acc: 0.9143\n",
            "Epoch 91/300\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.3703 - acc: 0.9143\n",
            "Epoch 92/300\n",
            "105/105 [==============================] - 0s 43us/sample - loss: 0.3668 - acc: 0.9143\n",
            "Epoch 93/300\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.3629 - acc: 0.9143\n",
            "Epoch 94/300\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.3598 - acc: 0.9143\n",
            "Epoch 95/300\n",
            "105/105 [==============================] - 0s 83us/sample - loss: 0.3569 - acc: 0.9143\n",
            "Epoch 96/300\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.3540 - acc: 0.9143\n",
            "Epoch 97/300\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.3512 - acc: 0.9143\n",
            "Epoch 98/300\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.3478 - acc: 0.9143\n",
            "Epoch 99/300\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3456 - acc: 0.9143\n",
            "Epoch 100/300\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.3426 - acc: 0.9143\n",
            "Epoch 101/300\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3398 - acc: 0.9143\n",
            "Epoch 102/300\n",
            "105/105 [==============================] - 0s 100us/sample - loss: 0.3375 - acc: 0.9143\n",
            "Epoch 103/300\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.3366 - acc: 0.9238\n",
            "Epoch 104/300\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.3324 - acc: 0.9238\n",
            "Epoch 105/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.3293 - acc: 0.9238\n",
            "Epoch 106/300\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3270 - acc: 0.9238\n",
            "Epoch 107/300\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.3256 - acc: 0.9238\n",
            "Epoch 108/300\n",
            "105/105 [==============================] - 0s 56us/sample - loss: 0.3239 - acc: 0.9333\n",
            "Epoch 109/300\n",
            "105/105 [==============================] - 0s 66us/sample - loss: 0.3190 - acc: 0.9238\n",
            "Epoch 110/300\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.3166 - acc: 0.9238\n",
            "Epoch 111/300\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.3160 - acc: 0.9238\n",
            "Epoch 112/300\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.3125 - acc: 0.9238\n",
            "Epoch 113/300\n",
            "105/105 [==============================] - 0s 84us/sample - loss: 0.3094 - acc: 0.9238\n",
            "Epoch 114/300\n",
            "105/105 [==============================] - 0s 60us/sample - loss: 0.3074 - acc: 0.9048\n",
            "Epoch 115/300\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.3056 - acc: 0.9143\n",
            "Epoch 116/300\n",
            "105/105 [==============================] - 0s 106us/sample - loss: 0.3027 - acc: 0.9238\n",
            "Epoch 117/300\n",
            "105/105 [==============================] - 0s 59us/sample - loss: 0.3009 - acc: 0.9333\n",
            "Epoch 118/300\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.2987 - acc: 0.9238\n",
            "Epoch 119/300\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.2965 - acc: 0.9238\n",
            "Epoch 120/300\n",
            "105/105 [==============================] - 0s 79us/sample - loss: 0.2954 - acc: 0.9238\n",
            "Epoch 121/300\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.2924 - acc: 0.9048\n",
            "Epoch 122/300\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.2902 - acc: 0.9333\n",
            "Epoch 123/300\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2886 - acc: 0.9333\n",
            "Epoch 124/300\n",
            "105/105 [==============================] - 0s 102us/sample - loss: 0.2872 - acc: 0.9333\n",
            "Epoch 125/300\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.2845 - acc: 0.9333\n",
            "Epoch 126/300\n",
            "105/105 [==============================] - 0s 59us/sample - loss: 0.2828 - acc: 0.9333\n",
            "Epoch 127/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.2811 - acc: 0.9333\n",
            "Epoch 128/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.2793 - acc: 0.9333\n",
            "Epoch 129/300\n",
            "105/105 [==============================] - 0s 121us/sample - loss: 0.2773 - acc: 0.9238\n",
            "Epoch 130/300\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.2753 - acc: 0.9238\n",
            "Epoch 131/300\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.2732 - acc: 0.9238\n",
            "Epoch 132/300\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.2718 - acc: 0.9333\n",
            "Epoch 133/300\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 0.2720 - acc: 0.9238\n",
            "Epoch 134/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.2696 - acc: 0.9238\n",
            "Epoch 135/300\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.2666 - acc: 0.9333\n",
            "Epoch 136/300\n",
            "105/105 [==============================] - 0s 103us/sample - loss: 0.2646 - acc: 0.9429\n",
            "Epoch 137/300\n",
            "105/105 [==============================] - 0s 81us/sample - loss: 0.2636 - acc: 0.9429\n",
            "Epoch 138/300\n",
            "105/105 [==============================] - 0s 59us/sample - loss: 0.2617 - acc: 0.9238\n",
            "Epoch 139/300\n",
            "105/105 [==============================] - 0s 51us/sample - loss: 0.2598 - acc: 0.9238\n",
            "Epoch 140/300\n",
            "105/105 [==============================] - 0s 49us/sample - loss: 0.2587 - acc: 0.9143\n",
            "Epoch 141/300\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.2581 - acc: 0.9143\n",
            "Epoch 142/300\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.2552 - acc: 0.9333\n",
            "Epoch 143/300\n",
            "105/105 [==============================] - 0s 52us/sample - loss: 0.2534 - acc: 0.9333\n",
            "Epoch 144/300\n",
            "105/105 [==============================] - 0s 51us/sample - loss: 0.2517 - acc: 0.9333\n",
            "Epoch 145/300\n",
            "105/105 [==============================] - 0s 54us/sample - loss: 0.2501 - acc: 0.9429\n",
            "Epoch 146/300\n",
            "105/105 [==============================] - 0s 52us/sample - loss: 0.2485 - acc: 0.9429\n",
            "Epoch 147/300\n",
            "105/105 [==============================] - 0s 41us/sample - loss: 0.2472 - acc: 0.9333\n",
            "Epoch 148/300\n",
            "105/105 [==============================] - 0s 50us/sample - loss: 0.2459 - acc: 0.9238\n",
            "Epoch 149/300\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.2449 - acc: 0.9238\n",
            "Epoch 150/300\n",
            "105/105 [==============================] - 0s 50us/sample - loss: 0.2423 - acc: 0.9238\n",
            "Epoch 151/300\n",
            "105/105 [==============================] - 0s 58us/sample - loss: 0.2428 - acc: 0.9333\n",
            "Epoch 152/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.2408 - acc: 0.9333\n",
            "Epoch 153/300\n",
            "105/105 [==============================] - 0s 54us/sample - loss: 0.2393 - acc: 0.9333\n",
            "Epoch 154/300\n",
            "105/105 [==============================] - 0s 50us/sample - loss: 0.2376 - acc: 0.9333\n",
            "Epoch 155/300\n",
            "105/105 [==============================] - 0s 51us/sample - loss: 0.2356 - acc: 0.9429\n",
            "Epoch 156/300\n",
            "105/105 [==============================] - 0s 50us/sample - loss: 0.2347 - acc: 0.9429\n",
            "Epoch 157/300\n",
            "105/105 [==============================] - 0s 49us/sample - loss: 0.2330 - acc: 0.9429\n",
            "Epoch 158/300\n",
            "105/105 [==============================] - 0s 49us/sample - loss: 0.2318 - acc: 0.9429\n",
            "Epoch 159/300\n",
            "105/105 [==============================] - 0s 49us/sample - loss: 0.2301 - acc: 0.9429\n",
            "Epoch 160/300\n",
            "105/105 [==============================] - 0s 53us/sample - loss: 0.2292 - acc: 0.9429\n",
            "Epoch 161/300\n",
            "105/105 [==============================] - 0s 53us/sample - loss: 0.2276 - acc: 0.9333\n",
            "Epoch 162/300\n",
            "105/105 [==============================] - 0s 53us/sample - loss: 0.2291 - acc: 0.9333\n",
            "Epoch 163/300\n",
            "105/105 [==============================] - 0s 43us/sample - loss: 0.2261 - acc: 0.9333\n",
            "Epoch 164/300\n",
            "105/105 [==============================] - 0s 54us/sample - loss: 0.2236 - acc: 0.9429\n",
            "Epoch 165/300\n",
            "105/105 [==============================] - 0s 52us/sample - loss: 0.2231 - acc: 0.9238\n",
            "Epoch 166/300\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2222 - acc: 0.9238\n",
            "Epoch 167/300\n",
            "105/105 [==============================] - 0s 54us/sample - loss: 0.2213 - acc: 0.9333\n",
            "Epoch 168/300\n",
            "105/105 [==============================] - 0s 57us/sample - loss: 0.2188 - acc: 0.9429\n",
            "Epoch 169/300\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.2175 - acc: 0.9429\n",
            "Epoch 170/300\n",
            "105/105 [==============================] - 0s 52us/sample - loss: 0.2172 - acc: 0.9429\n",
            "Epoch 171/300\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.2163 - acc: 0.9333\n",
            "Epoch 172/300\n",
            "105/105 [==============================] - 0s 58us/sample - loss: 0.2148 - acc: 0.9429\n",
            "Epoch 173/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.2135 - acc: 0.9429\n",
            "Epoch 174/300\n",
            "105/105 [==============================] - 0s 61us/sample - loss: 0.2122 - acc: 0.9429\n",
            "Epoch 175/300\n",
            "105/105 [==============================] - 0s 52us/sample - loss: 0.2116 - acc: 0.9333\n",
            "Epoch 176/300\n",
            "105/105 [==============================] - 0s 46us/sample - loss: 0.2099 - acc: 0.9429\n",
            "Epoch 177/300\n",
            "105/105 [==============================] - 0s 46us/sample - loss: 0.2091 - acc: 0.9429\n",
            "Epoch 178/300\n",
            "105/105 [==============================] - 0s 53us/sample - loss: 0.2080 - acc: 0.9429\n",
            "Epoch 179/300\n",
            "105/105 [==============================] - 0s 37us/sample - loss: 0.2072 - acc: 0.9429\n",
            "Epoch 180/300\n",
            "105/105 [==============================] - 0s 47us/sample - loss: 0.2081 - acc: 0.9333\n",
            "Epoch 181/300\n",
            "105/105 [==============================] - 0s 48us/sample - loss: 0.2052 - acc: 0.9333\n",
            "Epoch 182/300\n",
            "105/105 [==============================] - 0s 120us/sample - loss: 0.2026 - acc: 0.9429\n",
            "Epoch 183/300\n",
            "105/105 [==============================] - 0s 99us/sample - loss: 0.2034 - acc: 0.9238\n",
            "Epoch 184/300\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.2033 - acc: 0.9238\n",
            "Epoch 185/300\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.2010 - acc: 0.9238\n",
            "Epoch 186/300\n",
            "105/105 [==============================] - 0s 53us/sample - loss: 0.2008 - acc: 0.9429\n",
            "Epoch 187/300\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.2005 - acc: 0.9429\n",
            "Epoch 188/300\n",
            "105/105 [==============================] - 0s 47us/sample - loss: 0.1980 - acc: 0.9429\n",
            "Epoch 189/300\n",
            "105/105 [==============================] - 0s 51us/sample - loss: 0.1973 - acc: 0.9429\n",
            "Epoch 190/300\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.1961 - acc: 0.9333\n",
            "Epoch 191/300\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.1953 - acc: 0.9238\n",
            "Epoch 192/300\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.1948 - acc: 0.9238\n",
            "Epoch 193/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.1927 - acc: 0.9333\n",
            "Epoch 194/300\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.1934 - acc: 0.9429\n",
            "Epoch 195/300\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.1910 - acc: 0.9429\n",
            "Epoch 196/300\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.1902 - acc: 0.9429\n",
            "Epoch 197/300\n",
            "105/105 [==============================] - 0s 89us/sample - loss: 0.1902 - acc: 0.9333\n",
            "Epoch 198/300\n",
            "105/105 [==============================] - 0s 109us/sample - loss: 0.1886 - acc: 0.9333\n",
            "Epoch 199/300\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.1887 - acc: 0.9429\n",
            "Epoch 200/300\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.1876 - acc: 0.9429\n",
            "Epoch 201/300\n",
            "105/105 [==============================] - 0s 96us/sample - loss: 0.1875 - acc: 0.9429\n",
            "Epoch 202/300\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.1861 - acc: 0.9429\n",
            "Epoch 203/300\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 0.1861 - acc: 0.9238\n",
            "Epoch 204/300\n",
            "105/105 [==============================] - 0s 92us/sample - loss: 0.1838 - acc: 0.9333\n",
            "Epoch 205/300\n",
            "105/105 [==============================] - 0s 38us/sample - loss: 0.1832 - acc: 0.9333\n",
            "Epoch 206/300\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.1819 - acc: 0.9429\n",
            "Epoch 207/300\n",
            "105/105 [==============================] - 0s 53us/sample - loss: 0.1829 - acc: 0.9333\n",
            "Epoch 208/300\n",
            "105/105 [==============================] - 0s 47us/sample - loss: 0.1819 - acc: 0.9429\n",
            "Epoch 209/300\n",
            "105/105 [==============================] - 0s 46us/sample - loss: 0.1808 - acc: 0.9429\n",
            "Epoch 210/300\n",
            "105/105 [==============================] - 0s 44us/sample - loss: 0.1793 - acc: 0.9333\n",
            "Epoch 211/300\n",
            "105/105 [==============================] - 0s 56us/sample - loss: 0.1784 - acc: 0.9429\n",
            "Epoch 212/300\n",
            "105/105 [==============================] - 0s 59us/sample - loss: 0.1780 - acc: 0.9333\n",
            "Epoch 213/300\n",
            "105/105 [==============================] - 0s 58us/sample - loss: 0.1776 - acc: 0.9333\n",
            "Epoch 214/300\n",
            "105/105 [==============================] - 0s 61us/sample - loss: 0.1770 - acc: 0.9333\n",
            "Epoch 215/300\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.1760 - acc: 0.9333\n",
            "Epoch 216/300\n",
            "105/105 [==============================] - 0s 56us/sample - loss: 0.1756 - acc: 0.9333\n",
            "Epoch 217/300\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.1753 - acc: 0.9429\n",
            "Epoch 218/300\n",
            "105/105 [==============================] - 0s 65us/sample - loss: 0.1765 - acc: 0.9429\n",
            "Epoch 219/300\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.1723 - acc: 0.9524\n",
            "Epoch 220/300\n",
            "105/105 [==============================] - 0s 97us/sample - loss: 0.1744 - acc: 0.9333\n",
            "Epoch 221/300\n",
            "105/105 [==============================] - 0s 90us/sample - loss: 0.1757 - acc: 0.9429\n",
            "Epoch 222/300\n",
            "105/105 [==============================] - 0s 54us/sample - loss: 0.1715 - acc: 0.9333\n",
            "Epoch 223/300\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.1722 - acc: 0.9333\n",
            "Epoch 224/300\n",
            "105/105 [==============================] - 0s 59us/sample - loss: 0.1759 - acc: 0.9429\n",
            "Epoch 225/300\n",
            "105/105 [==============================] - 0s 61us/sample - loss: 0.1753 - acc: 0.9429\n",
            "Epoch 226/300\n",
            "105/105 [==============================] - 0s 123us/sample - loss: 0.1716 - acc: 0.9333\n",
            "Epoch 227/300\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.1693 - acc: 0.9429\n",
            "Epoch 228/300\n",
            "105/105 [==============================] - 0s 58us/sample - loss: 0.1679 - acc: 0.9333\n",
            "Epoch 229/300\n",
            "105/105 [==============================] - 0s 57us/sample - loss: 0.1667 - acc: 0.9429\n",
            "Epoch 230/300\n",
            "105/105 [==============================] - 0s 66us/sample - loss: 0.1660 - acc: 0.9524\n",
            "Epoch 231/300\n",
            "105/105 [==============================] - 0s 46us/sample - loss: 0.1658 - acc: 0.9429\n",
            "Epoch 232/300\n",
            "105/105 [==============================] - 0s 85us/sample - loss: 0.1655 - acc: 0.9429\n",
            "Epoch 233/300\n",
            "105/105 [==============================] - 0s 45us/sample - loss: 0.1648 - acc: 0.9429\n",
            "Epoch 234/300\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.1636 - acc: 0.9429\n",
            "Epoch 235/300\n",
            "105/105 [==============================] - 0s 56us/sample - loss: 0.1637 - acc: 0.9333\n",
            "Epoch 236/300\n",
            "105/105 [==============================] - 0s 61us/sample - loss: 0.1644 - acc: 0.9333\n",
            "Epoch 237/300\n",
            "105/105 [==============================] - 0s 54us/sample - loss: 0.1616 - acc: 0.9333\n",
            "Epoch 238/300\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.1611 - acc: 0.9429\n",
            "Epoch 239/300\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.1654 - acc: 0.9429\n",
            "Epoch 240/300\n",
            "105/105 [==============================] - 0s 74us/sample - loss: 0.1645 - acc: 0.9429\n",
            "Epoch 241/300\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.1592 - acc: 0.9429\n",
            "Epoch 242/300\n",
            "105/105 [==============================] - 0s 67us/sample - loss: 0.1584 - acc: 0.9524\n",
            "Epoch 243/300\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.1639 - acc: 0.9429\n",
            "Epoch 244/300\n",
            "105/105 [==============================] - 0s 72us/sample - loss: 0.1629 - acc: 0.9429\n",
            "Epoch 245/300\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.1592 - acc: 0.9429\n",
            "Epoch 246/300\n",
            "105/105 [==============================] - 0s 69us/sample - loss: 0.1579 - acc: 0.9429\n",
            "Epoch 247/300\n",
            "105/105 [==============================] - 0s 45us/sample - loss: 0.1567 - acc: 0.9429\n",
            "Epoch 248/300\n",
            "105/105 [==============================] - 0s 41us/sample - loss: 0.1571 - acc: 0.9429\n",
            "Epoch 249/300\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.1557 - acc: 0.9429\n",
            "Epoch 250/300\n",
            "105/105 [==============================] - 0s 49us/sample - loss: 0.1556 - acc: 0.9429\n",
            "Epoch 251/300\n",
            "105/105 [==============================] - 0s 40us/sample - loss: 0.1542 - acc: 0.9429\n",
            "Epoch 252/300\n",
            "105/105 [==============================] - 0s 49us/sample - loss: 0.1558 - acc: 0.9429\n",
            "Epoch 253/300\n",
            "105/105 [==============================] - 0s 38us/sample - loss: 0.1568 - acc: 0.9429\n",
            "Epoch 254/300\n",
            "105/105 [==============================] - 0s 40us/sample - loss: 0.1549 - acc: 0.9429\n",
            "Epoch 255/300\n",
            "105/105 [==============================] - 0s 93us/sample - loss: 0.1545 - acc: 0.9429\n",
            "Epoch 256/300\n",
            "105/105 [==============================] - 0s 47us/sample - loss: 0.1522 - acc: 0.9429\n",
            "Epoch 257/300\n",
            "105/105 [==============================] - 0s 61us/sample - loss: 0.1532 - acc: 0.9429\n",
            "Epoch 258/300\n",
            "105/105 [==============================] - 0s 36us/sample - loss: 0.1526 - acc: 0.9429\n",
            "Epoch 259/300\n",
            "105/105 [==============================] - 0s 58us/sample - loss: 0.1500 - acc: 0.9429\n",
            "Epoch 260/300\n",
            "105/105 [==============================] - 0s 76us/sample - loss: 0.1524 - acc: 0.9429\n",
            "Epoch 261/300\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.1520 - acc: 0.9429\n",
            "Epoch 262/300\n",
            "105/105 [==============================] - 0s 54us/sample - loss: 0.1495 - acc: 0.9429\n",
            "Epoch 263/300\n",
            "105/105 [==============================] - 0s 125us/sample - loss: 0.1474 - acc: 0.9524\n",
            "Epoch 264/300\n",
            "105/105 [==============================] - 0s 73us/sample - loss: 0.1513 - acc: 0.9524\n",
            "Epoch 265/300\n",
            "105/105 [==============================] - 0s 87us/sample - loss: 0.1518 - acc: 0.9429\n",
            "Epoch 266/300\n",
            "105/105 [==============================] - 0s 86us/sample - loss: 0.1493 - acc: 0.9524\n",
            "Epoch 267/300\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.1478 - acc: 0.9429\n",
            "Epoch 268/300\n",
            "105/105 [==============================] - 0s 78us/sample - loss: 0.1472 - acc: 0.9429\n",
            "Epoch 269/300\n",
            "105/105 [==============================] - 0s 77us/sample - loss: 0.1468 - acc: 0.9429\n",
            "Epoch 270/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.1467 - acc: 0.9429\n",
            "Epoch 271/300\n",
            "105/105 [==============================] - 0s 75us/sample - loss: 0.1466 - acc: 0.9429\n",
            "Epoch 272/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.1449 - acc: 0.9333\n",
            "Epoch 273/300\n",
            "105/105 [==============================] - 0s 105us/sample - loss: 0.1479 - acc: 0.9429\n",
            "Epoch 274/300\n",
            "105/105 [==============================] - 0s 64us/sample - loss: 0.1460 - acc: 0.9524\n",
            "Epoch 275/300\n",
            "105/105 [==============================] - 0s 63us/sample - loss: 0.1441 - acc: 0.9524\n",
            "Epoch 276/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.1441 - acc: 0.9524\n",
            "Epoch 277/300\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.1437 - acc: 0.9524\n",
            "Epoch 278/300\n",
            "105/105 [==============================] - 0s 70us/sample - loss: 0.1427 - acc: 0.9524\n",
            "Epoch 279/300\n",
            "105/105 [==============================] - 0s 91us/sample - loss: 0.1435 - acc: 0.9429\n",
            "Epoch 280/300\n",
            "105/105 [==============================] - 0s 51us/sample - loss: 0.1433 - acc: 0.9429\n",
            "Epoch 281/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.1429 - acc: 0.9524\n",
            "Epoch 282/300\n",
            "105/105 [==============================] - 0s 58us/sample - loss: 0.1408 - acc: 0.9524\n",
            "Epoch 283/300\n",
            "105/105 [==============================] - 0s 53us/sample - loss: 0.1410 - acc: 0.9619\n",
            "Epoch 284/300\n",
            "105/105 [==============================] - 0s 54us/sample - loss: 0.1415 - acc: 0.9429\n",
            "Epoch 285/300\n",
            "105/105 [==============================] - 0s 60us/sample - loss: 0.1410 - acc: 0.9429\n",
            "Epoch 286/300\n",
            "105/105 [==============================] - 0s 60us/sample - loss: 0.1436 - acc: 0.9429\n",
            "Epoch 287/300\n",
            "105/105 [==============================] - 0s 58us/sample - loss: 0.1437 - acc: 0.9524\n",
            "Epoch 288/300\n",
            "105/105 [==============================] - 0s 56us/sample - loss: 0.1412 - acc: 0.9429\n",
            "Epoch 289/300\n",
            "105/105 [==============================] - 0s 59us/sample - loss: 0.1392 - acc: 0.9429\n",
            "Epoch 290/300\n",
            "105/105 [==============================] - 0s 62us/sample - loss: 0.1387 - acc: 0.9524\n",
            "Epoch 291/300\n",
            "105/105 [==============================] - 0s 55us/sample - loss: 0.1384 - acc: 0.9524\n",
            "Epoch 292/300\n",
            "105/105 [==============================] - 0s 57us/sample - loss: 0.1376 - acc: 0.9524\n",
            "Epoch 293/300\n",
            "105/105 [==============================] - 0s 52us/sample - loss: 0.1376 - acc: 0.9619\n",
            "Epoch 294/300\n",
            "105/105 [==============================] - 0s 45us/sample - loss: 0.1378 - acc: 0.9524\n",
            "Epoch 295/300\n",
            "105/105 [==============================] - 0s 36us/sample - loss: 0.1378 - acc: 0.9429\n",
            "Epoch 296/300\n",
            "105/105 [==============================] - 0s 34us/sample - loss: 0.1354 - acc: 0.9619\n",
            "Epoch 297/300\n",
            "105/105 [==============================] - 0s 68us/sample - loss: 0.1383 - acc: 0.9429\n",
            "Epoch 298/300\n",
            "105/105 [==============================] - 0s 88us/sample - loss: 0.1390 - acc: 0.9429\n",
            "Epoch 299/300\n",
            "105/105 [==============================] - 0s 99us/sample - loss: 0.1386 - acc: 0.9524\n",
            "Epoch 300/300\n",
            "105/105 [==============================] - 0s 82us/sample - loss: 0.1352 - acc: 0.9524\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2bb5b59d50>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(test_X, np.argmax(test_y, axis=1), verbose=0)\n",
        "print(f'test accuracy : {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpl5dlj2WBnl",
        "outputId": "be6f3839-13d8-4c84-e30e-cf1405df1e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy : 0.9555555582046509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter tuning\n",
        "#decay : update마다 적용되는 학습률의 감소율 - decay=0으로 두면 작아지지 않음; 학습률 변화시키려고 두는 것\n",
        "#학습 진행하면서 학습률 점차 조정\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=0.002), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_X, np.argmax(train_y, axis=1), epochs=300, batch_size=32, verbose=0) #verbose=0; 로그 안 찍히도록 하는 옵션\n",
        "loss, acc = model.evaluate(test_X, np.argmax(test_y, axis=1), verbose=0)\n",
        "print(f'test accuracy : {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F1zm505Wave",
        "outputId": "2a54ab86-55c5-4c24-e210-510af76f179c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "v2 - Functional API"
      ],
      "metadata": {
        "id": "6z7WXYx6aHxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = Input(shape=(n_features, )) #입력층 만들기\n",
        "hidden_layer_1 = Dense(10)(input_layer)\n",
        "hidden_layer_2 = Dense(8)(hidden_layer_1)\n",
        "output_layer = Dense(3, activation='softmax')(hidden_layer_2)\n",
        "model = Model(input_layer, output_layer)\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=0.002), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_X, np.argmax(train_y, axis=1), epochs=300, batch_size=32, verbose=0)\n",
        "loss, acc = model.evaluate(test_X, np.argmax(test_y, axis=1), verbose=0)\n",
        "print(f'test accuracy : {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yEGcO25Xi1u",
        "outputId": "e5b3871a-03c4-43f3-e6a0-290e4be3a1f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy : 0.9777777791023254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xj7J6yZgbtBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}